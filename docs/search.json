[
  {
    "objectID": "seminar3.html",
    "href": "seminar3.html",
    "title": "Seminar 3",
    "section": "",
    "text": "Seminar 3\n\nExercise 1\n\n\nExercise 2\n\n\nExercise 3\n\n\nExercise 4\n\n\nExercise 5",
    "crumbs": [
      "Seminars",
      "Seminar 3"
    ]
  },
  {
    "objectID": "seminar1.html",
    "href": "seminar1.html",
    "title": "Seminar 1",
    "section": "",
    "text": "Seminar 1\n\nExercise 1\nConsider these 11 sorted numbers: \\[4.2, 4.2, 4.4, 4.7, 5.2, 5.3, 5.5, 5.6, 5.7, 6.5, 6.6.\\]\n\nFor the given set of data, find the i) Median, ii) 1st and 3rd quartile, iii) Interquartile range, iv) Min and Max.\nUse the information from (a) to draw a box plot of the data points.\n\n\n\nExercise 2 (Exam spring 2025, problem 1)\nYou are presented two graphs. Figure 1a) is an illustration that was published on Twitter by the official White House account for the Obama administration showing an increase in high school graduation rate under Obama’s presidency (2009-2017). The second graph was presented on Fox News during the 2012 Republican nomination. For each of the graphs, (a) and (b), discuss in a few sentences whether you think these are good graphics? Justify your answer.\n\n\n\na) Graduation rate under President Obama, posted on Twitter (now X), by the White House. b) Fox News presenting support for various Republican nominees for the 2012 US election.\n\n\n\n\nExercise 3 (Exam spring 2025 retake, problem 1)\nA company, called MetaZen Pharma, is selling a dietary supplement called VitaBoost365+. The first graphic (a) below was used in a nationwide campaign, where they claimed that their product lead to a large improvement in energy score compared to a placebo supplement.\nThe second graphic (b) was shown at the general assembly of share holders. Discuss how these graphics have been manipulated for pushing the agenda of the company. What is the company trying to achieve here?\n\n\n\n\n\n\n\n\n\n\n\nExercise 4 (Trial exam, Spring 2025, problem 2a)\nRoulette is a game of chance where a roulette wheel has 37 numbered pockets where a ball can land. 18 of these pockets are red, 18 are black, and 1 is green (at least in Europe).\nIf, for example, you bet 1, on red, and red occurs, then you win your bet back plus 1, (i.e., a gain of 1). But if black or green occurs, you lose the bet (a gain of -1).\nYou bet 1, on red. Answer the following:\n\nWhat is the expected gain?\nWhat is the variance of this gain?\nWhat is the probability of a positive gain?\n\n\n\nExercise 5 (Exam spring 2025, problem 3a-b)\nA startup offering hands-on craft beer brewing workshops is piloting a weekend course format. Each session runs on Saturdays only and is limited to 4 participants due to space and brewing setup constraints. Based on early demand, the number of participants X per Saturday follows this discrete distribution:\n\n\n\nParticipants (X)\n0\n1\n2\n3\n4\n\n\n\n\nProbability P(X=x)\n0.10\n0.25\n0.30\n0.20\n0.15\n\n\n\n\n\nWhat is the probability that a single workshop has at least one participant? What is the probability that the workshop has at least one participant every Saturday for five consecutive weeks?\nShow that the expected number of participants per week is \\(2.05\\) and that the standard deviation is \\(1.20\\).\n\n\n\nExercise 6\nLet \\(X\\) be a continuous random variable with density function \\[f(x) = \\lambda e^{-2x}, \\quad x\\ge 0.\\]\n\nFind \\(\\lambda\\) for \\(f\\) to fulfill the requirements of being a density.\nFind an expression for the cumulative distribution function \\(F(x)=P(X\\le x)\\).\nWhat is the probability that X is larger than 2?\nShow that the distribution of X fulfills the memoryless property, i.e. that \\[P(X&gt;s+t|X&gt;s)=P(X&gt;t).\\]\nX is exponentially distributed, which in general, has the memoryless property. This distribution is often used for waiting times. Can you give an intuitive interpretation of the memoryless property if X for example is the waiting time for the next arriving bus and you condition on already having waited \\(s\\) minutes?",
    "crumbs": [
      "Seminars",
      "Seminar 1"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction",
    "section": "",
    "text": "Welcome to the website for TECH3 Applied Statistics. We will use this website as a supplement to lectures. The website is ongoing development, so not all subjects will have content yet. Below you will find a detailed (preliminary) lecture plan for the Spring semester of 2026, link to the textbook and curriculum. The course description can be found here.\n\n\n\n\n\n\n\nWeek\nModule\nTuesday 12:15 -14:00\nThursday 12:15 - 14:00\n\n\n\n\n3\nModule 1\n13.01: Introduction to TECH3/Overview lecture 1 in Aud C\n15.01: Collaborative learning session 1 in Aud J\n\n\n4\nModule 1/2\n20.01: Overview lecture 2 in Aud C.\n22.01: Case session 1 in Aud S\n\n\n5\nModule 2\n27.01: Overview lecture 2 in Aud C\n29.01: Collaborative learning session 2 in Aud Q\n\n\n6\nModule 2\n03.02: Seminar 1 in Aud C\n05.02: Exercise session 2 in Aud Q\n\n\n7\nModule 2/3\n10.02: Overview lecture 3 in Aud C\n12.02: Case session 2 in Aud Q\n\n\n8\nModule 3\n17.02: Overview lecture 3 in Aud C\n19.02: Collaborative learning session 3 in Aud Q\n\n\n9\nModule 3\n24.02: Seminar 2 in Aud C.\n26.02: Case session 3 in Aud Q\n\n\n10\nModule 4\n03.03: Overview lecture 4 in Aud C\n05.03: Collaborative learning session 4 in Aud S\n\n\n11\nModule 4\n10:03: Overview lecture 4 in Aud C\n12.03: Case session 4 in Aud S\n\n\n12\nModule 5\n17.03: Overview lecture 5 i Aud C\n19.03: Collaborative learning session 5 in Aud S\n\n\n13\nModule 5\n24.03: Overview lecture 5 in Aud C\n26.03: Case session 5 in Aud S\n\n\n14\n\n31.03:No lecture (Easter)\n02.04: No lecture (Easter)\n\n\n15\nModule 5\n07.04: No lecture (oral exams in MAB1)\n09.04: Oracle session in Aud S\n\n\n16\nModule 5\n14.04: Seminar 3 in Aud C\n16.04: Practical information about exam in Aud C\n\n\n\n\n\n\n\n\n\nStatistical Thinking in the 21st Century\nPython Companion to Statistical Thinking in the 21st Century\n\n\n\n\nAll the material on this website, including chapters 1-10, 12-14, and 17 of Statistical Thinking in the 21st Century and Python Companion to Statistical Thinking in the 21st Century.\n\n\n\nUpon completing the course, the students can:\n\n\n\nUnderstand basic statistical theory and corresponding methods, and how to apply this knowledge in practical situations.\n\n\n\n\n\nExplore data using software that can summarize and visualize data.\nMaster basic probability theory.\nMake inferences about an entire population based on a sample of individuals from that population using both classical statistical methods and modern resampling techniques.\nDesign basic experiments, perform hypothesis testing, and quantify effects.\nMeasure relationships between both categorical and continuous variables.\nFit and evaluate regression models for both inference and prediction.\n\n\n\n\n\nIdentify and solve statistical problems.\nPerform basic data analysis using modern computer tools.\nPerform data-driven decision-making for a sustainable future."
  },
  {
    "objectID": "index.html#lecture-plan",
    "href": "index.html#lecture-plan",
    "title": "Introduction",
    "section": "",
    "text": "Week\nModule\nTuesday 12:15 -14:00\nThursday 12:15 - 14:00\n\n\n\n\n3\nModule 1\n13.01: Introduction to TECH3/Overview lecture 1 in Aud C\n15.01: Collaborative learning session 1 in Aud J\n\n\n4\nModule 1/2\n20.01: Overview lecture 2 in Aud C.\n22.01: Case session 1 in Aud S\n\n\n5\nModule 2\n27.01: Overview lecture 2 in Aud C\n29.01: Collaborative learning session 2 in Aud Q\n\n\n6\nModule 2\n03.02: Seminar 1 in Aud C\n05.02: Exercise session 2 in Aud Q\n\n\n7\nModule 2/3\n10.02: Overview lecture 3 in Aud C\n12.02: Case session 2 in Aud Q\n\n\n8\nModule 3\n17.02: Overview lecture 3 in Aud C\n19.02: Collaborative learning session 3 in Aud Q\n\n\n9\nModule 3\n24.02: Seminar 2 in Aud C.\n26.02: Case session 3 in Aud Q\n\n\n10\nModule 4\n03.03: Overview lecture 4 in Aud C\n05.03: Collaborative learning session 4 in Aud S\n\n\n11\nModule 4\n10:03: Overview lecture 4 in Aud C\n12.03: Case session 4 in Aud S\n\n\n12\nModule 5\n17.03: Overview lecture 5 i Aud C\n19.03: Collaborative learning session 5 in Aud S\n\n\n13\nModule 5\n24.03: Overview lecture 5 in Aud C\n26.03: Case session 5 in Aud S\n\n\n14\n\n31.03:No lecture (Easter)\n02.04: No lecture (Easter)\n\n\n15\nModule 5\n07.04: No lecture (oral exams in MAB1)\n09.04: Oracle session in Aud S\n\n\n16\nModule 5\n14.04: Seminar 3 in Aud C\n16.04: Practical information about exam in Aud C"
  },
  {
    "objectID": "index.html#literature",
    "href": "index.html#literature",
    "title": "Introduction",
    "section": "",
    "text": "Statistical Thinking in the 21st Century\nPython Companion to Statistical Thinking in the 21st Century"
  },
  {
    "objectID": "index.html#curriculum",
    "href": "index.html#curriculum",
    "title": "Introduction",
    "section": "",
    "text": "All the material on this website, including chapters 1-10, 12-14, and 17 of Statistical Thinking in the 21st Century and Python Companion to Statistical Thinking in the 21st Century."
  },
  {
    "objectID": "index.html#learning-outcomes",
    "href": "index.html#learning-outcomes",
    "title": "Introduction",
    "section": "",
    "text": "Upon completing the course, the students can:\n\n\n\nUnderstand basic statistical theory and corresponding methods, and how to apply this knowledge in practical situations.\n\n\n\n\n\nExplore data using software that can summarize and visualize data.\nMaster basic probability theory.\nMake inferences about an entire population based on a sample of individuals from that population using both classical statistical methods and modern resampling techniques.\nDesign basic experiments, perform hypothesis testing, and quantify effects.\nMeasure relationships between both categorical and continuous variables.\nFit and evaluate regression models for both inference and prediction.\n\n\n\n\n\nIdentify and solve statistical problems.\nPerform basic data analysis using modern computer tools.\nPerform data-driven decision-making for a sustainable future."
  },
  {
    "objectID": "datalab5.html",
    "href": "datalab5.html",
    "title": "Datalab 5",
    "section": "",
    "text": "Datalab 5",
    "crumbs": [
      "Datalabs",
      "Datalab 5"
    ]
  },
  {
    "objectID": "datalab3.html",
    "href": "datalab3.html",
    "title": "Datalab 3",
    "section": "",
    "text": "Datalab 3\nThe third datalab is about Monte Carlo simulation and bootstrapping the sampling distribution of the maximum likelihood estimator for the exponential distribution. The content of the datalab can be cloned using the following line in git bash:\ngit clone https://github.com/holleland/TECH3_datalab3\nThis will make a local copy of the files on the github repository at the current location of the git bash terminal.\nIf you dont want to use git bash, you can also open the github link in your browser and download the files “manually”.\nThe repository consists of three files:\n\ndatalab3.ipynb: The file for you to be working with.\ndatalab3_sol.ipynb: Suggestive solutions.\nenvironment.yml: Conda environment file.\n\nTo create the conda environment TECH3_student from the environment file, you can open an anaconda prompt and write:\nconda env create --file environment.yml\nAfter the environment has been set up, you can activate the conda environment and open Jupyter Notebook using\nconda activate TECH3_student\njupyter notebook\nFor further details on how to set up the conda environment, see the guides on the repository for TECH2\nOpen the file datalab3.ipynb, and go through the document. Write your own code and try to solve the problems yourself before consulting the solutions notebook.",
    "crumbs": [
      "Datalabs",
      "Datalab 3"
    ]
  },
  {
    "objectID": "datalab1.html",
    "href": "datalab1.html",
    "title": "Datalab 1",
    "section": "",
    "text": "Datalab 1\nThe first datalab is about summarizing and visualizing data. The content of the datalab can be forked or cloned using the following line in git bash:\ngit clone https://github.com/holleland/TECH3_datalab1\nThis will make a local copy of the files on the github repository at the current location of the git bash terminal.\nIf you dont want to use git bash, you can also open the github link in your browser and download the files “manually”.\nThe repository consists of three files:\n\ndatalab1.ipynb: The file for you to be working with.\ndatalab1_solutions.ipynb: Suggestive solutions.\nenvironment.yml: Conda environment file.\n\nTo create the conda environment TECH3_student from the environment file, you can open an anaconda prompt and write:\nconda env create --file environment.yml\nAfter the environment has been set up, you can activate the conda environment and open Jupyter Notebbok using\nconda activate TECH3_student\njupyter notebook\nFor further details on how to set up the conda environment, see the guides on the repository for TECH2\nOpen the file datalab1.ipynb, and go through the document. Write your own code where this is indicated. Try to solve the problems yourself before consulting the solutions notebook.",
    "crumbs": [
      "Datalabs",
      "Datalab 1"
    ]
  },
  {
    "objectID": "case-2.html",
    "href": "case-2.html",
    "title": "Case 2",
    "section": "",
    "text": "Case 2"
  },
  {
    "objectID": "calender.html",
    "href": "calender.html",
    "title": "Calender",
    "section": "",
    "text": "Calender\n\n\n\n\n\nWeek\nModule\nTuesday 12:15 -14:00\nThursday 12:15 - 14:00\n\n\n\n\n3\nModule 1\n13.01: Introduction to TECH3/Overview lecture 1 in Aud C\n15.01: Collaborative learning session 1 in Aud J\n\n\n4\nModule 1/2\n20.01: Overview lecture 2 in Aud C.\n22.01: Case session 1 in Aud S\n\n\n5\nModule 2\n27.01: Overview lecture 2 in Aud C\n29.01: Collaborative learning session 2 in Aud Q\n\n\n6\nModule 2\n03.02: Seminar 1 in Aud C\n05.02: Exercise session 2 in Aud Q\n\n\n7\nModule 2/3\n10.02: Overview lecture 3 in Aud C\n12.02: Case session 2 in Aud Q\n\n\n8\nModule 3\n17.02: Overview lecture 3 in Aud C\n19.02: Collaborative learning session 3 in Aud Q\n\n\n9\nModule 3\n24.02: Seminar 2 in Aud C.\n26.02: Case session 3 in Aud Q\n\n\n10\nModule 4\n03.03: Overview lecture 4 in Aud C\n05.03: Collaborative learning session 4 in Aud S\n\n\n11\nModule 4\n10:03: Overview lecture 4 in Aud C\n12.03: Case session 4 in Aud S\n\n\n12\nModule 5\n17.03: Overview lecture 5 i Aud C\n19.03: Collaborative learning session 5 in Aud S\n\n\n13\nModule 5\n24.03: Overview lecture 5 in Aud C\n26.03: Case session 5 in Aud S\n\n\n14\n\n31.03:No lecture (Easter)\n02.04: No lecture (Easter)\n\n\n15\nModule 5\n07.04: No lecture (oral exams in MAB1)\n09.04: Oracle session in Aud S\n\n\n16\nModule 5\n14.04: Seminar 3 in Aud C\n16.04: Practical information about exam in Aud C"
  },
  {
    "objectID": "5-videos.html",
    "href": "5-videos.html",
    "title": "Videos",
    "section": "",
    "text": "Videos"
  },
  {
    "objectID": "5-simpsons-paradox.html",
    "href": "5-simpsons-paradox.html",
    "title": "Simpson’s paradox",
    "section": "",
    "text": "Slides for “Simpson’s paradox”\n\n\n\nWhat is Simpson’s paradox?\nHow does it occur?",
    "crumbs": [
      "Modules",
      "Module 5: Measuring relationships and fitting models",
      "Simpson's paradox"
    ]
  },
  {
    "objectID": "5-simpsons-paradox.html#control-questions",
    "href": "5-simpsons-paradox.html#control-questions",
    "title": "Simpson’s paradox",
    "section": "",
    "text": "What is Simpson’s paradox?\nHow does it occur?",
    "crumbs": [
      "Modules",
      "Module 5: Measuring relationships and fitting models",
      "Simpson's paradox"
    ]
  },
  {
    "objectID": "5-prediction-models.html",
    "href": "5-prediction-models.html",
    "title": "Prediction models",
    "section": "",
    "text": "“Explain vs predict”\n“Prediction and overfitting”\n“Confidence and prediction intervals”\n“Cross validation”\n\n\n\n\n\nWhat is your goal if you are concerned about model assumptions being fulfilled?\nWhat is your goal if you are mostly concerned with out-of-sample performance?\nWhat is overfitting and how can we avoid it?\nWhich is widest - confidence interval of prediction interval?\nWhy do we need a train-test split?\nWhat is the end-goal of a cross validation routine?",
    "crumbs": [
      "Modules",
      "Module 5: Measuring relationships and fitting models",
      "Prediction models"
    ]
  },
  {
    "objectID": "5-prediction-models.html#slides",
    "href": "5-prediction-models.html#slides",
    "title": "Prediction models",
    "section": "",
    "text": "“Explain vs predict”\n“Prediction and overfitting”\n“Confidence and prediction intervals”\n“Cross validation”",
    "crumbs": [
      "Modules",
      "Module 5: Measuring relationships and fitting models",
      "Prediction models"
    ]
  },
  {
    "objectID": "5-prediction-models.html#control-questions",
    "href": "5-prediction-models.html#control-questions",
    "title": "Prediction models",
    "section": "",
    "text": "What is your goal if you are concerned about model assumptions being fulfilled?\nWhat is your goal if you are mostly concerned with out-of-sample performance?\nWhat is overfitting and how can we avoid it?\nWhich is widest - confidence interval of prediction interval?\nWhy do we need a train-test split?\nWhat is the end-goal of a cross validation routine?",
    "crumbs": [
      "Modules",
      "Module 5: Measuring relationships and fitting models",
      "Prediction models"
    ]
  },
  {
    "objectID": "5-linear-regression.html",
    "href": "5-linear-regression.html",
    "title": "Linear regression",
    "section": "",
    "text": "“Linear regression”\n“Least squares estimation”\n“Adding covariates”\n“Least squares estimators”\n“Correlation and regression”\n“Hypothesis testing on regression parameters”\n“Adding more covariates”\n“Adding interactions between variables”\n“Nonlinear linear regression”\n\n\n\n\n\nWhat is the interpretation of \\(\\beta_0\\) is a simple linear regression \\(Y=\\beta_0+\\beta_1 X+\\epsilon\\)?\nWhat is the interpretation of \\(\\beta_1\\)?\nWhat are we minimizing when estimating \\(\\beta_0\\) and \\(\\beta_1\\)?\nWhen is there a one-to-one relationship between the correlation between \\(X\\) and \\(Y\\) and the regression coefficient for \\(X\\)?\nWhat is the null hypothesis when we are looking at the default table from a linear regression in Python?\nIf \\(\\text{Height}_i=\\beta_0+\\beta_a \\text{Age}_i + \\beta_m\\text{Male}_i+\\epsilon_i\\), where \\(\\text{Male}_i\\) is 1 if the child is male, what is the intercept for male and females?\nIf \\(\\text{Height}_i=\\beta_0+\\beta_a \\text{Age}_i + \\beta_m\\text{Male}_i+\\beta_{a\\times m}\\text{Age}_i\\text{Male}_i+\\epsilon_i\\), where \\(\\text{Male}_i\\) is 1 if the child is male, what is the slope for male and females?\nHow can we implement a nonlinear relations in a linear regression?",
    "crumbs": [
      "Modules",
      "Module 5: Measuring relationships and fitting models",
      "Linear regression"
    ]
  },
  {
    "objectID": "5-linear-regression.html#slides",
    "href": "5-linear-regression.html#slides",
    "title": "Linear regression",
    "section": "",
    "text": "“Linear regression”\n“Least squares estimation”\n“Adding covariates”\n“Least squares estimators”\n“Correlation and regression”\n“Hypothesis testing on regression parameters”\n“Adding more covariates”\n“Adding interactions between variables”\n“Nonlinear linear regression”",
    "crumbs": [
      "Modules",
      "Module 5: Measuring relationships and fitting models",
      "Linear regression"
    ]
  },
  {
    "objectID": "5-linear-regression.html#control-questions",
    "href": "5-linear-regression.html#control-questions",
    "title": "Linear regression",
    "section": "",
    "text": "What is the interpretation of \\(\\beta_0\\) is a simple linear regression \\(Y=\\beta_0+\\beta_1 X+\\epsilon\\)?\nWhat is the interpretation of \\(\\beta_1\\)?\nWhat are we minimizing when estimating \\(\\beta_0\\) and \\(\\beta_1\\)?\nWhen is there a one-to-one relationship between the correlation between \\(X\\) and \\(Y\\) and the regression coefficient for \\(X\\)?\nWhat is the null hypothesis when we are looking at the default table from a linear regression in Python?\nIf \\(\\text{Height}_i=\\beta_0+\\beta_a \\text{Age}_i + \\beta_m\\text{Male}_i+\\epsilon_i\\), where \\(\\text{Male}_i\\) is 1 if the child is male, what is the intercept for male and females?\nIf \\(\\text{Height}_i=\\beta_0+\\beta_a \\text{Age}_i + \\beta_m\\text{Male}_i+\\beta_{a\\times m}\\text{Age}_i\\text{Male}_i+\\epsilon_i\\), where \\(\\text{Male}_i\\) is 1 if the child is male, what is the slope for male and females?\nHow can we implement a nonlinear relations in a linear regression?",
    "crumbs": [
      "Modules",
      "Module 5: Measuring relationships and fitting models",
      "Linear regression"
    ]
  },
  {
    "objectID": "5-intro.html",
    "href": "5-intro.html",
    "title": "TECH3 Applied statistics",
    "section": "",
    "text": "Measuring relationships and fitting models\n\nFocus: The difference between modelling continuous and categorical relationships.\nKey topics: Correlation, causation, linear regression, predictions, odds ratio, binary regression, chi-squared tests.\n\nSpecial Emphasis: Address overfitting using cross-validation.",
    "crumbs": [
      "Modules",
      "Module 5: Measuring relationships and fitting models",
      "Introduction"
    ]
  },
  {
    "objectID": "5-correlation-and-regression.html",
    "href": "5-correlation-and-regression.html",
    "title": "Correlation and regression",
    "section": "",
    "text": "Slides for “Correlation and regression”"
  },
  {
    "objectID": "5-continuous-relationships-correlation.html",
    "href": "5-continuous-relationships-correlation.html",
    "title": "Modeling continuous relationships: Correlation",
    "section": "",
    "text": "Slides for “Modeling continuous relationships: Correlation”\n\n\n\nIf an increase in X is associated with an increase in Y, what sign will the correlation coefficient have?\nIf an increase in Y is associated with an decrease in X, what sign will the correlation coefficient have?\nWhat type of dependence is described by correlations?\nWhat does zero correlation mean for the relationship between X and Y?\nWhat kind of distribution do we use in a hypothesis test of correlation?",
    "crumbs": [
      "Modules",
      "Module 5: Measuring relationships and fitting models",
      "Modeling continuous relationships: Correlation"
    ]
  },
  {
    "objectID": "5-continuous-relationships-correlation.html#control-questions",
    "href": "5-continuous-relationships-correlation.html#control-questions",
    "title": "Modeling continuous relationships: Correlation",
    "section": "",
    "text": "If an increase in X is associated with an increase in Y, what sign will the correlation coefficient have?\nIf an increase in Y is associated with an decrease in X, what sign will the correlation coefficient have?\nWhat type of dependence is described by correlations?\nWhat does zero correlation mean for the relationship between X and Y?\nWhat kind of distribution do we use in a hypothesis test of correlation?",
    "crumbs": [
      "Modules",
      "Module 5: Measuring relationships and fitting models",
      "Modeling continuous relationships: Correlation"
    ]
  },
  {
    "objectID": "5-categorical-relationships.html",
    "href": "5-categorical-relationships.html",
    "title": "Modeling categorical relationships",
    "section": "",
    "text": "Slides for “Modeling categorical relationships”\n\n\n\nWhat do we test when doing this kind of chi-squared test?\nWhat is the relationship between chi-squared distribution and a standard normal distribution?\nDo we reject the null hypothesis for small or large values of the chi-squared test statistic?",
    "crumbs": [
      "Modules",
      "Module 5: Measuring relationships and fitting models",
      "Modeling categorical relationships"
    ]
  },
  {
    "objectID": "5-categorical-relationships.html#control-questions",
    "href": "5-categorical-relationships.html#control-questions",
    "title": "Modeling categorical relationships",
    "section": "",
    "text": "What do we test when doing this kind of chi-squared test?\nWhat is the relationship between chi-squared distribution and a standard normal distribution?\nDo we reject the null hypothesis for small or large values of the chi-squared test statistic?",
    "crumbs": [
      "Modules",
      "Module 5: Measuring relationships and fitting models",
      "Modeling categorical relationships"
    ]
  },
  {
    "objectID": "4-videos.html",
    "href": "4-videos.html",
    "title": "Videos",
    "section": "",
    "text": "Videos",
    "crumbs": [
      "Modules",
      "Module 4: Designing studies, hypothesis testing, and quantifying effects",
      "Videos"
    ]
  },
  {
    "objectID": "4-significance-level.html",
    "href": "4-significance-level.html",
    "title": "The significance level as a long-run error rate",
    "section": "",
    "text": "Slides for “The significance level as a long-run error rate”\n\n\n\nWhat is the decision rule in hypothesis testing?\nWhat is a Type I error?\nWhat is the interpretation of the significance level in the Neyman-Pearson framework?\nIn what types of situations is this interpretation particularly useful?\nIf \\(\\alpha = 0.05\\), does that mean that 5% of significant results are false?",
    "crumbs": [
      "Modules",
      "Module 4: Designing studies, hypothesis testing, and quantifying effects",
      "The significance level as a long-run error rate"
    ]
  },
  {
    "objectID": "4-significance-level.html#control-questions",
    "href": "4-significance-level.html#control-questions",
    "title": "The significance level as a long-run error rate",
    "section": "",
    "text": "What is the decision rule in hypothesis testing?\nWhat is a Type I error?\nWhat is the interpretation of the significance level in the Neyman-Pearson framework?\nIn what types of situations is this interpretation particularly useful?\nIf \\(\\alpha = 0.05\\), does that mean that 5% of significant results are false?",
    "crumbs": [
      "Modules",
      "Module 4: Designing studies, hypothesis testing, and quantifying effects",
      "The significance level as a long-run error rate"
    ]
  },
  {
    "objectID": "4-power-analysis.html",
    "href": "4-power-analysis.html",
    "title": "Power analysis",
    "section": "",
    "text": "video",
    "crumbs": [
      "Modules",
      "Module 4: Designing studies, hypothesis testing, and quantifying effects",
      "Power analysis"
    ]
  },
  {
    "objectID": "4-power-analysis.html#control-questions",
    "href": "4-power-analysis.html#control-questions",
    "title": "Power analysis",
    "section": "Control questions",
    "text": "Control questions",
    "crumbs": [
      "Modules",
      "Module 4: Designing studies, hypothesis testing, and quantifying effects",
      "Power analysis"
    ]
  },
  {
    "objectID": "4-power-analysis.html#power-analysis-in-python",
    "href": "4-power-analysis.html#power-analysis-in-python",
    "title": "Power analysis",
    "section": "Power analysis in Python",
    "text": "Power analysis in Python\nStatistical power is the probability of detecting an effect if one truly exists, and it is defined as \\(\\text{Power} = 1-\\beta\\), where \\(\\beta\\) is the probability of a Type II error (false negative): Failing to reject \\(H_0\\) when \\(H_A\\) is true. High power means we are less likely to miss real effects.\nStatistical power is mainly affected by the following three factors:\n\nSample size\nEffect size\nSignificance level\n\nWe can only control 1 and 3. Imagine that we know the effect size is 0.75 and set the significance level to 5%. We can the calculate how big the sample needs to be for our test to have a power of 0.8.\nThis is how we do it in Python:\n\nimport statsmodels.stats.power as smp\nfrom scipy import stats\npower_analysis = smp.TTestIndPower()\nsample_size = power_analysis.solve_power(effect_size=0.75, \n                                         power=0.8, \n                                         alpha=0.05)\nprint(\"Sample size: \", round(sample_size,2))\n\nSample size:  28.9\n\n\nPlay with the inputs to the function. How does increasing the effect size, power or significance level influence the necessary sample size?",
    "crumbs": [
      "Modules",
      "Module 4: Designing studies, hypothesis testing, and quantifying effects",
      "Power analysis"
    ]
  },
  {
    "objectID": "4-hypothesis-testing.html",
    "href": "4-hypothesis-testing.html",
    "title": "The logic of null hypothesis statistical testing",
    "section": "",
    "text": "Slides for “The logic of null hypothesis statistical testing”\n\n\n\nHow do you translate a research question into a testable hypothesis?\nHow do you translate a hypothesis into null and alternative hypotheses?\nWhat data should be collected to test the hypothesis?\nWhat does a test statistic measure?\nWhat is a null distribution?\nHow is a p-value computed, and how should it be interpreted?\n\n… no scientific worker has a fixed level of significance at which from year to year, and in all circumstances, he rejects hypotheses; he rather gives his mind to each particular case in the light of his evidence and his ideas.\nSir Ronald A. Fisher (1956)\n\n\n\nMindless statistics by Gerd Gigerenzer, Journal of Socio-Economics",
    "crumbs": [
      "Modules",
      "Module 4: Designing studies, hypothesis testing, and quantifying effects",
      "The logic of null hypothesis statistical testing"
    ]
  },
  {
    "objectID": "4-hypothesis-testing.html#control-questions",
    "href": "4-hypothesis-testing.html#control-questions",
    "title": "The logic of null hypothesis statistical testing",
    "section": "",
    "text": "How do you translate a research question into a testable hypothesis?\nHow do you translate a hypothesis into null and alternative hypotheses?\nWhat data should be collected to test the hypothesis?\nWhat does a test statistic measure?\nWhat is a null distribution?\nHow is a p-value computed, and how should it be interpreted?\n\n… no scientific worker has a fixed level of significance at which from year to year, and in all circumstances, he rejects hypotheses; he rather gives his mind to each particular case in the light of his evidence and his ideas.\nSir Ronald A. Fisher (1956)\n\n\n\nMindless statistics by Gerd Gigerenzer, Journal of Socio-Economics",
    "crumbs": [
      "Modules",
      "Module 4: Designing studies, hypothesis testing, and quantifying effects",
      "The logic of null hypothesis statistical testing"
    ]
  },
  {
    "objectID": "4-exercises.html",
    "href": "4-exercises.html",
    "title": "Exercises",
    "section": "",
    "text": "For the following exercises, we will use the test statistic for one-sample t-test with null hypothesis \\(H_0:\\mu=\\mu_0\\): \\[T=\\frac{\\bar x-\\mu_0}{s/\\sqrt{n}},\\] where \\(n\\) is the sample size, \\(\\bar x\\) is the sample mean, \\(s\\) is the sample standard deviation and \\(\\mu_0\\) is the expected value under the null hypothesis.\n\n\nA company selling premium coffee beans wants to determine whether reducing the price of its product leads to a significant increase in the average number of bags sold per day. The marketing team conducted a test by lowering the price for a month in select stores and measuring the sales volume. They want to analyze whether the mean daily sales after the price reduction are significantly higher than before.\n\nSet up the null- and alternative hypothesis, and decide on a suitable significance level.\n\nBefore the price reduction, the mean daily sales were 200 bags. After the price reduction, a sample of 30 days of sales data showed a sample mean of 215 bags, with standard deviation 25 bags.\n\nCalculate the test statistic.\nFind the p-value. What is the conclusion?\n\n\n\nShow solutions\n\n\n\\(H_0: \\mu\\le 200\\) vs \\(H_1: \\mu&gt;200\\). We could allow for a relatively high significance level in this context, because rejecting a true null hypothesis (Type I error) will most likely not have large consequences, e.g. \\(\\alpha =0.1\\).\n\\(T=(215-200)/(25/\\sqrt{30})=3.29\\)\n\\(\\text{p-value}=P(T_{29}&gt;3.29)=0.00132\\). The p-value is very low, indicating that we may reject the null hypothesis at (almost) any significance level in (a). The company should continue with the reduced price.\n\n\nfrom scipy import stats\nprint(1-stats.t.cdf(3.29, 29))\n\n0.0013169979736814552\n\n\n\n\n\n\nA large company is trying to encourage employees to save more for retirement. Previously, employees had to actively sign up for the company’s pension savings plan (opt-in system). Recently, the company changed its policy so that employees are automatically enrolled in the plan but can opt out if they wish (opt-out system). Behavioral economists suggest that default options strongly influence decision-making, leading to higher participation rates in savings plans.\nThe HR department wants to test with a significance level of 5% whether the average monthly contribution to the retirement plan has increased significantly after implementing the opt-out system.\n\nSet up the null- and alternative hypothesis.\n\nBefore the change, the average monthly contribution was $250 per employee. After the policy change, a random sample of 50 employees showed: Sample mean of $270 and standard deviation of $90.\n\nCalculate the test statistic.\nFind the p-value. What is the conclusion?\n\n\n\nShow solutions\n\n\n\\(H_0: \\mu\\le 250\\) vs \\(H_1: \\mu&gt;250\\).\n\\(T=(270-250)/(90/\\sqrt{50})=1.57\\)\n\\(\\text{p-value}=P(T_{29}&gt;3.29)=0.0613\\). The p-value is higher than 5%, indicating that we may reject not the null hypothesis. The pension payments have not increased due to the change from opting in to opting out.\n\n\nfrom scipy import stats\nprint(1-stats.t.cdf(2.94, 49))\n\n0.002498152330066006\n\n\n\n\n\n\nA nutritionist wants to test whether a new diet plan leads to a significant reduction in average daily calorie intake. A sample of 40 individuals followed the diet for one month, and their average daily calorie intake was recorded.\nThe recommended daily intake before the diet was 2200 kcal. After one month, the sample had a mean intake of 2100 kcal with a standard deviation of 250 kcal.\n\nSet up the null- and alternative hypotheses.\nCalculate the test statistic.\nFind the p-value and interpret the result.\n\n\n\nShow solutions\n\n\n\\(H_0: \\mu\\geq 2200\\) vs \\(H_1: \\mu&lt;2200\\).\n\\[T=\\frac{2100-2200}{250/\\sqrt{40}}=-2.53\\]\n\\[\\text{p-value}=P(T_{39}&lt;-2.53)=0.0079\\]\n\nSince the p-value is lower than the conventional 5% significance level, we reject the null hypothesis, suggesting that the new diet plan significantly reduces daily calorie intake.\n\nfrom scipy import stats\nprint(stats.t.cdf(-2.53, 39))\n\n0.007778305846562239\n\n\n\n\n\n\nA smartphone manufacturer wants to determine whether the battery life of their new model is significantly different from the advertised 24 hours. A sample of 50 devices was tested, showing a mean battery life of 23.5 hours with a standard deviation of 1.2 hours.\n\nFormulate the hypotheses.\nCompute the test statistic.\nDetermine the p-value and state your conclusion.\n\n\n\nShow solutions\n\n\n\\(H_0: \\mu=24\\) vs \\(H_1: \\mu\\neq 24\\).\n\\[T=\\frac{23.5-24}{1.2/\\sqrt{50}}=-2.946\\]\n\\[\\text{p-value}=2 \\times P(T_{49}&lt;-2.94)=0.0049\\]\n\nSince the p-value is very small, we reject the null hypothesis, indicating that the battery life is significantly different from 24 hours.\n\nprint(2*stats.t.cdf(-2.946, 49))\n\n0.004914894166068594\n\n\n\n\n\n\nA fitness instructor claims that the average resting pulse rate of adults in their program is less than 70 beats per minute. A sample of 10 participants yields:\n\\[ \\text{Pulse Rates:} \\ 68, \\ 72, \\ 69,  \\ 71,  \\ 67,  \\ 65,  \\ 70,\\ 66,\\  68, \\ 64 \\]\nAssume approximate normality.\n\nState the null and alternative hypotheses.\nCompute the sample mean and standard deviation.\nConduct a one-sample t-test at the 5% significance level.\nWhat do you conclude?\nIf the sample had been twice as large with the same mean and standard deviation, how would the p-value likely change?\n\n\n\nShow solution\n\n\nThe statement of the instructor is that the mean is less than 70 bpm, so that becomes our alternative hypothesis, and the opposite case becomes the null hypothesis. We use \\(H_0: \\mu=70\\) as we are only interested in the edge case when testing.\n\n\\(H_0: \\mu = 70\\) \\(H_1: \\mu &lt; 70\\)\n\nWe simply compute using the given data and\n\n\\(\\bar{x} = \\frac{1}{10}\\sum^{10}_{i=1}x_i=68.0\\), \\(s=\\sqrt{\\frac{1}{n-1}\\sum^{10}_{i=1}\\left(x_i-\\bar{x}\\right)^2} \\approx 2.58\\)\n\n\\(t = \\frac{68 - 70}{2.58/\\sqrt{10}} \\approx -2.45\\)\nCritical value (df = n-1=9) at \\(\\alpha = 0.05\\) is approximately -1.833. This number can easily be found in a t-table, or by running some code to estimate it.\n\n\n### qt gives the quantile function for the\n# t distribution. We use 0.05 to find the \n# critical value as this is a one sided test. \n# 9 is just the degrees of freedom \nfrom scipy import stats\nprint(stats.t.ppf(-0.05, 9))\n\nnan\n\n### Underneath I am finding the p-value by \n# looking at the distribution function of \n# the t-distribution and plugging in the \n# found t statistic. \nprint(stats.t.cdf(-2.45, 9))\n\n0.0183783590582972\n\n\nSince -2.58 &lt; -1.833, we reject \\(H_0\\), and thus conclude that the average bpm is less than 70.\n\nWith a larger sample, the standard error would decrease, leading to a larger magnitude of the test statistic and a smaller p-value. This illustrates the effect of sample size on power.\n\n\n\n\n\nSuppose a manufacturer claims their machine fills soda bottles with 500 ml of soda. You suspect it underfills. You collect a sample of 12 bottles and measure:\n\nimport numpy as np\nx = np.array([498, 495, 497, 496, 499, 498, 494, 496, 495, 497, 496, 495])\n\nThis problem and it’s solutions were made in R, but feel free to use an equivalent.\n\nTest if the true mean is less than 500 ml using R.\nProvide the p-value and interpret the result.\nWhat assumption are you making about the distribution?\n\n\n\nShow solution\n\n\n### The t.test command let's you run a t.test by \n# simply adding the H_0 mean and specifying the\n# variant you'd liek to run. Here we test for \n# H_1: mu&lt;500 and H_0: mu=500 \nt_result = stats.ttest_1samp(x, popmean=500, alternative='less')\nprint(t_result)\n\nTtestResult(statistic=np.float64(-8.482095610516701), pvalue=np.float64(1.864450821881949e-06), df=np.int64(11))\n\n\n\nOutput might give \\(t \\approx -8.49\\), \\(p &lt; 0.001\\). Strong evidence the machine underfills.\nWe assume that the population distribution is approximately normal, as required for small-sample t-tests.\n\n\n\n\n\n\nFor the following exercises, we will use the test statistic for two-sample t-test with null hypothesis \\(H_0:\\mu_1=\\mu_2+d_0\\): \\[T=\\frac{\\bar x_1-\\bar x_2-d_0}{\\sqrt{s_1^2/n_1+s_2^2/n_2}},\\] where \\(n_1\\) and \\(n_2\\) are the sample sizes of the two samples, \\(\\bar x_1\\) and \\(\\bar x_2\\) are the sample means, \\(s_1\\) and \\(s_2\\) are the sample standard deviations and \\(d_0\\) is the difference in expected value under the null hypothesis (often \\(d_0=0\\)). The test statistic follows a t-distribution with degrees of freedom (df): \\[\\text{df}=\\frac{\\big(\\frac{s_1^2}{n_1}+\\frac{s_2^2}{n_2}\\big)^2}{\\frac{(s_1^2/n_1)^2}{n_1-1}+\\frac{(s_1^2/n_2)^2}{n_2-1}}\\]\n\n\nAn e-commerce company wants to test whether showing customer reviews on product pages increases the average number of purchases. Behavioral economics suggests that social proof (i.e., seeing others’ positive experiences) influences decision-making and can lead to higher sales.\nTo test this, the company randomly selected two groups of products: one group displayed customer reviews, and the other did not. They then compared the average daily sales per product between the two groups.\n\nSet up the null- and alternative hypothesis. Use 10% significance level.\nDiscuss how the test statistic will behave under the various hypothesis.\n\nThe experiment result in the following data:\n\nProducts with reviews: Sample mean daily sales = 120 units, Sample standard deviation = 30, Sample size = 40\nProducts without reviews: Sample mean daily sales = 105 units, Sample standard deviation = 28, Sample size = 40\n\n\nCalculate the test statistic and the degrees of freedom.\nFind the p-value. What is the conclusion?\n\n\n\nShow solutions\n\nLet \\(\\mu_1\\) be the expected daily sales with reviews and \\(\\mu_2\\) without reviewes.\n\n\\(H_0: \\mu_1\\le \\mu_2\\) vs \\(H_A: \\mu_1&gt;\\mu_2\\).\nUnder the null hypothesis, we expect the numerator of the test statistic to be close to zero, making the test statistic close to zero. Under the alternative hypothesis, we expect \\(\\bar x_1&gt;\\bar x_2\\) such that the numerator of \\(T\\) will be positive. Higher values will therefore favor the alternative hypothesis.\n\n\n\nimport numpy as np\ns1 = 30;  s2 = 28; n1 = 40; n2 = 40\nm1 = 120; m2 = 105;\ndf = (s1**2/n1 + s2**2/n2)**2/((s1**2/n1)**2/(n1-1)+(s2**2/n2)**2/(n2-1))\nt = (m1-m2)/np.sqrt(s1*s1/n1+s2*s2/n2)\nprint(\"df=\",df)\n\ndf= 77.63164160330635\n\nprint(\"T=\",t)\n\nT= 2.311799743112827\n\n\n\\(T=(102-105)/\\sqrt{30^2/40+28^2/40}=-0.462\\) \\[\\text{df}=\\frac{\\big(\\frac{s_1^2}{n_1}+\\frac{s_2^2}{n_2}\\big)^2}{\\frac{(s_1^2/n_1)^2}{n_1-1}+\\frac{(s_1^2/n_2)^2}{n_2-1}}\\]\n\n\\(\\text{p-value}=P(T_{29}&gt;0.462)=0.32\\). The p-value is lower than 10%, indicating that we may reject the null hypothesis. There is enough evidence to suggest that the reviews increase the sales at a 10% significance level. The company should show reviews of their products to increase sales.\n\n\nfrom scipy import stats\nprint(1-stats.t.cdf(t, df))\n\n0.011720746167147467\n\n\n\n\n\n\nA company is testing whether offering a performance-based bonus increases employee productivity. They implement a new bonus system in one office location (Group A), while another office continues without bonuses (Group B). After three months, they measure the average number of completed sales calls per week in both offices. The company wants you to determine if the bonus system leads to higher productivity (mean number of sales calls).\n\nFormulate the hypothesis problem including \\(H_0\\), \\(H_A\\) and setting a suitable significance level.\n\nGroup A (Bonus group):\n\nSample mean: 52 sales calls per week\nSample standard deviation: 8\nSample size: n = 35\n\nGroup B (No bonus group):\n\nSample mean: 48 sales calls per week\nSample standard deviation: 9\nSample size: n = 35\n\n\nCalculate the test statistic based on this information. Find the p-value.\nWhat is your conclusion and your recommendation to the company?\n\n\n\nShow solutions\n\n\n\\(H_0: \\mu_A =\\mu_B\\) vs \\(H_A: \\mu_A&gt;\\mu_B\\). In this case, the bonus scheme might be expensive, so we want to avoid type I errors (rejecting the null, when it is truly no difference in productivity). We use \\(\\alpha = 1\\%\\).\n\\(T=1.96\\) and p-value \\(=0.0267\\).\nThe p-value is low, indicating evidence for the alternative hypothesis, but we have set a significance level of 1%, for which the p-value exceeds. This would lead us to not recommend implementing the bonus, because it does not improve productivity to a high enough degree at this significance level.\n\n\nm1 = 52; s1 = 8; n1 = 35\nm2 = 48; s2 = 9; n2 = 35\nstats.ttest_ind_from_stats(m1,s1,n1,m2,s2,n2,alternative=\"greater\")\n\nTtest_indResult(statistic=np.float64(1.9652147377620703), pvalue=np.float64(0.026737892346682984))\n\n\n\n\n\n\nA university wants to determine if students who attend tutoring sessions perform better on exams than those who do not. Two independent samples of students were taken:\n\nTutoring group: Mean score = 78, Standard deviation = 10, Sample size = 30\nNon-tutoring group: Mean score = 72, Standard deviation = 12, Sample size = 35\n\n\nState the hypotheses.\nCalculate the test statistic and degrees of freedom.\nFind the p-value and interpret the result.\n\n\n\nShow solutions\n\n\n\\(H_0: \\mu_1 = \\mu_2\\) vs \\(H_A: \\mu_1 &gt; \\mu_2\\).\n\n\n\ns1 = 10; s2 = 12; n1 = 30; n2 = 35\nm1 = 78; m2 = 72;\ndf = (s1**2/n1 + s2**2/n2)**2 / ((s1**2/n1)**2/(n1-1) + (s2**2/n2)**2/(n2-1))\nt = (m1-m2) / ((s1**2/n1 + s2**2/n2)**0.5)\nprint(\"df=\", df)\n\ndf= 62.958820084919275\n\nprint(\"T=\", t)\n\nT= 2.1985812677253573\n\n\n\nThe computed p-value is \\(P(T_{63}&gt;t)=0.0158\\). The evidence thus points towards the conclusion that attending tutoring improves performance, due to a low p-value.\n\n\nprint(1-stats.t.cdf(t, df))\n\n0.01579534729247911\n\n\n\n\n\n\nA professor wants to compare exam scores between online and in-person students, as he suspects his students preform systematically better with online exams. The results:\n\nOnline: 82, 85, 88, 79, 84\nIn-person: 78, 74, 80, 76, 77\n\n\nSet up the hypotheses.\nConduct a two-sample t-test.\nAt the 0.05 significance level, what do you conclude?\n\n\n\n\nShow solution\n\n\n\\(H_0: \\mu_{\\text{online}} = \\mu_{\\text{in-person}}\\) \\(H_1: \\mu_{\\text{online}} &gt; \\mu_{\\text{in-person}}\\)\nCompute sample means and the sample variances. For sample 1, being the online results.\n\n\n### We use the same methods as always to find the sample mean and variance, but here is a simple way to do it in R. I made this problem have very small datasets so it would be feasible to do by hand, so please make sure you find teh same results. \nx_1 = np.array([82, 85, 88, 79, 84])\nmean_x1 = np.mean(x_1)\nvar_x1 = np.var(x_1,ddof=1)  \nprint(\"Mean:\", mean_x1)\n\nMean: 83.6\n\nprint(\"Variance:\", var_x1)\n\nVariance: 11.299999999999999\n\n\nI.e., we end up with \\(\\bar{x}_1=83.6\\) and \\(s^2_1=11.3\\)\nNow for sample 2, in person.\n\n### Same as above, just new names\nx_2 = np.array([78, 74, 80, 76, 77])\nmean_x2 = np.mean(x_2)\nvar_x2 = np.var(x_2,ddof=1)  \nprint(\"Mean:\", mean_x2)\n\nMean: 77.0\n\nprint(\"Variance:\", var_x2)\n\nVariance: 5.0\n\n\nI.e., we end up with \\(\\bar{x}_1=77\\) and \\(s^2_1=5\\)\nTest statistic:\n\\[\nt = \\frac{83.6 - 77.0}{\\sqrt{\\left(11.3/5+5/5\\right)}} \\approx 3.6554\n\\]\n\nThis is a one sided, and now we only need find the degrees of freedom to determine the proper critical value.\n\n\\[\n\\text{df}=\\frac{\\left(11.3/5+5/5\\right)^2}{\\frac{(11.3/5)^2}{4}+\\frac{(5/5)^2}{4}}=6.9602\\approx7\n\\] Using a table we can find our critical value when we let the df be 7 is 1.895.\nWith this we reject \\(H_0\\), significant difference.\nBeing more exact we can use some simple Python code.\n\n### Since we are using a \"greater than\" test we need to use the 0.95 quantile to find the proper critical value, but since the t-distribution is symmetric, 0.95 and 0.05 will only be separated by a minus sign. \nprint(stats.t.ppf(0.95, df = 6.9602))\n\n1.8962097832217037\n\n### The probability function finds the finds the probability of being under a value, we here take the complement to find the probability of being over 3.6554\nprint(1-stats.t.cdf(3.6554, 6.9602))\n\n0.004100373037377536\n\n\nUnderneath is an example of python code to do the exact same test.\n\nx_1 = np.array([82, 85, 88, 79, 84])\nx_2 = np.array([78, 74, 80, 76, 77])\n\n# One-sided t-test: H1 is that mean of x_1 &gt; mean of x_2\nresult = stats.ttest_ind(x_1, x_2, equal_var=False, alternative='greater')\n\nprint(\"t-statistic:\", result.statistic)\n\nt-statistic: 3.6554019191032916\n\nprint(\"p-value:\", result.pvalue)\n\np-value: 0.004100315140564395\n\n\n\n\n\n\nSuppose a two-sample t-test yields a p-value of 0.045 for comparing the means of two groups, using a significance level of 0.05.\n\nCan we say the means are “significantly different”?\nWhat if the significance level had been 0.01 instead?\nExplain how this relates to Type I error.\n\n\n\nShow solution\n\n\nYes — at \\(\\alpha = 0.05\\), we reject \\(H_0\\).\nNo — we would not reject at \\(\\alpha = 0.01\\).\nSignificance thresholds control the probability of a Type I error (rejecting a true \\(H_0\\)). A lower threshold is more conservative.\n\n\n\n\n\nTwo teachers use different methods to teach the same material, and they want to test if their methods give different average results. Students are randomly assigned to either class.\n\nGroup A (n = 10): Mean = 78, SD = 6\nGroup B (n = 14): Mean = 73, SD = 10\n\n\nState the hypotheses.\nCalculate the Welch t-statistic.\nEstimate the degrees of freedom.\nTest at the 5% level: is there a significant difference in mean scores?\n\n\n\nShow solution\n\n\nWe have no indication of directionality for this test. Thus we end up with the following null and alternative hypotheses.\n\\(H_0: \\mu_A = \\mu_B\\) \\(H_1: \\mu_A \\ne \\mu_B\\)\nWe can plug i the means, square the standard deviatons and plug in the numbers in each sample for the test statistic. \\[\nt = \\frac{78 - 73}{\\sqrt{6^2/10 + 10^2/14}} = \\frac{5}{\\sqrt{3.6 + 7.14}} = \\frac{5}{\\sqrt{10.74}} \\approx 1.53\n\\]\n\n\n\\[\ndf \\approx \\frac{(10.74)^2}{(3.6^2/9 + 7.14^2/13)} \\approx 21.51\n\\]\n\nAt \\(df = 22\\), two-tailed 5% critical t ≈ ±2.074. (found in table for critical values) Since 1.52 &lt; 2.08, we fail to reject \\(H_0\\).\n\nUsing Python code we can also find the exact and approximate df.\n\n## lower and upper bound for approximate df\nstats.t.ppf(0.025, 22)\n\nnp.float64(-2.073873067904015)\n\nstats.t.ppf(0.975, 22)\n\nnp.float64(2.0738730679040147)\n\n## for exact df\nstats.t.ppf(0.025, 21.51)\n\nnp.float64(-2.076615611268758)\n\nstats.t.ppf(0.975, 21.51)\n\nnp.float64(2.076615611268758)\n\n\n## Note that the lower and upper bounds have the same absolute value. This is a consequence of the t-distribution being symmetric. \n\n\n\n\n\nYou are given two samples:\n\nA = np.array([85, 87, 83, 84, 89, 91])\nB = np.array([78, 77, 79, 81, 76, 75, 80])\n\n\nConduct a two-sample t-test assuming unequal variances.\nReport the t-statistic, degrees of freedom, and p-value.\nInterpret the result at \\(\\alpha=0.05\\).\nDoes the output indicate anything about equality of variances?\n\n\n\nShow solution\n\n\nWe can use a simplePythoncommand for this\n\n\nresult = stats.ttest_ind(A, B, equal_var=False)\nprint(result)\n\nTtestResult(statistic=np.float64(5.666666666666667), pvalue=np.float64(0.00033388838046637467), df=np.float64(8.797264682220435))\n\n\n\nFrom, the test summary we can just read\n\n\\[\nt=5.6667 \\quad\ndf=8.793 \\quad\n\\text{p-value}=0.0003339\n\\]\n\nStrong evidence against \\(H_0\\) in form of such a low p-value, this indicates that the two samples are different at a 5 percent significance level.\nWelch’s t-test does not require equal variances and is safer when variances are not assumed equal.\n\n\n\n\n\n\nLet \\(p\\) denote a proportion and \\(p_0\\) be the proportion under the null hypothesis. We can then test the null hypothesis \\(H_0: p=p_0\\) against an alternative hypothesis using the test statistic \\[Z = \\frac{\\hat p-p_0}{\\sqrt{p_0(1-p_0)/n}},\\] which is approximately normally distributed when \\(n\\) is sufficiently large and the probabilies are not too small (often \\(np&gt;5\\) is used as a rule of thumb).\n\n\nA retail chain is considering phasing out cash payments in favor of digital transactions. Management wants to determine whether at least 60% of customers are already using mobile payment apps (such as Apple Pay or Google Pay) when shopping. If the proportion is significantly higher than 60%, they may move forward with reducing cash payment options.\n\nSet up the null hypothesis and alternative hypothesis. Discuss how the test statistic will behave under the various hypotheses.\n\nThe retail chain conducts a survey where they ask customers whether they used a mobile payment app for their most recent purchase. Out of the 400 customers asked, 260 used mobile payment apps.\n\nCalculate \\(Z\\) based on this information. What is the p-value?\nWhat does the p-value tell you about people’s payment preferences?\n\n\n\nShow solutions\n\n\n\\(H_0: p=0.6\\) vs \\(H_A: \\mu&gt;0.6\\). The numerator of \\(Z\\) will be expected close to 0 if the null is zero, making Z close to zero. If \\(H_A\\) is true, \\(\\widehat p\\) will be expected larger than 0.6, making Z positive. Large values of \\(Z\\) will favor \\(H_A\\).\n\\(Z = (0.65-0.6)/\\sqrt{0.6\\cdot 0.4/400}=2.04\\) and p-value\\(=P(Z&gt;2.04)=0.0206\\).\n\n\nphat = 260/400\nz = (phat-0.6)/np.sqrt(0.6*(1-0.6)/400)\nprint(\"Z=\", z)\n\nZ= 2.041241452319317\n\nprint(\"Pvalue: \", 1-stats.norm.cdf(z))\n\nPvalue:  0.02061341666858174\n\n\n\nThe p-value is low, which indicates evidence against \\(H_0\\). It indicates that the proportion of mobile app payments is above 60%.\n\n\n\n\n\nA sustainable fashion brand wants to know whether at least 30% of consumers are willing to pay a premium for eco-friendly clothing. If the proportion is significantly higher than 30% with significance level 10%, they will launch a new premium-priced, eco-friendly clothing line.\nThey conduct a market survey where respondents indicate whether they would be willing to pay 10% more for sustainably produced clothing.\n\nSet up the null- and alternative hypothesis.\n\nThe market survey got 500 respondents, and out of those, 166 were willing to pay 10% more for sustainable clothing.\n\nCalculate \\(Z\\) based on this information. What is the p-value?\nWhat would your recommendation to the fashion brand be?\n\n\n\nShow solutions\n\n\n\\(H_0: p\\le 30\\%\\) vs \\(H_A: p&gt;30\\%\\).\n\\(\\hat p=166/500= 33.2\\%\\), \\(z=(0.332-0.3)/\\sqrt{0.3*0.7/500}=1.56\\) and \\(P(Z&gt;z)=P(Z&gt;1.56)=1-\\Phi(1.56)=0.0592\\).\nAt a 10% signifance level, we would reject the null hypothesis, since the p-value of 0.06 is smaller than 0.1. At this significance level, we would recommend the brand to go ahead with the launch of their eco-friendly clothing.\n\n\nphat = 166/500\nz = (phat-0.3)/np.sqrt(0.3*(1-0.3)/500)\nprint(\"Z=\", z)\n\nZ= 1.5614401167176546\n\nprint(\"Pvalue: \", 1-stats.norm.cdf(z))\n\nPvalue:  0.05920997135406203\n\n\n\n\n\n\nA public transportation agency wants to determine whether at least 70% of residents support a new subway line expansion. A survey of 600 residents finds that 435 support the expansion.\n\nFormulate the hypotheses.\nCompute the test statistic and p-value.\nWhat is the conclusion?\n\n\n\nShow solutions\n\n\n\\(H_0: p = 0.7\\) vs \\(H_A: p &gt; 0.7\\).\n\\[Z=\\frac{0.725-0.7}{\\sqrt{0.7\\cdot 0.3/600}}=1.63\\]\n\np-value \\(=P(Z&gt;1.63)=0.0907\\)\n\np_hat = 435/600\nz = (p_hat-0.7) / ( (0.7*0.3/600)**0.5 )\nprint(\"Z=\", z)\n\nZ= 1.336306209562123\n\nprint(\"p-value=\", 1-stats.norm.cdf(z))\n\np-value= 0.09072460386070991\n\n\n\nA p-value of 9% indicates that the evidence against the null hypothesis is not very strong, and at a 5% significance level, we would not reject the null hypothesis.\n\n\n\n\n\nA factory claims that at most 5% of products are defective. In a random sample of 80 products, 7 were defective.\n\nTest whether the true defect rate is higher than claimed at the 5% level.\nCompute the test statistic.\nConclude and interpret.\nShould we consider this test trustworthy\n\n\n\nShow solution\n\n\nSince we want to explore if the proportion of defectives is higher than 5%, let’s make that our alternative hypothesis.\n\\(H_0: p = 0.05\\),\n\n\\(H_1: p &gt; 0.05\\)\n\n\n\n\\[\n\\hat{p} = \\frac{7}{80} = 0.0875,\\quad z = \\frac{0.0875 - 0.05}{\\sqrt{0.05(0.95)/80}} \\approx 1.54\n\\]\n\n\n\nSince our alternative hypothesis is that that \\(p&gt;0.05\\) we would need z to be higher than some critical value at the 5% level. Since we are using a one-sided test here, we find in a table that the critical value is given by \\(z\\approx 1.645\\). We can clearly tell that our test-statistic is not hihgh enough and this we won’t discard \\(H_0\\) in this case. Underneath I have done some computation giving a more exact critical value and finding the p-value is about 6 percent.\n\nprint(stats.norm.ppf(0.95))\n\n1.6448536269514722\n\nprint(1-stats.norm.cdf(1.54))\n\n0.06178017671181191\n\n\n\n\n\nThe common rule of thumb is \\(np&gt;5\\), and since \\(n\\hat{p}=7\\) in this case, we find little reason for doubt in this case.\n\n\n\n\n\nLet \\(p_1\\) and \\(p_2\\) denote two proportions. We can then test the null hypothesis \\(H_0: p_1=p_2\\) against an alternative hypothesis using the test statistic \\[Z = \\frac{\\hat p_1-\\hat p_2}{\\sqrt{p_\\text{pool}(1-p_\\text{pool})(1/n_1+1/n_2)}},\\] where \\[p_\\text{pool}=\\frac{n_1\\widehat p_1+n_2\\widehat p_2}{n_1+n_2}.\\] The test statistic Z is approximately normally distributed.\n\n\nAn insurance company is testing how the way they frame their sales pitch affects customers’ willingness to buy a travel insurance policy. They randomly assign potential customers into two groups:\n\nLoss-framed message: “If you don’t buy travel insurance, you could lose thousands in unexpected expenses.”\nGain-framed message: “If you buy travel insurance, you’ll have peace of mind and financial protection.”\n\nBehavioral economics suggests that loss aversion makes people more sensitive to potential losses than gains, meaning the loss-framed message might lead to higher purchase rates.\n\nSet up the null- and alternative hypothesis.\nDiscuss how the test statistic will behave under the various hypothesis.\n\nThe experiment result in the following data:\n\nLoss-framed message: 200 customers surveyed, 45% purchased insurance\nGain-framed message: 200 customers surveyed, 35% purchased insurance\n\n\nCalculate the test statistic and the p-value.\nConclude the test.\n\n\n\nShow solutions\n\nLet \\(p_L\\) denote proportion of purchases receiving a loss-framed message and \\(p_G\\) denote proportion of purchases receiving a gain-framed message. a) \\(H_0: p_L=p_G\\) vs \\(H_A: p_L&gt;p_G\\).\n\nWe use \\(p_1=p_L\\) and \\(p_2 =p_G\\) in refence to the formula for the test statistic. If the null hypothesis is true, Z is expected to be close to zero since the numerator is expected to be near 0. If the alternative hypothesis is true, we expect \\(\\hat p_L&gt;\\hat p_G\\), giving a positive numerator. Thus, large values of Z will favor the alternative hypothesis.\n\n\n\npL = 0.45\npG = 0.35\nnL = 200\nnG = 200\npPool = (nL*pL + nG*pG)/(nL+nG)\nz = (pL-pG)/np.sqrt(pPool*(1-pPool)*(1/nL+1/nG))\nprint(\"Z=\",z)\n\nZ= 2.041241452319316\n\nprint(\"pvalue=\",1-stats.norm.cdf(z))\n\npvalue= 0.02061341666858174\n\n\n\nThe p-value is low (around 2%). This indicates that the probability of making a Type I error is low and favor rejecting the null hypothesis. In that sense, the evidence points towards people being willing to pay to avoid loss than they are to potentially gain.\n\n\n\n\n\nAn online retailer wants to determine whether offering free shipping increases the proportion of customers who complete their purchases. They run an A/B test, where one group of customers sees free shipping on all orders (Group A), while another group sees the standard shipping fees (Group B). After a week, they compare the proportion of customers who completed a purchase in each group. The retailer asks you to use 5% significance level.\n\nFormulate the hypothesis problem.\n\nFor the two groups, the retailer gathered the following data:\nGroup A (Standard shipping):\n\nSample size: 469\nCompleted purchases: 191\n\nGroup B (Free shipping)\n\nSample size: 483\nCompleted purchases: 231\n\n\nPerform the test: Find Z and the p-value.\nWhat is your conclusion?\nThe chief economist in the company has taken a statistics course and learned about the difference between statistical significance and economical significance. How would this distinction influence your recommendation to the company?\n\n\n\nShow solutions\n\n\n\\(H_0: p_A=p_B\\) vs \\(H_A: p_A&lt;p_B\\).\n\n\n\nnA = 469\nnB = 483\npA = 191/nA\npB = 231/nB\nprint(pB-pA)\n\n0.07101140261425792\n\npPool = (nA*pA + nB*pB)/(nA+nB)\nz = (pA-pB)/np.sqrt(pPool*(1-pPool)*(1/nA+1/nB))\nprint(\"Z=\",z)\n\nZ= -2.2050192819722274\n\nprint(\"pvalue=\",stats.norm.cdf(z))\n\npvalue= 0.01372637075205902\n\n\n\nAt a 5% significance level, the free shipping group have a higher completing rate than the standard shipping group.\nHere the effect size \\(|\\widehat p_A-\\widehat p_B| = 7\\%\\), that is 7 percentage points higher rate of completing the order if the shipping is free. This increase in completed orders increases the volume, but the free shipping will reduce the margin on each order. We would need more information to assess whether this specific experiment that resulted in a statistically significant increase in volume, also resulted in a economically significant increase of profits.\n\n\n\n\n\nA company is testing whether a new website layout increases the proportion of users who complete a purchase. They conduct an A/B test with the following results:\n\nNew layout: 500 users, 210 completed a purchase\nOld layout: 500 users, 180 completed a purchase\n\n\nFormulate the hypotheses.\nCompute the test statistic and p-value.\nInterpret the result.\n\n\n\nShow solutions\n\n\n\\(H_0: p_1 = p_2\\) vs \\(H_A: p_1 &gt; p_2\\).\n\n\n\nfrom scipy import stats\nn1, n2 = 500, 500\np1, p2 = 210/n1, 180/n2\np_pool = (210 + 180) / (n1 + n2)\nz = (p1 - p2) / ((p_pool * (1 - p_pool) * (1/n1 + 1/n2)) ** 0.5)\nprint(\"Z=\", z)\n\nZ= 1.9450198311991271\n\nprint(\"p-value=\", 1-stats.norm.cdf(z))\n\np-value= 0.025886295779031898\n\n\n\nSince the p-value is low, this gives reason to doubt the null hypothesis and indicates that the new layout do increase the proportion of customers placing an order.\n\n\n\n\n\nIn a clinical trial, 35 of 50 patients in group A responded to a vaccine. In group B, 40 of 60 patients responded.\n\nState the hypotheses\nCompute the test statistic\nConclude on the test\nInterpret the result.\n\n\n\nShow solution\n\n\n\\(H_0: p_A = p_B\\), \\(H_a: p_A \\ne p_B\\)\n\n\n\\[\n\\hat{p}_1 =\\frac{35}{50}= 0.70,\\quad \\hat{p}_2=\\frac{40}{60} = 0.6667,\\quad \\hat{p}_{\\text{pooled}} = \\frac{35+40}{110} = 0.6818\n\\]\n\\[\nz = \\frac{0.70 - 0.6667}{\\sqrt{0.6818(1 - 0.6818)(1/50 + 1/60)}} \\approx 0.373\n\\]\n\nWe know that test statistic should be normally distributed in this case. To reject the two sided test we would need \\(|z|&gt;1.96\\), which we clearly see isn’t the case, and we thus fail to reject \\(H_0\\).\n\nLet’s find the p-value with Python to show this clearly.\n\n## Upper and lower bounds\nprint(stats.norm.ppf(0.025))\n\n-1.9599639845400545\n\nprint(stats.norm.ppf(0.975))\n\n1.959963984540054\n\n## Computing p value\nprint(1-stats.norm.cdf(0.373)+stats.norm.cdf(-0.373))\n\n0.7091484439599731\n\n\n\\(p \\approx 0.70\\), not significant.\n\nThere is no evidence to suggest a difference in proportions.\n\n\n\n\n\n\n\n\n\nWhat are the two parameters that define a normal distribution?\nWhat does the empirical rule (68-95-99.7 rule) state about the normal distribution?\n\n\n\nShow solution\n\n\nA normal distribution is fully defined by its mean (μ) and standard deviation (σ).\nThe normal distribution is a symmetric distribution around its mean, \\(\\mu\\). The rule gives an approximate measure of what proportion of the data that lies within a certain range of the mean.\n\n\n~68% of data lie within 1σ of the mean\n~95% within 2σ\n~99.7% within 3σ\n\nThis helps assess spread and typicality.\n\n\n\n\nGiven that \\(X \\sim \\mathcal{N}(100, 15^2)\\) (i.e. \\(X\\) is normally distributed with mean \\(\\mu=100\\) and standard deviation \\(\\sigma=15\\)), calculate:\n\nThe probability of \\(X &lt; 100\\). Why do we not need to consider the standard deviation when calculating this?\nThe approximate probability that \\(X &lt; 115\\)\nUse a z-transformation to b) to standard normal, and compute using a calculator. Let \\(Z\\sim \\mathcal{N}(0,1)\\), we call that a standard normal distribution. We also have that\n\n\\[\nZ=\\frac{X-\\mu}{\\sigma}\n\\]\n\n\nShow solution\n\n\nWe know that the normal distribution is symmetric around the mean, i.e. we have a 50-50 chance of finding an \\(X\\) above or below the mean. The mean here is 100, and was such we get\n\n\\[\nP(X&gt;100)=\\frac{1}{2}=0.5\n\\]\n\nWe can use the 68-95-99.7 rule to find the approximate probability. 115 is one standard deviation greater than the mean. That means we have to account for half of the 68% from the rule. I.e.\n\n\\[\nP(X\\in(100,115))=\\frac{0.68}{2}\\approx0.34\n\\] Now let’s also account for the cases where \\(X\\leq 100\\)\n\\[\nP(X&lt;115)=P(X\\leq100)+P(X\\in(100,115))\\approx0.5+0.34=0.84\n\\] Note: Since the normal distribution is continuous \\(P(X&lt;x)=P(X\\leq x)\\)\n\n\\[\nP(X&lt;115)=P\\left(\\frac{Z-100}{15}&lt;\\frac{115-100}{15}\\right)=P(Z&lt;1)=0.8413\n\\]\n\nInterpretation: There’s about an 84.13% chance that \\(X\\) will be below 115, which is equivalent to \\(Z\\) being below 1.\n\n\n\n\nSuppose a random variable \\(X \\sim \\mathcal{N}(\\mu, \\sigma^2)\\). Show that:\n\\[\nP(a &lt; X &lt; b) = P\\left( \\frac{a - \\mu}{\\sigma} &lt; Z &lt; \\frac{b - \\mu}{\\sigma} \\right)\n\\]\n\n\nShow solution\n\nThis transformation uses the standardization:\n\\[\nZ = \\frac{X - \\mu}{\\sigma}\n\\]\nBy applying this, we shift any normal distribution to the standard normal. We need only substitute and solve like a regular inequality, and the equivalence becomes clear.\n\\[\nP(a &lt; X &lt; b) = P\\left( \\frac{a - \\mu}{\\sigma} &lt; Z &lt; \\frac{b - \\mu}{\\sigma} \\right)\n\\]\n\n\n\n\n\nUse the symmetry of the standard normal distribution to explain why:\n\n\\[\nP(Z &gt; 1.96) = P(Z &lt; -1.96)\n\\]\n\nCalculate \\(P(|Z| &gt; 1.96)\\)\n\n\n\nShow solution\n\n\nThe standard normal is symmetric about 0, so:\n\n\\[\nP(Z &gt; c) = P(Z &lt; -c)\n\\] This equality comes from the logical fact of us a random varabiable being just as unlikely to take a value beyond a certain distance from the mean, no matter if the distance is in a positive of a negative directoion.\n\n\n\n\\[\nP(|Z| &gt; 1.96) =P(Z&lt;-1.96))+P(Z&gt;1.96)=2P(Z&lt;-1.96)\\approx 2*0.025=0.05\n\\]\n\n\n\n\n\nSketch or describe how the t-distribution changes as degrees of freedom increase.\nWhat does the t-distribution converge to?\n\n\n\nShow solution\n\n\nWith low degrees of freedom, the t-distribution is flatter and has fatter tails. As df increases, it becomes more peaked. The t-distribution also always has a mean of 0, and is symmetric around 0.\nAs df → ∞, the t-distribution approaches the standard normal distribution.\n\nWe can illustrate this quite simply without going into the ideas fully.\nLet first \\(X_1,\\dots,X_n\\sim i.i.d. \\mathcal{N}(\\mu, \\sigma^2)\\), and then let\n\\[ \\bar{X}=\\frac{1}{n}\\sum^n_{i=1}X_i, \\quad S^2=\\frac{1}{n-1}\\sum^n_{i=1}(X_i-\\bar{X})^2\n\\] It can then be shown that \\[ \\frac{\\bar{X}-\\mu}{\\sigma/\\sqrt{n}}\\sim\\mathcal{N}(0,1), \\quad \\frac{\\bar{X}-\\mu}{S/\\sqrt{n}}\\sim t\\_{n-1} \\] where \\(t_{n-1}\\) indicates a t-distribution with \\(df=n-1\\). Clearly these are very similar statements. All we need to recall now is that \\(S^2\\) is and unbiased and consistent estimator of \\(\\sigma^2\\). I.e.\n\\[\n\\lim_{n\\rightarrow\\infty}S^2=\\sigma^2 \\quad  \\Rightarrow \\quad  \\frac{\\bar{X}-\\mu}{S/\\sqrt{n}}\\sim t_{n-1} \\rightarrow \\frac{\\bar{X}-\\mu}{\\sigma/\\sqrt{n}}\\sim\\mathcal{N}(0,1).\n\\]\n\n\n\n\nA population has a skewed distribution with mean 10 and SD 5. Suppose we take samples of size 50. You can assume the samples are iid.\n\nWhat is the expected distribution of the sample mean?\nUse CLT to approximate \\(P(\\bar{X} &gt; 11)\\)\n\n\n\nShow solution\n\n\nCLT tells us that \\(\\bar{X} \\sim \\mathcal{N}\\left(10, \\frac{5}{\\sqrt{50}}\\right)\\)\n\n\n\\[\nZ = \\frac{11 - 10}{5/\\sqrt{50}} \\approx \\frac{1}{0.707} \\approx 1.41\n\\Rightarrow P(Z &gt; 1.41) \\approx 0.079\n\\]\n\n\n\n\nExplain when you should use the standard normal vs. the t-distribution in practice.\n\n\nShow solution\n\nUse the t-distribution if:\n\nPopulation standard deviation is unknown\nSample size is small (typically n &lt; 30)\n\nUse standard normal if:\n\nPopulation SD is known, or n is large, such that CLT becomes viable.\n\n\n\n\n\n\nWhy does the t-distribution have wider tails than the normal?\n\n\n\nWhat implication does this have on statistical testing?\n\n\n\nShow solution\n\n\nBecause of the extra uncertainty in estimating the population SD from the sample.\nWider tails mean larger critical values, so you’re less likely to reject \\(H_0\\) with small samples.\n\n\n\n\n\nUse Python (or something similar) to plot the density curves of \\(t_3, t_{10}, t_{30}\\), and standard normal.\n\nWhat do you observe as df increases?\nWhich distribution has the heaviest tails?\n\n\n\nShow solution\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import t, norm\n\n# x-values for plotting\nx = np.linspace(-4, 4, 500)\n\n# Plot t-distributions\nplt.plot(x, t.pdf(x, df=3), label=\"t3\", linewidth=2);\nplt.plot(x, t.pdf(x, df=10), label=\"t10\");\nplt.plot(x, t.pdf(x, df=30), label=\"t30\");\n\n# Plot standard normal\nplt.plot(x, norm.pdf(x), 'r--', label=\"N(0,1)\");\n\n# Legend and labels\nplt.legend(loc=\"upper right\");\nplt.ylabel(\"Density\");\nplt.title(\"t-distributions vs. Normal distribution\");\nplt.grid(True);\nplt.show()\n\n\n\n\n\n\n\n\n\nHeavier tails for small df. As df increases, the t-distribution converges to standard normal. This is just like we have discussed in the previous problems.\n\n\n\n\n\nLet \\(Z\\sim\\mathcal{N}(0,1)\\) Use Python to compute:\n\n\\(P(Z &gt; 1.96)\\)\n\\(P(-1.64 &lt; Z &lt; 1.64)\\)\n\\(P(X &gt; 120)\\), if \\(X \\sim \\mathcal{N}(100, 15^2)\\)\n\n\n\nShow solution\n\n\nprint(\"a) \", 1-stats.norm.cdf(1.96))\n\na)  0.024997895148220484\n\nprint(\"b) \", stats.norm.cdf(1.64) - stats.norm.cdf(-1.64))\n\nb)  0.8989948330517925\n\nprint(\"c) \", 1-stats.norm.cdf(120, loc = 100, scale = 15))\n\nc)  0.09121121972586788\n\n\n\n\n\n\nSimulate the sample mean of 1000 samples of size 5, 30, and 100 from an exponential distribution.\n\nPlot the histograms.\nComment on convergence to normality.\n\n\n\nShow solution\n\n\n\n\n\nnp.random.seed(42)\n\n# Function to generate 1000 sample means of size n\ndef sample_means(n):\n    return [np.mean(np.random.exponential(scale=1, size=n)) for _ in range(1000)]\n\n# Sample sizes\nsample_sizes = [5, 30, 100]\n\n# Plot histograms\nfor i, n in enumerate(sample_sizes, 1):\n    plt.figure();\n    plt.hist(sample_means(n), bins=30, range=(0, 5), color='skyblue', edgecolor='black');\n    plt.title(f'n = {n}');\n    plt.xlabel('Sample Mean');\n    plt.ylabel('Frequency');\n    plt.grid(True);\n    plt.show();\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs sample size increases, the distribution of the sample mean approaches a normal shape, even though the exponential is skewed.\n\n\n\n\n\nGenerate t-statistics using normal data and see how the distribution changes.\n\nGenerate 10,000 t-statistics from n = 5, 10, 30\nPlot and compare to standard normal\n\n\n\nShow solution\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\n# Function to generate 10,000 t-statistics from samples of size n\ndef gen_t(n):\n    t_stats = []\n    for _ in range(10000):\n        x = np.random.normal(loc=0, scale=1, size=n)\n        t_stat = np.mean(x) / (np.std(x, ddof=1) / np.sqrt(n))\n        t_stats.append(t_stat)\n    return np.array(t_stats)\n\n# Sample sizes to try\nsample_sizes = [5, 10, 30]\nbins = [30, 30, 20]\n\n# Plot histograms and overlay standard normal density\nx = np.linspace(-4, 4, 500)\nnormal_density = norm.pdf(x)\n\nfor n, b in zip(sample_sizes, bins):\n    t_vals = gen_t(n);\n    plt.figure();\n    plt.hist(t_vals, bins=b, density=True, color='lightblue', edgecolor='black');\n    plt.plot(x, normal_density, 'r--', label='Standard Normal');\n    plt.title(f\"Sampling Distribution of t-stat (n={n})\");\n    plt.xlabel(\"t-statistic\");\n    plt.ylabel(\"Density\");\n    plt.xlim(-4, 4);\n    plt.legend();\n    plt.grid(True);\n    plt.show();\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe distribution of the t-statistic becomes more like the normal as n increases.\n\n\n\n\nCompute using R:\n\n\\(P(T_5 &gt; 2.015)\\)\n\\(P(|T_{10}| &gt; 2.228)\\)\nInterpret the meaning of the second probability in a test setting.\n\n\n\nShow solution\n\n\nWe use the compliment to compute this case \\[\nP(T_5&gt;2.015)=1-P(T_5\\leq2.015)\n\\]\n\n\nprint(\"a)\", 1-stats.t.cdf(2.015, df=5))\n\na) 0.0500030861634031\n\n\n\nLet’s simplify the expression to see qhat we’re really looking for \\[\nP(|T_{10}|&gt;2.228)=P(T_{10}&lt;-2.228)+P(T_{10}&gt;2.228)\n\\] Recall that like the normal distribution, the t distribution is symmetrical around its mean, which is always 0. This makes it so \\[\nP(T&lt;-t)=P(T&gt;t)\n\\]\n\nAnd as such our expression becomes\n\\[ P(|T_{10}| &gt; 2.228) = 2\\cdot P(T_{10}&lt;-2.228) \\]\n\nprint(\"b) \", 2 * stats.t.cdf(-2.228, df=10))\n\nb)  0.050011771817111327\n\n\n\nSince the probability is 5%, we can interpret the value 2.228 as the threshold (critical value) for a two-sided t-test statistic with 10 degrees of freedom for rejecting the null hypothesis. Alternatively, if we observe a test statistic of value 2.228, the p-value of the test is 5%.\n\n\n\n\n\n\nFind the z-value such that \\(P(Z &lt; z) = 0.975\\)\nFind the z-value such that \\(P(|Z| &gt; z) = 0.01\\)\nWhat is the 10th percentile of the standard normal distribution?\n\n\n\nShow solution\n\nWe can find these qauntile values by using the inverse of the cumulative distribution function for the normal distribution. In practice we can often get adequaet results by using tables or use tools such as Python to calculate.\n\n\\(z \\approx 1.96\\)\nThe first implication arrow comes as a consequence of the normal distribtuion being symmetric.\n\\(P(|Z| &gt; z) = 0.01 \\Rightarrow P(Z &gt; z) = 0.005 \\Rightarrow z \\approx 2.576\\)\n\\(z_{0.10} \\approx -1.28\\)\n\nCan be verified in Python:\n\nprint(\"a) \", stats.norm.ppf(0.975))\n\na)  1.959963984540054\n\nprint(\"b) \", stats.norm.ppf(1-0.005))\n\nb)  2.5758293035489004\n\nprint(\"c) \", stats.norm.ppf(0.1))\n\nc)  -1.2815515655446004\n\n\n\n\n\n\nLet \\(X \\sim \\mathcal{N}(200, 30^2)\\). Compute:\n\n\\(P(X &gt; 240)\\)\n\\(P(160 &lt; X &lt; 220)\\)\nWhat value of \\(X\\) cuts off the top 5%? (What would the 95%th percentile be?)\n\n\n\nShow solution\n\nStandardize:\n\n\\(Z = \\frac{240 - 200}{30} = 1.33 \\Rightarrow P(Z &gt; 1.33)=1-P(Z\\leq 1.33) \\approx 0.0918\\)\nStandardize both: \\(Z_1 = \\frac{160 - 200}{30} = -1.33\\), \\(Z_2 = \\frac{220 - 200}{30} = 0.67\\) ⇒ \\(P(160&lt;X&lt;240)= P(X&lt;240)-P(X&lt;160)=P(Z&lt;0.67)-P(Z&lt;-1.33) \\approx 0.7486 - 0.0918 = 0.6568\\)\nFind \\(z\\) for 95th percentile. Here it’s easiest to start with the 95th percentile of the standard normal distribution, \\(z=1.645\\). \\(z = 1.645 \\Rightarrow X = 200 + 1.645 \\times 30 = 249.35\\)\n\nConfirming with an Python script\n\nprint(\"a)\", 1-stats.norm.cdf(240, loc=200, scale=30))\n\na) 0.09121121972586788\n\n## Gives 0.0912 due to different approximation than above\n\nprint(\"b)\", (stats.norm.cdf(220, loc=200, scale=30) - \nstats.norm.cdf(160, loc=200, scale=30)))\n\nb) 0.6562962427272092\n\n## c) First compute the quantile, then double check\nquant=200+1.645*30\nprint(\"c)\", stats.norm.cdf(quant, loc=200, scale=30))\n\nc) 0.9500150944608786\n\n## Finding the quantile can also do this more directly:\nprint(\"quant = \", stats.norm.ppf(0.95, loc=200, scale=30))\n\nquant =  249.34560880854417\n\n\n\n\n\n\nLet \\(T \\sim t_{12}\\). Feel free to use either a script or table to do this. Compute:\n\n\\(P(T &lt; 2.18)\\)\n\\(P(|T| &gt; 2.18)\\)\nThe 97.5th percentile of the distribution\n\n\n\nShow solution\n\n\n\n\n\nprint(\"a)\", stats.t.cdf(2.18, df = 12))\n\na) 0.9750530883473195\n\n\n\nTwo-tailed:\n\n\\[\nP(|T| &gt; 2.18) =\\cdots= 2 P(T &gt; 2.18) \\approx 2  (1 - 0.975) = 0.05\n\\] We double check with R:\n\nprint(\"b)\", 2*(1-stats.t.cdf(2.18, df=12)))\n\nb) 0.04989382330536096\n\n\n\nIt’s obvious that we’re really close from a) and b), but let’s still use Python to approximate even more exactly.\n\n\nstats.t.ppf(0.975, df = 12)\n\nnp.float64(2.1788128296634177)\n\n\n\n\n\n\nLet \\(Z \\sim \\mathcal{N}(0,1)\\). Compute:\n\n\\(P(Z &gt; -0.5)\\)\n\\(P(Z &lt; 0.84)\\)\nWhat percentile corresponds to \\(Z = 1.28\\)?\n\n\n\nShow solution\n\n\n\\(P(Z &gt; -0.5) = 1 - P(Z &lt; -0.5) = 1 - 0.3085 = 0.6915\\)\n\\(P(Z &lt; 0.84) = 0.7995\\)\n90th percentile (verify: pnorm(1.28) ≈ 0.8997)\n\nWe use Python to verify:\n\nprint(\"a)\", 1-stats.norm.cdf(-0.5))\n\na) 0.6914624612740131\n\nprint(\"b)\", stats.norm.cdf(0.84))\n\nb) 0.7995458067395503\n\n## c) We have the z value, and we want to know what percentle it is. I.e. whats P(Z&lt;1.28)\nprint(\"c)\", stats.norm.cdf(1.28))\n\nc) 0.8997274320455579\n\n\n\n\n\n\nLet \\(Z \\sim \\mathcal{N}(0,1)\\) and \\(T \\sim t_5\\)\n\nCompute the density \\(f_Z(1)\\)\nCompute the density \\(f_T(1)\\)\nCompare and interpret\n\n\n\nShow solution\n\na), b) For densitiy functions in Python we use stats.norm.pdf and stats.t.pdf to compute their values. Since the df is pretty low here, we can expect some difference between the standard normal and the t distribution.\n\nprint(\"a) Normal:\", stats.norm.pdf(1))\n\na) Normal: 0.24197072451914337\n\nprint(\"b) T with 5 degrees of freedom:\", stats.t.pdf(1, df=5))\n\nb) T with 5 degrees of freedom: 0.21967979735098053\n\n\n\nThe t-distribution has heavier tails, so at z = 1, its peak is slightly lower than that of the standard normal. The tails decay more slowly.",
    "crumbs": [
      "Modules",
      "Module 4: Designing studies, hypothesis testing, and quantifying effects",
      "Exercises"
    ]
  },
  {
    "objectID": "4-exercises.html#one-sample-t-test",
    "href": "4-exercises.html#one-sample-t-test",
    "title": "Exercises",
    "section": "",
    "text": "For the following exercises, we will use the test statistic for one-sample t-test with null hypothesis \\(H_0:\\mu=\\mu_0\\): \\[T=\\frac{\\bar x-\\mu_0}{s/\\sqrt{n}},\\] where \\(n\\) is the sample size, \\(\\bar x\\) is the sample mean, \\(s\\) is the sample standard deviation and \\(\\mu_0\\) is the expected value under the null hypothesis.\n\n\nA company selling premium coffee beans wants to determine whether reducing the price of its product leads to a significant increase in the average number of bags sold per day. The marketing team conducted a test by lowering the price for a month in select stores and measuring the sales volume. They want to analyze whether the mean daily sales after the price reduction are significantly higher than before.\n\nSet up the null- and alternative hypothesis, and decide on a suitable significance level.\n\nBefore the price reduction, the mean daily sales were 200 bags. After the price reduction, a sample of 30 days of sales data showed a sample mean of 215 bags, with standard deviation 25 bags.\n\nCalculate the test statistic.\nFind the p-value. What is the conclusion?\n\n\n\nShow solutions\n\n\n\\(H_0: \\mu\\le 200\\) vs \\(H_1: \\mu&gt;200\\). We could allow for a relatively high significance level in this context, because rejecting a true null hypothesis (Type I error) will most likely not have large consequences, e.g. \\(\\alpha =0.1\\).\n\\(T=(215-200)/(25/\\sqrt{30})=3.29\\)\n\\(\\text{p-value}=P(T_{29}&gt;3.29)=0.00132\\). The p-value is very low, indicating that we may reject the null hypothesis at (almost) any significance level in (a). The company should continue with the reduced price.\n\n\nfrom scipy import stats\nprint(1-stats.t.cdf(3.29, 29))\n\n0.0013169979736814552\n\n\n\n\n\n\nA large company is trying to encourage employees to save more for retirement. Previously, employees had to actively sign up for the company’s pension savings plan (opt-in system). Recently, the company changed its policy so that employees are automatically enrolled in the plan but can opt out if they wish (opt-out system). Behavioral economists suggest that default options strongly influence decision-making, leading to higher participation rates in savings plans.\nThe HR department wants to test with a significance level of 5% whether the average monthly contribution to the retirement plan has increased significantly after implementing the opt-out system.\n\nSet up the null- and alternative hypothesis.\n\nBefore the change, the average monthly contribution was $250 per employee. After the policy change, a random sample of 50 employees showed: Sample mean of $270 and standard deviation of $90.\n\nCalculate the test statistic.\nFind the p-value. What is the conclusion?\n\n\n\nShow solutions\n\n\n\\(H_0: \\mu\\le 250\\) vs \\(H_1: \\mu&gt;250\\).\n\\(T=(270-250)/(90/\\sqrt{50})=1.57\\)\n\\(\\text{p-value}=P(T_{29}&gt;3.29)=0.0613\\). The p-value is higher than 5%, indicating that we may reject not the null hypothesis. The pension payments have not increased due to the change from opting in to opting out.\n\n\nfrom scipy import stats\nprint(1-stats.t.cdf(2.94, 49))\n\n0.002498152330066006\n\n\n\n\n\n\nA nutritionist wants to test whether a new diet plan leads to a significant reduction in average daily calorie intake. A sample of 40 individuals followed the diet for one month, and their average daily calorie intake was recorded.\nThe recommended daily intake before the diet was 2200 kcal. After one month, the sample had a mean intake of 2100 kcal with a standard deviation of 250 kcal.\n\nSet up the null- and alternative hypotheses.\nCalculate the test statistic.\nFind the p-value and interpret the result.\n\n\n\nShow solutions\n\n\n\\(H_0: \\mu\\geq 2200\\) vs \\(H_1: \\mu&lt;2200\\).\n\\[T=\\frac{2100-2200}{250/\\sqrt{40}}=-2.53\\]\n\\[\\text{p-value}=P(T_{39}&lt;-2.53)=0.0079\\]\n\nSince the p-value is lower than the conventional 5% significance level, we reject the null hypothesis, suggesting that the new diet plan significantly reduces daily calorie intake.\n\nfrom scipy import stats\nprint(stats.t.cdf(-2.53, 39))\n\n0.007778305846562239\n\n\n\n\n\n\nA smartphone manufacturer wants to determine whether the battery life of their new model is significantly different from the advertised 24 hours. A sample of 50 devices was tested, showing a mean battery life of 23.5 hours with a standard deviation of 1.2 hours.\n\nFormulate the hypotheses.\nCompute the test statistic.\nDetermine the p-value and state your conclusion.\n\n\n\nShow solutions\n\n\n\\(H_0: \\mu=24\\) vs \\(H_1: \\mu\\neq 24\\).\n\\[T=\\frac{23.5-24}{1.2/\\sqrt{50}}=-2.946\\]\n\\[\\text{p-value}=2 \\times P(T_{49}&lt;-2.94)=0.0049\\]\n\nSince the p-value is very small, we reject the null hypothesis, indicating that the battery life is significantly different from 24 hours.\n\nprint(2*stats.t.cdf(-2.946, 49))\n\n0.004914894166068594\n\n\n\n\n\n\nA fitness instructor claims that the average resting pulse rate of adults in their program is less than 70 beats per minute. A sample of 10 participants yields:\n\\[ \\text{Pulse Rates:} \\ 68, \\ 72, \\ 69,  \\ 71,  \\ 67,  \\ 65,  \\ 70,\\ 66,\\  68, \\ 64 \\]\nAssume approximate normality.\n\nState the null and alternative hypotheses.\nCompute the sample mean and standard deviation.\nConduct a one-sample t-test at the 5% significance level.\nWhat do you conclude?\nIf the sample had been twice as large with the same mean and standard deviation, how would the p-value likely change?\n\n\n\nShow solution\n\n\nThe statement of the instructor is that the mean is less than 70 bpm, so that becomes our alternative hypothesis, and the opposite case becomes the null hypothesis. We use \\(H_0: \\mu=70\\) as we are only interested in the edge case when testing.\n\n\\(H_0: \\mu = 70\\) \\(H_1: \\mu &lt; 70\\)\n\nWe simply compute using the given data and\n\n\\(\\bar{x} = \\frac{1}{10}\\sum^{10}_{i=1}x_i=68.0\\), \\(s=\\sqrt{\\frac{1}{n-1}\\sum^{10}_{i=1}\\left(x_i-\\bar{x}\\right)^2} \\approx 2.58\\)\n\n\\(t = \\frac{68 - 70}{2.58/\\sqrt{10}} \\approx -2.45\\)\nCritical value (df = n-1=9) at \\(\\alpha = 0.05\\) is approximately -1.833. This number can easily be found in a t-table, or by running some code to estimate it.\n\n\n### qt gives the quantile function for the\n# t distribution. We use 0.05 to find the \n# critical value as this is a one sided test. \n# 9 is just the degrees of freedom \nfrom scipy import stats\nprint(stats.t.ppf(-0.05, 9))\n\nnan\n\n### Underneath I am finding the p-value by \n# looking at the distribution function of \n# the t-distribution and plugging in the \n# found t statistic. \nprint(stats.t.cdf(-2.45, 9))\n\n0.0183783590582972\n\n\nSince -2.58 &lt; -1.833, we reject \\(H_0\\), and thus conclude that the average bpm is less than 70.\n\nWith a larger sample, the standard error would decrease, leading to a larger magnitude of the test statistic and a smaller p-value. This illustrates the effect of sample size on power.\n\n\n\n\n\nSuppose a manufacturer claims their machine fills soda bottles with 500 ml of soda. You suspect it underfills. You collect a sample of 12 bottles and measure:\n\nimport numpy as np\nx = np.array([498, 495, 497, 496, 499, 498, 494, 496, 495, 497, 496, 495])\n\nThis problem and it’s solutions were made in R, but feel free to use an equivalent.\n\nTest if the true mean is less than 500 ml using R.\nProvide the p-value and interpret the result.\nWhat assumption are you making about the distribution?\n\n\n\nShow solution\n\n\n### The t.test command let's you run a t.test by \n# simply adding the H_0 mean and specifying the\n# variant you'd liek to run. Here we test for \n# H_1: mu&lt;500 and H_0: mu=500 \nt_result = stats.ttest_1samp(x, popmean=500, alternative='less')\nprint(t_result)\n\nTtestResult(statistic=np.float64(-8.482095610516701), pvalue=np.float64(1.864450821881949e-06), df=np.int64(11))\n\n\n\nOutput might give \\(t \\approx -8.49\\), \\(p &lt; 0.001\\). Strong evidence the machine underfills.\nWe assume that the population distribution is approximately normal, as required for small-sample t-tests.",
    "crumbs": [
      "Modules",
      "Module 4: Designing studies, hypothesis testing, and quantifying effects",
      "Exercises"
    ]
  },
  {
    "objectID": "4-exercises.html#two-sample-t-test",
    "href": "4-exercises.html#two-sample-t-test",
    "title": "Exercises",
    "section": "",
    "text": "For the following exercises, we will use the test statistic for two-sample t-test with null hypothesis \\(H_0:\\mu_1=\\mu_2+d_0\\): \\[T=\\frac{\\bar x_1-\\bar x_2-d_0}{\\sqrt{s_1^2/n_1+s_2^2/n_2}},\\] where \\(n_1\\) and \\(n_2\\) are the sample sizes of the two samples, \\(\\bar x_1\\) and \\(\\bar x_2\\) are the sample means, \\(s_1\\) and \\(s_2\\) are the sample standard deviations and \\(d_0\\) is the difference in expected value under the null hypothesis (often \\(d_0=0\\)). The test statistic follows a t-distribution with degrees of freedom (df): \\[\\text{df}=\\frac{\\big(\\frac{s_1^2}{n_1}+\\frac{s_2^2}{n_2}\\big)^2}{\\frac{(s_1^2/n_1)^2}{n_1-1}+\\frac{(s_1^2/n_2)^2}{n_2-1}}\\]\n\n\nAn e-commerce company wants to test whether showing customer reviews on product pages increases the average number of purchases. Behavioral economics suggests that social proof (i.e., seeing others’ positive experiences) influences decision-making and can lead to higher sales.\nTo test this, the company randomly selected two groups of products: one group displayed customer reviews, and the other did not. They then compared the average daily sales per product between the two groups.\n\nSet up the null- and alternative hypothesis. Use 10% significance level.\nDiscuss how the test statistic will behave under the various hypothesis.\n\nThe experiment result in the following data:\n\nProducts with reviews: Sample mean daily sales = 120 units, Sample standard deviation = 30, Sample size = 40\nProducts without reviews: Sample mean daily sales = 105 units, Sample standard deviation = 28, Sample size = 40\n\n\nCalculate the test statistic and the degrees of freedom.\nFind the p-value. What is the conclusion?\n\n\n\nShow solutions\n\nLet \\(\\mu_1\\) be the expected daily sales with reviews and \\(\\mu_2\\) without reviewes.\n\n\\(H_0: \\mu_1\\le \\mu_2\\) vs \\(H_A: \\mu_1&gt;\\mu_2\\).\nUnder the null hypothesis, we expect the numerator of the test statistic to be close to zero, making the test statistic close to zero. Under the alternative hypothesis, we expect \\(\\bar x_1&gt;\\bar x_2\\) such that the numerator of \\(T\\) will be positive. Higher values will therefore favor the alternative hypothesis.\n\n\n\nimport numpy as np\ns1 = 30;  s2 = 28; n1 = 40; n2 = 40\nm1 = 120; m2 = 105;\ndf = (s1**2/n1 + s2**2/n2)**2/((s1**2/n1)**2/(n1-1)+(s2**2/n2)**2/(n2-1))\nt = (m1-m2)/np.sqrt(s1*s1/n1+s2*s2/n2)\nprint(\"df=\",df)\n\ndf= 77.63164160330635\n\nprint(\"T=\",t)\n\nT= 2.311799743112827\n\n\n\\(T=(102-105)/\\sqrt{30^2/40+28^2/40}=-0.462\\) \\[\\text{df}=\\frac{\\big(\\frac{s_1^2}{n_1}+\\frac{s_2^2}{n_2}\\big)^2}{\\frac{(s_1^2/n_1)^2}{n_1-1}+\\frac{(s_1^2/n_2)^2}{n_2-1}}\\]\n\n\\(\\text{p-value}=P(T_{29}&gt;0.462)=0.32\\). The p-value is lower than 10%, indicating that we may reject the null hypothesis. There is enough evidence to suggest that the reviews increase the sales at a 10% significance level. The company should show reviews of their products to increase sales.\n\n\nfrom scipy import stats\nprint(1-stats.t.cdf(t, df))\n\n0.011720746167147467\n\n\n\n\n\n\nA company is testing whether offering a performance-based bonus increases employee productivity. They implement a new bonus system in one office location (Group A), while another office continues without bonuses (Group B). After three months, they measure the average number of completed sales calls per week in both offices. The company wants you to determine if the bonus system leads to higher productivity (mean number of sales calls).\n\nFormulate the hypothesis problem including \\(H_0\\), \\(H_A\\) and setting a suitable significance level.\n\nGroup A (Bonus group):\n\nSample mean: 52 sales calls per week\nSample standard deviation: 8\nSample size: n = 35\n\nGroup B (No bonus group):\n\nSample mean: 48 sales calls per week\nSample standard deviation: 9\nSample size: n = 35\n\n\nCalculate the test statistic based on this information. Find the p-value.\nWhat is your conclusion and your recommendation to the company?\n\n\n\nShow solutions\n\n\n\\(H_0: \\mu_A =\\mu_B\\) vs \\(H_A: \\mu_A&gt;\\mu_B\\). In this case, the bonus scheme might be expensive, so we want to avoid type I errors (rejecting the null, when it is truly no difference in productivity). We use \\(\\alpha = 1\\%\\).\n\\(T=1.96\\) and p-value \\(=0.0267\\).\nThe p-value is low, indicating evidence for the alternative hypothesis, but we have set a significance level of 1%, for which the p-value exceeds. This would lead us to not recommend implementing the bonus, because it does not improve productivity to a high enough degree at this significance level.\n\n\nm1 = 52; s1 = 8; n1 = 35\nm2 = 48; s2 = 9; n2 = 35\nstats.ttest_ind_from_stats(m1,s1,n1,m2,s2,n2,alternative=\"greater\")\n\nTtest_indResult(statistic=np.float64(1.9652147377620703), pvalue=np.float64(0.026737892346682984))\n\n\n\n\n\n\nA university wants to determine if students who attend tutoring sessions perform better on exams than those who do not. Two independent samples of students were taken:\n\nTutoring group: Mean score = 78, Standard deviation = 10, Sample size = 30\nNon-tutoring group: Mean score = 72, Standard deviation = 12, Sample size = 35\n\n\nState the hypotheses.\nCalculate the test statistic and degrees of freedom.\nFind the p-value and interpret the result.\n\n\n\nShow solutions\n\n\n\\(H_0: \\mu_1 = \\mu_2\\) vs \\(H_A: \\mu_1 &gt; \\mu_2\\).\n\n\n\ns1 = 10; s2 = 12; n1 = 30; n2 = 35\nm1 = 78; m2 = 72;\ndf = (s1**2/n1 + s2**2/n2)**2 / ((s1**2/n1)**2/(n1-1) + (s2**2/n2)**2/(n2-1))\nt = (m1-m2) / ((s1**2/n1 + s2**2/n2)**0.5)\nprint(\"df=\", df)\n\ndf= 62.958820084919275\n\nprint(\"T=\", t)\n\nT= 2.1985812677253573\n\n\n\nThe computed p-value is \\(P(T_{63}&gt;t)=0.0158\\). The evidence thus points towards the conclusion that attending tutoring improves performance, due to a low p-value.\n\n\nprint(1-stats.t.cdf(t, df))\n\n0.01579534729247911\n\n\n\n\n\n\nA professor wants to compare exam scores between online and in-person students, as he suspects his students preform systematically better with online exams. The results:\n\nOnline: 82, 85, 88, 79, 84\nIn-person: 78, 74, 80, 76, 77\n\n\nSet up the hypotheses.\nConduct a two-sample t-test.\nAt the 0.05 significance level, what do you conclude?\n\n\n\n\nShow solution\n\n\n\\(H_0: \\mu_{\\text{online}} = \\mu_{\\text{in-person}}\\) \\(H_1: \\mu_{\\text{online}} &gt; \\mu_{\\text{in-person}}\\)\nCompute sample means and the sample variances. For sample 1, being the online results.\n\n\n### We use the same methods as always to find the sample mean and variance, but here is a simple way to do it in R. I made this problem have very small datasets so it would be feasible to do by hand, so please make sure you find teh same results. \nx_1 = np.array([82, 85, 88, 79, 84])\nmean_x1 = np.mean(x_1)\nvar_x1 = np.var(x_1,ddof=1)  \nprint(\"Mean:\", mean_x1)\n\nMean: 83.6\n\nprint(\"Variance:\", var_x1)\n\nVariance: 11.299999999999999\n\n\nI.e., we end up with \\(\\bar{x}_1=83.6\\) and \\(s^2_1=11.3\\)\nNow for sample 2, in person.\n\n### Same as above, just new names\nx_2 = np.array([78, 74, 80, 76, 77])\nmean_x2 = np.mean(x_2)\nvar_x2 = np.var(x_2,ddof=1)  \nprint(\"Mean:\", mean_x2)\n\nMean: 77.0\n\nprint(\"Variance:\", var_x2)\n\nVariance: 5.0\n\n\nI.e., we end up with \\(\\bar{x}_1=77\\) and \\(s^2_1=5\\)\nTest statistic:\n\\[\nt = \\frac{83.6 - 77.0}{\\sqrt{\\left(11.3/5+5/5\\right)}} \\approx 3.6554\n\\]\n\nThis is a one sided, and now we only need find the degrees of freedom to determine the proper critical value.\n\n\\[\n\\text{df}=\\frac{\\left(11.3/5+5/5\\right)^2}{\\frac{(11.3/5)^2}{4}+\\frac{(5/5)^2}{4}}=6.9602\\approx7\n\\] Using a table we can find our critical value when we let the df be 7 is 1.895.\nWith this we reject \\(H_0\\), significant difference.\nBeing more exact we can use some simple Python code.\n\n### Since we are using a \"greater than\" test we need to use the 0.95 quantile to find the proper critical value, but since the t-distribution is symmetric, 0.95 and 0.05 will only be separated by a minus sign. \nprint(stats.t.ppf(0.95, df = 6.9602))\n\n1.8962097832217037\n\n### The probability function finds the finds the probability of being under a value, we here take the complement to find the probability of being over 3.6554\nprint(1-stats.t.cdf(3.6554, 6.9602))\n\n0.004100373037377536\n\n\nUnderneath is an example of python code to do the exact same test.\n\nx_1 = np.array([82, 85, 88, 79, 84])\nx_2 = np.array([78, 74, 80, 76, 77])\n\n# One-sided t-test: H1 is that mean of x_1 &gt; mean of x_2\nresult = stats.ttest_ind(x_1, x_2, equal_var=False, alternative='greater')\n\nprint(\"t-statistic:\", result.statistic)\n\nt-statistic: 3.6554019191032916\n\nprint(\"p-value:\", result.pvalue)\n\np-value: 0.004100315140564395\n\n\n\n\n\n\nSuppose a two-sample t-test yields a p-value of 0.045 for comparing the means of two groups, using a significance level of 0.05.\n\nCan we say the means are “significantly different”?\nWhat if the significance level had been 0.01 instead?\nExplain how this relates to Type I error.\n\n\n\nShow solution\n\n\nYes — at \\(\\alpha = 0.05\\), we reject \\(H_0\\).\nNo — we would not reject at \\(\\alpha = 0.01\\).\nSignificance thresholds control the probability of a Type I error (rejecting a true \\(H_0\\)). A lower threshold is more conservative.\n\n\n\n\n\nTwo teachers use different methods to teach the same material, and they want to test if their methods give different average results. Students are randomly assigned to either class.\n\nGroup A (n = 10): Mean = 78, SD = 6\nGroup B (n = 14): Mean = 73, SD = 10\n\n\nState the hypotheses.\nCalculate the Welch t-statistic.\nEstimate the degrees of freedom.\nTest at the 5% level: is there a significant difference in mean scores?\n\n\n\nShow solution\n\n\nWe have no indication of directionality for this test. Thus we end up with the following null and alternative hypotheses.\n\\(H_0: \\mu_A = \\mu_B\\) \\(H_1: \\mu_A \\ne \\mu_B\\)\nWe can plug i the means, square the standard deviatons and plug in the numbers in each sample for the test statistic. \\[\nt = \\frac{78 - 73}{\\sqrt{6^2/10 + 10^2/14}} = \\frac{5}{\\sqrt{3.6 + 7.14}} = \\frac{5}{\\sqrt{10.74}} \\approx 1.53\n\\]\n\n\n\\[\ndf \\approx \\frac{(10.74)^2}{(3.6^2/9 + 7.14^2/13)} \\approx 21.51\n\\]\n\nAt \\(df = 22\\), two-tailed 5% critical t ≈ ±2.074. (found in table for critical values) Since 1.52 &lt; 2.08, we fail to reject \\(H_0\\).\n\nUsing Python code we can also find the exact and approximate df.\n\n## lower and upper bound for approximate df\nstats.t.ppf(0.025, 22)\n\nnp.float64(-2.073873067904015)\n\nstats.t.ppf(0.975, 22)\n\nnp.float64(2.0738730679040147)\n\n## for exact df\nstats.t.ppf(0.025, 21.51)\n\nnp.float64(-2.076615611268758)\n\nstats.t.ppf(0.975, 21.51)\n\nnp.float64(2.076615611268758)\n\n\n## Note that the lower and upper bounds have the same absolute value. This is a consequence of the t-distribution being symmetric. \n\n\n\n\n\nYou are given two samples:\n\nA = np.array([85, 87, 83, 84, 89, 91])\nB = np.array([78, 77, 79, 81, 76, 75, 80])\n\n\nConduct a two-sample t-test assuming unequal variances.\nReport the t-statistic, degrees of freedom, and p-value.\nInterpret the result at \\(\\alpha=0.05\\).\nDoes the output indicate anything about equality of variances?\n\n\n\nShow solution\n\n\nWe can use a simplePythoncommand for this\n\n\nresult = stats.ttest_ind(A, B, equal_var=False)\nprint(result)\n\nTtestResult(statistic=np.float64(5.666666666666667), pvalue=np.float64(0.00033388838046637467), df=np.float64(8.797264682220435))\n\n\n\nFrom, the test summary we can just read\n\n\\[\nt=5.6667 \\quad\ndf=8.793 \\quad\n\\text{p-value}=0.0003339\n\\]\n\nStrong evidence against \\(H_0\\) in form of such a low p-value, this indicates that the two samples are different at a 5 percent significance level.\nWelch’s t-test does not require equal variances and is safer when variances are not assumed equal.",
    "crumbs": [
      "Modules",
      "Module 4: Designing studies, hypothesis testing, and quantifying effects",
      "Exercises"
    ]
  },
  {
    "objectID": "4-exercises.html#tests-of-proportions-one-sample",
    "href": "4-exercises.html#tests-of-proportions-one-sample",
    "title": "Exercises",
    "section": "",
    "text": "Let \\(p\\) denote a proportion and \\(p_0\\) be the proportion under the null hypothesis. We can then test the null hypothesis \\(H_0: p=p_0\\) against an alternative hypothesis using the test statistic \\[Z = \\frac{\\hat p-p_0}{\\sqrt{p_0(1-p_0)/n}},\\] which is approximately normally distributed when \\(n\\) is sufficiently large and the probabilies are not too small (often \\(np&gt;5\\) is used as a rule of thumb).\n\n\nA retail chain is considering phasing out cash payments in favor of digital transactions. Management wants to determine whether at least 60% of customers are already using mobile payment apps (such as Apple Pay or Google Pay) when shopping. If the proportion is significantly higher than 60%, they may move forward with reducing cash payment options.\n\nSet up the null hypothesis and alternative hypothesis. Discuss how the test statistic will behave under the various hypotheses.\n\nThe retail chain conducts a survey where they ask customers whether they used a mobile payment app for their most recent purchase. Out of the 400 customers asked, 260 used mobile payment apps.\n\nCalculate \\(Z\\) based on this information. What is the p-value?\nWhat does the p-value tell you about people’s payment preferences?\n\n\n\nShow solutions\n\n\n\\(H_0: p=0.6\\) vs \\(H_A: \\mu&gt;0.6\\). The numerator of \\(Z\\) will be expected close to 0 if the null is zero, making Z close to zero. If \\(H_A\\) is true, \\(\\widehat p\\) will be expected larger than 0.6, making Z positive. Large values of \\(Z\\) will favor \\(H_A\\).\n\\(Z = (0.65-0.6)/\\sqrt{0.6\\cdot 0.4/400}=2.04\\) and p-value\\(=P(Z&gt;2.04)=0.0206\\).\n\n\nphat = 260/400\nz = (phat-0.6)/np.sqrt(0.6*(1-0.6)/400)\nprint(\"Z=\", z)\n\nZ= 2.041241452319317\n\nprint(\"Pvalue: \", 1-stats.norm.cdf(z))\n\nPvalue:  0.02061341666858174\n\n\n\nThe p-value is low, which indicates evidence against \\(H_0\\). It indicates that the proportion of mobile app payments is above 60%.\n\n\n\n\n\nA sustainable fashion brand wants to know whether at least 30% of consumers are willing to pay a premium for eco-friendly clothing. If the proportion is significantly higher than 30% with significance level 10%, they will launch a new premium-priced, eco-friendly clothing line.\nThey conduct a market survey where respondents indicate whether they would be willing to pay 10% more for sustainably produced clothing.\n\nSet up the null- and alternative hypothesis.\n\nThe market survey got 500 respondents, and out of those, 166 were willing to pay 10% more for sustainable clothing.\n\nCalculate \\(Z\\) based on this information. What is the p-value?\nWhat would your recommendation to the fashion brand be?\n\n\n\nShow solutions\n\n\n\\(H_0: p\\le 30\\%\\) vs \\(H_A: p&gt;30\\%\\).\n\\(\\hat p=166/500= 33.2\\%\\), \\(z=(0.332-0.3)/\\sqrt{0.3*0.7/500}=1.56\\) and \\(P(Z&gt;z)=P(Z&gt;1.56)=1-\\Phi(1.56)=0.0592\\).\nAt a 10% signifance level, we would reject the null hypothesis, since the p-value of 0.06 is smaller than 0.1. At this significance level, we would recommend the brand to go ahead with the launch of their eco-friendly clothing.\n\n\nphat = 166/500\nz = (phat-0.3)/np.sqrt(0.3*(1-0.3)/500)\nprint(\"Z=\", z)\n\nZ= 1.5614401167176546\n\nprint(\"Pvalue: \", 1-stats.norm.cdf(z))\n\nPvalue:  0.05920997135406203\n\n\n\n\n\n\nA public transportation agency wants to determine whether at least 70% of residents support a new subway line expansion. A survey of 600 residents finds that 435 support the expansion.\n\nFormulate the hypotheses.\nCompute the test statistic and p-value.\nWhat is the conclusion?\n\n\n\nShow solutions\n\n\n\\(H_0: p = 0.7\\) vs \\(H_A: p &gt; 0.7\\).\n\\[Z=\\frac{0.725-0.7}{\\sqrt{0.7\\cdot 0.3/600}}=1.63\\]\n\np-value \\(=P(Z&gt;1.63)=0.0907\\)\n\np_hat = 435/600\nz = (p_hat-0.7) / ( (0.7*0.3/600)**0.5 )\nprint(\"Z=\", z)\n\nZ= 1.336306209562123\n\nprint(\"p-value=\", 1-stats.norm.cdf(z))\n\np-value= 0.09072460386070991\n\n\n\nA p-value of 9% indicates that the evidence against the null hypothesis is not very strong, and at a 5% significance level, we would not reject the null hypothesis.\n\n\n\n\n\nA factory claims that at most 5% of products are defective. In a random sample of 80 products, 7 were defective.\n\nTest whether the true defect rate is higher than claimed at the 5% level.\nCompute the test statistic.\nConclude and interpret.\nShould we consider this test trustworthy\n\n\n\nShow solution\n\n\nSince we want to explore if the proportion of defectives is higher than 5%, let’s make that our alternative hypothesis.\n\\(H_0: p = 0.05\\),\n\n\\(H_1: p &gt; 0.05\\)\n\n\n\n\\[\n\\hat{p} = \\frac{7}{80} = 0.0875,\\quad z = \\frac{0.0875 - 0.05}{\\sqrt{0.05(0.95)/80}} \\approx 1.54\n\\]\n\n\n\nSince our alternative hypothesis is that that \\(p&gt;0.05\\) we would need z to be higher than some critical value at the 5% level. Since we are using a one-sided test here, we find in a table that the critical value is given by \\(z\\approx 1.645\\). We can clearly tell that our test-statistic is not hihgh enough and this we won’t discard \\(H_0\\) in this case. Underneath I have done some computation giving a more exact critical value and finding the p-value is about 6 percent.\n\nprint(stats.norm.ppf(0.95))\n\n1.6448536269514722\n\nprint(1-stats.norm.cdf(1.54))\n\n0.06178017671181191\n\n\n\n\n\nThe common rule of thumb is \\(np&gt;5\\), and since \\(n\\hat{p}=7\\) in this case, we find little reason for doubt in this case.",
    "crumbs": [
      "Modules",
      "Module 4: Designing studies, hypothesis testing, and quantifying effects",
      "Exercises"
    ]
  },
  {
    "objectID": "4-exercises.html#tests-of-proportions-two-samples",
    "href": "4-exercises.html#tests-of-proportions-two-samples",
    "title": "Exercises",
    "section": "",
    "text": "Let \\(p_1\\) and \\(p_2\\) denote two proportions. We can then test the null hypothesis \\(H_0: p_1=p_2\\) against an alternative hypothesis using the test statistic \\[Z = \\frac{\\hat p_1-\\hat p_2}{\\sqrt{p_\\text{pool}(1-p_\\text{pool})(1/n_1+1/n_2)}},\\] where \\[p_\\text{pool}=\\frac{n_1\\widehat p_1+n_2\\widehat p_2}{n_1+n_2}.\\] The test statistic Z is approximately normally distributed.\n\n\nAn insurance company is testing how the way they frame their sales pitch affects customers’ willingness to buy a travel insurance policy. They randomly assign potential customers into two groups:\n\nLoss-framed message: “If you don’t buy travel insurance, you could lose thousands in unexpected expenses.”\nGain-framed message: “If you buy travel insurance, you’ll have peace of mind and financial protection.”\n\nBehavioral economics suggests that loss aversion makes people more sensitive to potential losses than gains, meaning the loss-framed message might lead to higher purchase rates.\n\nSet up the null- and alternative hypothesis.\nDiscuss how the test statistic will behave under the various hypothesis.\n\nThe experiment result in the following data:\n\nLoss-framed message: 200 customers surveyed, 45% purchased insurance\nGain-framed message: 200 customers surveyed, 35% purchased insurance\n\n\nCalculate the test statistic and the p-value.\nConclude the test.\n\n\n\nShow solutions\n\nLet \\(p_L\\) denote proportion of purchases receiving a loss-framed message and \\(p_G\\) denote proportion of purchases receiving a gain-framed message. a) \\(H_0: p_L=p_G\\) vs \\(H_A: p_L&gt;p_G\\).\n\nWe use \\(p_1=p_L\\) and \\(p_2 =p_G\\) in refence to the formula for the test statistic. If the null hypothesis is true, Z is expected to be close to zero since the numerator is expected to be near 0. If the alternative hypothesis is true, we expect \\(\\hat p_L&gt;\\hat p_G\\), giving a positive numerator. Thus, large values of Z will favor the alternative hypothesis.\n\n\n\npL = 0.45\npG = 0.35\nnL = 200\nnG = 200\npPool = (nL*pL + nG*pG)/(nL+nG)\nz = (pL-pG)/np.sqrt(pPool*(1-pPool)*(1/nL+1/nG))\nprint(\"Z=\",z)\n\nZ= 2.041241452319316\n\nprint(\"pvalue=\",1-stats.norm.cdf(z))\n\npvalue= 0.02061341666858174\n\n\n\nThe p-value is low (around 2%). This indicates that the probability of making a Type I error is low and favor rejecting the null hypothesis. In that sense, the evidence points towards people being willing to pay to avoid loss than they are to potentially gain.\n\n\n\n\n\nAn online retailer wants to determine whether offering free shipping increases the proportion of customers who complete their purchases. They run an A/B test, where one group of customers sees free shipping on all orders (Group A), while another group sees the standard shipping fees (Group B). After a week, they compare the proportion of customers who completed a purchase in each group. The retailer asks you to use 5% significance level.\n\nFormulate the hypothesis problem.\n\nFor the two groups, the retailer gathered the following data:\nGroup A (Standard shipping):\n\nSample size: 469\nCompleted purchases: 191\n\nGroup B (Free shipping)\n\nSample size: 483\nCompleted purchases: 231\n\n\nPerform the test: Find Z and the p-value.\nWhat is your conclusion?\nThe chief economist in the company has taken a statistics course and learned about the difference between statistical significance and economical significance. How would this distinction influence your recommendation to the company?\n\n\n\nShow solutions\n\n\n\\(H_0: p_A=p_B\\) vs \\(H_A: p_A&lt;p_B\\).\n\n\n\nnA = 469\nnB = 483\npA = 191/nA\npB = 231/nB\nprint(pB-pA)\n\n0.07101140261425792\n\npPool = (nA*pA + nB*pB)/(nA+nB)\nz = (pA-pB)/np.sqrt(pPool*(1-pPool)*(1/nA+1/nB))\nprint(\"Z=\",z)\n\nZ= -2.2050192819722274\n\nprint(\"pvalue=\",stats.norm.cdf(z))\n\npvalue= 0.01372637075205902\n\n\n\nAt a 5% significance level, the free shipping group have a higher completing rate than the standard shipping group.\nHere the effect size \\(|\\widehat p_A-\\widehat p_B| = 7\\%\\), that is 7 percentage points higher rate of completing the order if the shipping is free. This increase in completed orders increases the volume, but the free shipping will reduce the margin on each order. We would need more information to assess whether this specific experiment that resulted in a statistically significant increase in volume, also resulted in a economically significant increase of profits.\n\n\n\n\n\nA company is testing whether a new website layout increases the proportion of users who complete a purchase. They conduct an A/B test with the following results:\n\nNew layout: 500 users, 210 completed a purchase\nOld layout: 500 users, 180 completed a purchase\n\n\nFormulate the hypotheses.\nCompute the test statistic and p-value.\nInterpret the result.\n\n\n\nShow solutions\n\n\n\\(H_0: p_1 = p_2\\) vs \\(H_A: p_1 &gt; p_2\\).\n\n\n\nfrom scipy import stats\nn1, n2 = 500, 500\np1, p2 = 210/n1, 180/n2\np_pool = (210 + 180) / (n1 + n2)\nz = (p1 - p2) / ((p_pool * (1 - p_pool) * (1/n1 + 1/n2)) ** 0.5)\nprint(\"Z=\", z)\n\nZ= 1.9450198311991271\n\nprint(\"p-value=\", 1-stats.norm.cdf(z))\n\np-value= 0.025886295779031898\n\n\n\nSince the p-value is low, this gives reason to doubt the null hypothesis and indicates that the new layout do increase the proportion of customers placing an order.\n\n\n\n\n\nIn a clinical trial, 35 of 50 patients in group A responded to a vaccine. In group B, 40 of 60 patients responded.\n\nState the hypotheses\nCompute the test statistic\nConclude on the test\nInterpret the result.\n\n\n\nShow solution\n\n\n\\(H_0: p_A = p_B\\), \\(H_a: p_A \\ne p_B\\)\n\n\n\\[\n\\hat{p}_1 =\\frac{35}{50}= 0.70,\\quad \\hat{p}_2=\\frac{40}{60} = 0.6667,\\quad \\hat{p}_{\\text{pooled}} = \\frac{35+40}{110} = 0.6818\n\\]\n\\[\nz = \\frac{0.70 - 0.6667}{\\sqrt{0.6818(1 - 0.6818)(1/50 + 1/60)}} \\approx 0.373\n\\]\n\nWe know that test statistic should be normally distributed in this case. To reject the two sided test we would need \\(|z|&gt;1.96\\), which we clearly see isn’t the case, and we thus fail to reject \\(H_0\\).\n\nLet’s find the p-value with Python to show this clearly.\n\n## Upper and lower bounds\nprint(stats.norm.ppf(0.025))\n\n-1.9599639845400545\n\nprint(stats.norm.ppf(0.975))\n\n1.959963984540054\n\n## Computing p value\nprint(1-stats.norm.cdf(0.373)+stats.norm.cdf(-0.373))\n\n0.7091484439599731\n\n\n\\(p \\approx 0.70\\), not significant.\n\nThere is no evidence to suggest a difference in proportions.",
    "crumbs": [
      "Modules",
      "Module 4: Designing studies, hypothesis testing, and quantifying effects",
      "Exercises"
    ]
  },
  {
    "objectID": "4-exercises.html#normal-distribution-and-student-t-dsitribution",
    "href": "4-exercises.html#normal-distribution-and-student-t-dsitribution",
    "title": "Exercises",
    "section": "",
    "text": "What are the two parameters that define a normal distribution?\nWhat does the empirical rule (68-95-99.7 rule) state about the normal distribution?\n\n\n\nShow solution\n\n\nA normal distribution is fully defined by its mean (μ) and standard deviation (σ).\nThe normal distribution is a symmetric distribution around its mean, \\(\\mu\\). The rule gives an approximate measure of what proportion of the data that lies within a certain range of the mean.\n\n\n~68% of data lie within 1σ of the mean\n~95% within 2σ\n~99.7% within 3σ\n\nThis helps assess spread and typicality.\n\n\n\n\nGiven that \\(X \\sim \\mathcal{N}(100, 15^2)\\) (i.e. \\(X\\) is normally distributed with mean \\(\\mu=100\\) and standard deviation \\(\\sigma=15\\)), calculate:\n\nThe probability of \\(X &lt; 100\\). Why do we not need to consider the standard deviation when calculating this?\nThe approximate probability that \\(X &lt; 115\\)\nUse a z-transformation to b) to standard normal, and compute using a calculator. Let \\(Z\\sim \\mathcal{N}(0,1)\\), we call that a standard normal distribution. We also have that\n\n\\[\nZ=\\frac{X-\\mu}{\\sigma}\n\\]\n\n\nShow solution\n\n\nWe know that the normal distribution is symmetric around the mean, i.e. we have a 50-50 chance of finding an \\(X\\) above or below the mean. The mean here is 100, and was such we get\n\n\\[\nP(X&gt;100)=\\frac{1}{2}=0.5\n\\]\n\nWe can use the 68-95-99.7 rule to find the approximate probability. 115 is one standard deviation greater than the mean. That means we have to account for half of the 68% from the rule. I.e.\n\n\\[\nP(X\\in(100,115))=\\frac{0.68}{2}\\approx0.34\n\\] Now let’s also account for the cases where \\(X\\leq 100\\)\n\\[\nP(X&lt;115)=P(X\\leq100)+P(X\\in(100,115))\\approx0.5+0.34=0.84\n\\] Note: Since the normal distribution is continuous \\(P(X&lt;x)=P(X\\leq x)\\)\n\n\\[\nP(X&lt;115)=P\\left(\\frac{Z-100}{15}&lt;\\frac{115-100}{15}\\right)=P(Z&lt;1)=0.8413\n\\]\n\nInterpretation: There’s about an 84.13% chance that \\(X\\) will be below 115, which is equivalent to \\(Z\\) being below 1.\n\n\n\n\nSuppose a random variable \\(X \\sim \\mathcal{N}(\\mu, \\sigma^2)\\). Show that:\n\\[\nP(a &lt; X &lt; b) = P\\left( \\frac{a - \\mu}{\\sigma} &lt; Z &lt; \\frac{b - \\mu}{\\sigma} \\right)\n\\]\n\n\nShow solution\n\nThis transformation uses the standardization:\n\\[\nZ = \\frac{X - \\mu}{\\sigma}\n\\]\nBy applying this, we shift any normal distribution to the standard normal. We need only substitute and solve like a regular inequality, and the equivalence becomes clear.\n\\[\nP(a &lt; X &lt; b) = P\\left( \\frac{a - \\mu}{\\sigma} &lt; Z &lt; \\frac{b - \\mu}{\\sigma} \\right)\n\\]\n\n\n\n\n\nUse the symmetry of the standard normal distribution to explain why:\n\n\\[\nP(Z &gt; 1.96) = P(Z &lt; -1.96)\n\\]\n\nCalculate \\(P(|Z| &gt; 1.96)\\)\n\n\n\nShow solution\n\n\nThe standard normal is symmetric about 0, so:\n\n\\[\nP(Z &gt; c) = P(Z &lt; -c)\n\\] This equality comes from the logical fact of us a random varabiable being just as unlikely to take a value beyond a certain distance from the mean, no matter if the distance is in a positive of a negative directoion.\n\n\n\n\\[\nP(|Z| &gt; 1.96) =P(Z&lt;-1.96))+P(Z&gt;1.96)=2P(Z&lt;-1.96)\\approx 2*0.025=0.05\n\\]\n\n\n\n\n\nSketch or describe how the t-distribution changes as degrees of freedom increase.\nWhat does the t-distribution converge to?\n\n\n\nShow solution\n\n\nWith low degrees of freedom, the t-distribution is flatter and has fatter tails. As df increases, it becomes more peaked. The t-distribution also always has a mean of 0, and is symmetric around 0.\nAs df → ∞, the t-distribution approaches the standard normal distribution.\n\nWe can illustrate this quite simply without going into the ideas fully.\nLet first \\(X_1,\\dots,X_n\\sim i.i.d. \\mathcal{N}(\\mu, \\sigma^2)\\), and then let\n\\[ \\bar{X}=\\frac{1}{n}\\sum^n_{i=1}X_i, \\quad S^2=\\frac{1}{n-1}\\sum^n_{i=1}(X_i-\\bar{X})^2\n\\] It can then be shown that \\[ \\frac{\\bar{X}-\\mu}{\\sigma/\\sqrt{n}}\\sim\\mathcal{N}(0,1), \\quad \\frac{\\bar{X}-\\mu}{S/\\sqrt{n}}\\sim t\\_{n-1} \\] where \\(t_{n-1}\\) indicates a t-distribution with \\(df=n-1\\). Clearly these are very similar statements. All we need to recall now is that \\(S^2\\) is and unbiased and consistent estimator of \\(\\sigma^2\\). I.e.\n\\[\n\\lim_{n\\rightarrow\\infty}S^2=\\sigma^2 \\quad  \\Rightarrow \\quad  \\frac{\\bar{X}-\\mu}{S/\\sqrt{n}}\\sim t_{n-1} \\rightarrow \\frac{\\bar{X}-\\mu}{\\sigma/\\sqrt{n}}\\sim\\mathcal{N}(0,1).\n\\]\n\n\n\n\nA population has a skewed distribution with mean 10 and SD 5. Suppose we take samples of size 50. You can assume the samples are iid.\n\nWhat is the expected distribution of the sample mean?\nUse CLT to approximate \\(P(\\bar{X} &gt; 11)\\)\n\n\n\nShow solution\n\n\nCLT tells us that \\(\\bar{X} \\sim \\mathcal{N}\\left(10, \\frac{5}{\\sqrt{50}}\\right)\\)\n\n\n\\[\nZ = \\frac{11 - 10}{5/\\sqrt{50}} \\approx \\frac{1}{0.707} \\approx 1.41\n\\Rightarrow P(Z &gt; 1.41) \\approx 0.079\n\\]\n\n\n\n\nExplain when you should use the standard normal vs. the t-distribution in practice.\n\n\nShow solution\n\nUse the t-distribution if:\n\nPopulation standard deviation is unknown\nSample size is small (typically n &lt; 30)\n\nUse standard normal if:\n\nPopulation SD is known, or n is large, such that CLT becomes viable.\n\n\n\n\n\n\nWhy does the t-distribution have wider tails than the normal?\n\n\n\nWhat implication does this have on statistical testing?\n\n\n\nShow solution\n\n\nBecause of the extra uncertainty in estimating the population SD from the sample.\nWider tails mean larger critical values, so you’re less likely to reject \\(H_0\\) with small samples.\n\n\n\n\n\nUse Python (or something similar) to plot the density curves of \\(t_3, t_{10}, t_{30}\\), and standard normal.\n\nWhat do you observe as df increases?\nWhich distribution has the heaviest tails?\n\n\n\nShow solution\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import t, norm\n\n# x-values for plotting\nx = np.linspace(-4, 4, 500)\n\n# Plot t-distributions\nplt.plot(x, t.pdf(x, df=3), label=\"t3\", linewidth=2);\nplt.plot(x, t.pdf(x, df=10), label=\"t10\");\nplt.plot(x, t.pdf(x, df=30), label=\"t30\");\n\n# Plot standard normal\nplt.plot(x, norm.pdf(x), 'r--', label=\"N(0,1)\");\n\n# Legend and labels\nplt.legend(loc=\"upper right\");\nplt.ylabel(\"Density\");\nplt.title(\"t-distributions vs. Normal distribution\");\nplt.grid(True);\nplt.show()\n\n\n\n\n\n\n\n\n\nHeavier tails for small df. As df increases, the t-distribution converges to standard normal. This is just like we have discussed in the previous problems.\n\n\n\n\n\nLet \\(Z\\sim\\mathcal{N}(0,1)\\) Use Python to compute:\n\n\\(P(Z &gt; 1.96)\\)\n\\(P(-1.64 &lt; Z &lt; 1.64)\\)\n\\(P(X &gt; 120)\\), if \\(X \\sim \\mathcal{N}(100, 15^2)\\)\n\n\n\nShow solution\n\n\nprint(\"a) \", 1-stats.norm.cdf(1.96))\n\na)  0.024997895148220484\n\nprint(\"b) \", stats.norm.cdf(1.64) - stats.norm.cdf(-1.64))\n\nb)  0.8989948330517925\n\nprint(\"c) \", 1-stats.norm.cdf(120, loc = 100, scale = 15))\n\nc)  0.09121121972586788\n\n\n\n\n\n\nSimulate the sample mean of 1000 samples of size 5, 30, and 100 from an exponential distribution.\n\nPlot the histograms.\nComment on convergence to normality.\n\n\n\nShow solution\n\n\n\n\n\nnp.random.seed(42)\n\n# Function to generate 1000 sample means of size n\ndef sample_means(n):\n    return [np.mean(np.random.exponential(scale=1, size=n)) for _ in range(1000)]\n\n# Sample sizes\nsample_sizes = [5, 30, 100]\n\n# Plot histograms\nfor i, n in enumerate(sample_sizes, 1):\n    plt.figure();\n    plt.hist(sample_means(n), bins=30, range=(0, 5), color='skyblue', edgecolor='black');\n    plt.title(f'n = {n}');\n    plt.xlabel('Sample Mean');\n    plt.ylabel('Frequency');\n    plt.grid(True);\n    plt.show();\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs sample size increases, the distribution of the sample mean approaches a normal shape, even though the exponential is skewed.\n\n\n\n\n\nGenerate t-statistics using normal data and see how the distribution changes.\n\nGenerate 10,000 t-statistics from n = 5, 10, 30\nPlot and compare to standard normal\n\n\n\nShow solution\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\n# Function to generate 10,000 t-statistics from samples of size n\ndef gen_t(n):\n    t_stats = []\n    for _ in range(10000):\n        x = np.random.normal(loc=0, scale=1, size=n)\n        t_stat = np.mean(x) / (np.std(x, ddof=1) / np.sqrt(n))\n        t_stats.append(t_stat)\n    return np.array(t_stats)\n\n# Sample sizes to try\nsample_sizes = [5, 10, 30]\nbins = [30, 30, 20]\n\n# Plot histograms and overlay standard normal density\nx = np.linspace(-4, 4, 500)\nnormal_density = norm.pdf(x)\n\nfor n, b in zip(sample_sizes, bins):\n    t_vals = gen_t(n);\n    plt.figure();\n    plt.hist(t_vals, bins=b, density=True, color='lightblue', edgecolor='black');\n    plt.plot(x, normal_density, 'r--', label='Standard Normal');\n    plt.title(f\"Sampling Distribution of t-stat (n={n})\");\n    plt.xlabel(\"t-statistic\");\n    plt.ylabel(\"Density\");\n    plt.xlim(-4, 4);\n    plt.legend();\n    plt.grid(True);\n    plt.show();\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe distribution of the t-statistic becomes more like the normal as n increases.\n\n\n\n\nCompute using R:\n\n\\(P(T_5 &gt; 2.015)\\)\n\\(P(|T_{10}| &gt; 2.228)\\)\nInterpret the meaning of the second probability in a test setting.\n\n\n\nShow solution\n\n\nWe use the compliment to compute this case \\[\nP(T_5&gt;2.015)=1-P(T_5\\leq2.015)\n\\]\n\n\nprint(\"a)\", 1-stats.t.cdf(2.015, df=5))\n\na) 0.0500030861634031\n\n\n\nLet’s simplify the expression to see qhat we’re really looking for \\[\nP(|T_{10}|&gt;2.228)=P(T_{10}&lt;-2.228)+P(T_{10}&gt;2.228)\n\\] Recall that like the normal distribution, the t distribution is symmetrical around its mean, which is always 0. This makes it so \\[\nP(T&lt;-t)=P(T&gt;t)\n\\]\n\nAnd as such our expression becomes\n\\[ P(|T_{10}| &gt; 2.228) = 2\\cdot P(T_{10}&lt;-2.228) \\]\n\nprint(\"b) \", 2 * stats.t.cdf(-2.228, df=10))\n\nb)  0.050011771817111327\n\n\n\nSince the probability is 5%, we can interpret the value 2.228 as the threshold (critical value) for a two-sided t-test statistic with 10 degrees of freedom for rejecting the null hypothesis. Alternatively, if we observe a test statistic of value 2.228, the p-value of the test is 5%.\n\n\n\n\n\n\nFind the z-value such that \\(P(Z &lt; z) = 0.975\\)\nFind the z-value such that \\(P(|Z| &gt; z) = 0.01\\)\nWhat is the 10th percentile of the standard normal distribution?\n\n\n\nShow solution\n\nWe can find these qauntile values by using the inverse of the cumulative distribution function for the normal distribution. In practice we can often get adequaet results by using tables or use tools such as Python to calculate.\n\n\\(z \\approx 1.96\\)\nThe first implication arrow comes as a consequence of the normal distribtuion being symmetric.\n\\(P(|Z| &gt; z) = 0.01 \\Rightarrow P(Z &gt; z) = 0.005 \\Rightarrow z \\approx 2.576\\)\n\\(z_{0.10} \\approx -1.28\\)\n\nCan be verified in Python:\n\nprint(\"a) \", stats.norm.ppf(0.975))\n\na)  1.959963984540054\n\nprint(\"b) \", stats.norm.ppf(1-0.005))\n\nb)  2.5758293035489004\n\nprint(\"c) \", stats.norm.ppf(0.1))\n\nc)  -1.2815515655446004\n\n\n\n\n\n\nLet \\(X \\sim \\mathcal{N}(200, 30^2)\\). Compute:\n\n\\(P(X &gt; 240)\\)\n\\(P(160 &lt; X &lt; 220)\\)\nWhat value of \\(X\\) cuts off the top 5%? (What would the 95%th percentile be?)\n\n\n\nShow solution\n\nStandardize:\n\n\\(Z = \\frac{240 - 200}{30} = 1.33 \\Rightarrow P(Z &gt; 1.33)=1-P(Z\\leq 1.33) \\approx 0.0918\\)\nStandardize both: \\(Z_1 = \\frac{160 - 200}{30} = -1.33\\), \\(Z_2 = \\frac{220 - 200}{30} = 0.67\\) ⇒ \\(P(160&lt;X&lt;240)= P(X&lt;240)-P(X&lt;160)=P(Z&lt;0.67)-P(Z&lt;-1.33) \\approx 0.7486 - 0.0918 = 0.6568\\)\nFind \\(z\\) for 95th percentile. Here it’s easiest to start with the 95th percentile of the standard normal distribution, \\(z=1.645\\). \\(z = 1.645 \\Rightarrow X = 200 + 1.645 \\times 30 = 249.35\\)\n\nConfirming with an Python script\n\nprint(\"a)\", 1-stats.norm.cdf(240, loc=200, scale=30))\n\na) 0.09121121972586788\n\n## Gives 0.0912 due to different approximation than above\n\nprint(\"b)\", (stats.norm.cdf(220, loc=200, scale=30) - \nstats.norm.cdf(160, loc=200, scale=30)))\n\nb) 0.6562962427272092\n\n## c) First compute the quantile, then double check\nquant=200+1.645*30\nprint(\"c)\", stats.norm.cdf(quant, loc=200, scale=30))\n\nc) 0.9500150944608786\n\n## Finding the quantile can also do this more directly:\nprint(\"quant = \", stats.norm.ppf(0.95, loc=200, scale=30))\n\nquant =  249.34560880854417\n\n\n\n\n\n\nLet \\(T \\sim t_{12}\\). Feel free to use either a script or table to do this. Compute:\n\n\\(P(T &lt; 2.18)\\)\n\\(P(|T| &gt; 2.18)\\)\nThe 97.5th percentile of the distribution\n\n\n\nShow solution\n\n\n\n\n\nprint(\"a)\", stats.t.cdf(2.18, df = 12))\n\na) 0.9750530883473195\n\n\n\nTwo-tailed:\n\n\\[\nP(|T| &gt; 2.18) =\\cdots= 2 P(T &gt; 2.18) \\approx 2  (1 - 0.975) = 0.05\n\\] We double check with R:\n\nprint(\"b)\", 2*(1-stats.t.cdf(2.18, df=12)))\n\nb) 0.04989382330536096\n\n\n\nIt’s obvious that we’re really close from a) and b), but let’s still use Python to approximate even more exactly.\n\n\nstats.t.ppf(0.975, df = 12)\n\nnp.float64(2.1788128296634177)\n\n\n\n\n\n\nLet \\(Z \\sim \\mathcal{N}(0,1)\\). Compute:\n\n\\(P(Z &gt; -0.5)\\)\n\\(P(Z &lt; 0.84)\\)\nWhat percentile corresponds to \\(Z = 1.28\\)?\n\n\n\nShow solution\n\n\n\\(P(Z &gt; -0.5) = 1 - P(Z &lt; -0.5) = 1 - 0.3085 = 0.6915\\)\n\\(P(Z &lt; 0.84) = 0.7995\\)\n90th percentile (verify: pnorm(1.28) ≈ 0.8997)\n\nWe use Python to verify:\n\nprint(\"a)\", 1-stats.norm.cdf(-0.5))\n\na) 0.6914624612740131\n\nprint(\"b)\", stats.norm.cdf(0.84))\n\nb) 0.7995458067395503\n\n## c) We have the z value, and we want to know what percentle it is. I.e. whats P(Z&lt;1.28)\nprint(\"c)\", stats.norm.cdf(1.28))\n\nc) 0.8997274320455579\n\n\n\n\n\n\nLet \\(Z \\sim \\mathcal{N}(0,1)\\) and \\(T \\sim t_5\\)\n\nCompute the density \\(f_Z(1)\\)\nCompute the density \\(f_T(1)\\)\nCompare and interpret\n\n\n\nShow solution\n\na), b) For densitiy functions in Python we use stats.norm.pdf and stats.t.pdf to compute their values. Since the df is pretty low here, we can expect some difference between the standard normal and the t distribution.\n\nprint(\"a) Normal:\", stats.norm.pdf(1))\n\na) Normal: 0.24197072451914337\n\nprint(\"b) T with 5 degrees of freedom:\", stats.t.pdf(1, df=5))\n\nb) T with 5 degrees of freedom: 0.21967979735098053\n\n\n\nThe t-distribution has heavier tails, so at z = 1, its peak is slightly lower than that of the standard normal. The tails decay more slowly.",
    "crumbs": [
      "Modules",
      "Module 4: Designing studies, hypothesis testing, and quantifying effects",
      "Exercises"
    ]
  },
  {
    "objectID": "3-what-is-a-model.html",
    "href": "3-what-is-a-model.html",
    "title": "What is a model?",
    "section": "",
    "text": "Slides for “What is a model?”\n\n\n\nWhat is a statistical model modelling?\nWhat is a residual?\nWhat does it mean to be identically distributed?\nWhat does it mean to be independently distributed?",
    "crumbs": [
      "Modules",
      "Module 3: Estimation, sampling distributions and resampling",
      "What is a model?"
    ]
  },
  {
    "objectID": "3-what-is-a-model.html#control-questions",
    "href": "3-what-is-a-model.html#control-questions",
    "title": "What is a model?",
    "section": "",
    "text": "What is a statistical model modelling?\nWhat is a residual?\nWhat does it mean to be identically distributed?\nWhat does it mean to be independently distributed?",
    "crumbs": [
      "Modules",
      "Module 3: Estimation, sampling distributions and resampling",
      "What is a model?"
    ]
  },
  {
    "objectID": "3-sampling-error.html",
    "href": "3-sampling-error.html",
    "title": "Sampling error and distribution",
    "section": "",
    "text": "Slides for “Sampling error and distribution”\n\n\n\nWhat is a sampling error?\nWhat is a sampling distribution?\nHow does the sample mean standard deviation depend on the sample size?",
    "crumbs": [
      "Modules",
      "Module 3: Estimation, sampling distributions and resampling",
      "Sampling error and distribution"
    ]
  },
  {
    "objectID": "3-sampling-error.html#control-questions",
    "href": "3-sampling-error.html#control-questions",
    "title": "Sampling error and distribution",
    "section": "",
    "text": "What is a sampling error?\nWhat is a sampling distribution?\nHow does the sample mean standard deviation depend on the sample size?",
    "crumbs": [
      "Modules",
      "Module 3: Estimation, sampling distributions and resampling",
      "Sampling error and distribution"
    ]
  },
  {
    "objectID": "3-python.html",
    "href": "3-python.html",
    "title": "Python",
    "section": "",
    "text": "Here we will present some useful python commands relevant for what we want to do in Module 3.",
    "crumbs": [
      "Modules",
      "Module 3: Estimation, sampling distributions and resampling",
      "Python"
    ]
  },
  {
    "objectID": "3-python.html#generate-random-numbers-from-a-distribution",
    "href": "3-python.html#generate-random-numbers-from-a-distribution",
    "title": "Python",
    "section": "Generate random numbers from a distribution",
    "text": "Generate random numbers from a distribution\n\nfrom scipy import stats\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Generate 2000 random numbers from a uniform distribution: \nx = stats.uniform.rvs(size = 2000)\n# Plot histogram of the simulated data: \nsns.histplot(x, stat = \"density\")\nplt.show()\n# Add uniform density function to the plot:\nsns.histplot(x, stat = \"density\")\nxvalues = np.linspace(0, 1, 2000)\npdf = stats.uniform.pdf(xvalues)\nplt.plot(xvalues, pdf, color = \"red\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIt is often a good idea to set a seed when generating random numbers. This has the benefit that every time you rerun your code, the same random random are generated. If you want a new draw from the distribution, you change the seed and you will get a new set of numbers. Test this out your self using the code below:\n\nnp.random.seed(123)\nx = stats.uniform.rvs(size = 5)\nprint(x)\nnp.random.seed(123)\nx = stats.uniform.rvs(size = 5)\nprint(x)\nnp.random.seed(124)\nx = stats.uniform.rvs(size = 5)\nprint(x)\n\n[0.69646919 0.28613933 0.22685145 0.55131477 0.71946897]\n[0.69646919 0.28613933 0.22685145 0.55131477 0.71946897]\n[0.10606491 0.74547148 0.57231354 0.45824118 0.3847059 ]",
    "crumbs": [
      "Modules",
      "Module 3: Estimation, sampling distributions and resampling",
      "Python"
    ]
  },
  {
    "objectID": "3-python.html#monte-carlo-simulation-yatzi",
    "href": "3-python.html#monte-carlo-simulation-yatzi",
    "title": "Python",
    "section": "Monte Carlo simulation: Yatzi",
    "text": "Monte Carlo simulation: Yatzi\nHere we will implement the Yatzi bonus example used in the video here. First, we need a way to simulate the roll of 5 dice and count the number of ones, twos, etc. We could simulate the actual dice, sampling numbers from 1-6 with equal probability. This can be done by the following code:\n\nimport numpy as np\nnp.random.seed(123)\n# roll 5 dice:\nx1 = np.random.randint(1, 7, size = 5)\nprint(x1)\n\n[6 3 5 3 2]\n\n\nWe could then count the number of 1 ones and roll the remaining dice.\n\n# Count number of ones:\ncount_ones1 = np.sum(x1 == 1) \n # Roll remaining dice:\nx2 = np.random.randint(1, 7, size = 5-count_ones1)\nprint(x2) # Print the outcome\n# Count ones in second roll: \ncount_ones2 = np.sum(x2 == 1) \n# Roll the remaining dice: \nx3 = np.random.randint(1, 7, size = 5-count_ones1-count_ones2)\nprint(x3) # Print the outcome\ncount_ones3 = np.sum(x3 == 1) # count the number of ones\n# Calculate the total score: \nscore = count_ones1+count_ones2+count_ones3\nprint(score)\n\n[4 3 4 2 2]\n[1 2 2 1 1]\n3\n\n\nThis is a bit cumbersome, but it illustrates how one can simulate dice rolls. Since the actual number is not so important for the game, we can rather (as is done in the video), simulate directly the number of ones in the dice roll by simulating from a binomial distribution with sucess probability \\(p=1/6\\) and \\(n=5\\) in the first roll, \\(n-X_1\\) in the second and \\(n-X_1-X_2\\) in the third.\n\nnp.random.seed(321)\nfrom scipy.stats import binom\nx1 = binom.rvs(n=5, p=1/6, size = 1) # First roll\nx2 = binom.rvs(n=5-x1, p=1/6, size = 1) # Second roll \nx3 = binom.rvs(n=5-x1-x2, p=1/6, size = 1) # Second roll \nprint(\"After first roll:\",   x1)\nprint(\"After second roll:\",  x1+x2)\nprint(\"Score of the round:\", x1+x2+x3)\n\nAfter first roll: [2]\nAfter second roll: [2]\nScore of the round: [4]\n\n\nThis was one round of Yatzi, where the score is just the number of equal dice rolls or you could think of it as the round where we collect ones. Let’s make this a function. We also add the argument number, which is the number we are collect in the specific round.\n\nfrom scipy.stats import binom\n\ndef play_round(number, n=5, p=1/6):\n    x1 = binom.rvs(n=n, p=p, size=1)[0]          # First roll\n    x2 = binom.rvs(n=n - x1, p=p, size=1)[0]     # Second roll\n    x3 = binom.rvs(n=n - x1 - x2, p=p, size=1)[0]  # Third roll\n    return (x1 + x2 + x3)*number\n  \n# Play round collecting ones:\nprint(play_round(1))\n\n2\n\n\nand use it to play a full game:\n\ndef play_game():\n    total_score = 0\n    for i in range(1, 7):\n        total_score += play_round(i) # add the next round to total score\n    return total_score\nprint(\"Game score:\", play_game())\n# No bonus this round.\n\nGame score: 34\n\n\nWe can now run many-many games. That is, we are ready to do a Monte Carlo simulation of the game.\n\n# Play 10,000 games: \nscores = np.array([play_game() for _ in range(10_000)])\n\nHaving played 10,000 games, we can estimate the expected score:\n\nm = scores.mean() \nprint(\"Expected score: \", m)\n\nExpected score:  44.237\n\n\nThe probability of achieving the bonus requirement (total score \\(\\le 63\\)):\n\nbonus_prob = np.mean(scores&gt;=63)\nprint(\"Expected score: \", bonus_prob)\n\nExpected score:  0.0432\n\n\nOr plot the distribution of the scores:\n\nsns.histplot(scores, bins = 15, stat = \"density\")\nplt.axvline(63, color = \"red\")\nplt.show()",
    "crumbs": [
      "Modules",
      "Module 3: Estimation, sampling distributions and resampling",
      "Python"
    ]
  },
  {
    "objectID": "3-python.html#bootstrap",
    "href": "3-python.html#bootstrap",
    "title": "Python",
    "section": "Bootstrap",
    "text": "Bootstrap\nWe consider the same height measurements of ten 20-year old male athletes as used in the video on the bootstrap. We can first confirm that the sample mean is 179.6 and the standard deviation is 2.41. This the standard error of the sample mean estimator being \\(\\approx \\sigma/\\sqrt{n}= 2.41/\\sqrt{10}=0.76\\).\n\nimport pandas as pd\nimport math\nnumbers = [183, 179, 179, 183, 178, 176, 182, 177, 180, 179]\nvector = pd.Series(numbers)\nprint(vector.mean())\nprint(vector.std())\nprint(vector.std()/math.sqrt(10))\n\n179.6\n2.412928142780514\n0.7630348761506397\n\n\nTo sample randomly from this vector of observations, we can use the following code:\n\nnp.random.seed(1234)\nnewsample = vector.sample(3, replace = False)\nprint(newsample)\n\n7    177\n2    179\n9    179\ndtype: int64\n\n\nAs you can see from the output, I have sampled 3 heights from the vector. Here I have sampled without replacement, so I know that these are three different observations selected at random. I set a seed to get the same three each time I run the code.\nNow, say we want to use the bootstrap method to find the sampling distribution of the sample mean estimator from just this one sample of ten observations. We then pretend that the data we have (the ten observations) is our population, and we repeatedly sample from the vector with replacement. For each new sample we calculate the average value and the vector containing many such mean value (boot_means) is treated as observations of the sample mean estimators for multiple samples. We can summarize the sampling distribution by e.g. considering the standard deviation of the boot_means. This should be close to the \\(\\sigma/\\sqrt{n}\\) value that we found above, due to the central limit theorem and the fact that we are considering a mean estimator here.\n\nnp.random.seed(123)\nbootstrap_samples = 2000\nboot_means = np.zeros(bootstrap_samples)\nfor b in range(0, bootstrap_samples) : \n    boot_sample = vector.sample(len(vector), replace = True)\n    boot_means[b] = boot_sample.mean()\nprint(boot_means.std())\n\n0.7292611997768702\n\n\nWe can also plot the sampling distribution:\n\nfrom scipy.stats import norm\n# Histogram of sampling distribution\nsns.histplot(boot_means, stat = \"density\", binwidth = .2)\nplt.axvline(x=vector.mean(), color = \"red\")\n\n# Add normal density curve: \nx = np.linspace(min(boot_means), max(boot_means), 500)\nmu = np.mean(vector)\nsigma = np.std(vector, ddof=1)/np.sqrt(10)\n\nplt.plot(x, norm.pdf(x, mu, sigma), color=\"black\", linewidth=2)\n\nplt.show()\n\n\n\n\n\n\n\n\nNote that the black curve is based on the normal approximation (central limit theorem). Quite a good fit!\nNow, let us consider a slightly larger dataset. This example is from the python supplement to the textbook. Here we imagine that the full NHANES dataset is our population. We pull out a sample of size 100 as our “original sample”. We then bootstrap the sampling distribution of the mean height. Here, we are not interested in the mean or the standard deviation, but the 2.5 and 97.5 percentiles. This gives the central interval containing 95% of the population.\n\nfrom nhanes.load import load_NHANES_data\nimport pandas as pd\nnhanes_data = load_NHANES_data()\nadult_nhanes_data = nhanes_data.query('AgeInYearsAtScreening &gt; 17')\nadult_nhanes_data = adult_nhanes_data.dropna(subset=['StandingHeightCm']).rename(columns={'StandingHeightCm': 'Height'})\n\nnum_runs = 5000\nsample_size = 100\n\n# Take a sample for which we will perform the bootstrap\n\nnhanes_sample = adult_nhanes_data.sample(sample_size)\n\n# Perform the resampling\n\nbootstrap_df = pd.DataFrame({'mean': np.zeros(num_runs)})\nfor sampling_run in range(num_runs):\n    bootstrap_sample = nhanes_sample.sample(sample_size, replace=True)\n    bootstrap_df.loc[sampling_run, 'mean'] = bootstrap_sample['Height'].mean()\n\n# Compute the 2.5% and 97.5% percentiles of the distribution\nbootstrap_ci = np.percentile(bootstrap_df['mean'], [2.5, 97.5])\nprint(\"Bootstrap 95% CI:\", bootstrap_ci)\n\n# Normal approximation:\nsample_mean = nhanes_sample[\"Height\"].mean()\nsample_std = nhanes_sample[\"Height\"].std()/np.sqrt(sample_size)\nCLT_ci = norm.ppf([0.025,0.975], sample_mean, sample_std)\nprint(\"Normal approximation 95% CI:\", CLT_ci)\n\nBootstrap 95% CI: [165.458975 168.960025]\nNormal approximation 95% CI: [165.4873607 168.9686393]\n\n\nIn this case, we expect the two approaches to give similar results, and this is also what we get here.",
    "crumbs": [
      "Modules",
      "Module 3: Estimation, sampling distributions and resampling",
      "Python"
    ]
  },
  {
    "objectID": "3-monte-carlo-simulation.html",
    "href": "3-monte-carlo-simulation.html",
    "title": "Monte Carlo simulation",
    "section": "",
    "text": "Slides for “Monte Carlo simulation”\n\n\n\nWhat is the purpose of using Monte Carlo simulations?\nWhat does the law of large numbers have to do with Monte Carlo simulations?\nWhat is the expected total score after the 6th round in Yatzi?\nWhat is the probability of achieving the bonus in Yatzi?\n\nYou will find how to implement this Yatzi Monte Carlo simulation here.",
    "crumbs": [
      "Modules",
      "Module 3: Estimation, sampling distributions and resampling",
      "Monte Carlo simulation"
    ]
  },
  {
    "objectID": "3-monte-carlo-simulation.html#control-questions",
    "href": "3-monte-carlo-simulation.html#control-questions",
    "title": "Monte Carlo simulation",
    "section": "",
    "text": "What is the purpose of using Monte Carlo simulations?\nWhat does the law of large numbers have to do with Monte Carlo simulations?\nWhat is the expected total score after the 6th round in Yatzi?\nWhat is the probability of achieving the bonus in Yatzi?\n\nYou will find how to implement this Yatzi Monte Carlo simulation here.",
    "crumbs": [
      "Modules",
      "Module 3: Estimation, sampling distributions and resampling",
      "Monte Carlo simulation"
    ]
  },
  {
    "objectID": "3-intro.html",
    "href": "3-intro.html",
    "title": "TECH3 Applied statistics",
    "section": "",
    "text": "Estimation, sampling distributions and resampling\n\nFocus: Modern inference using Monte Carlo methods.\nKey topics: Central limit theorem, sampling error of various statistics, Monte Carlo simulation.\nSpecial Emphasis: Distinguish between a population and a sample and between population parameters and sample statistics.",
    "crumbs": [
      "Modules",
      "Module 3: Estimation, sampling distributions and resampling",
      "Introduction"
    ]
  },
  {
    "objectID": "3-central-limit-theorem.html",
    "href": "3-central-limit-theorem.html",
    "title": "Central Limit Theorem",
    "section": "",
    "text": "Slides for “Central limit theorem”\n\n\n\nWhat does the central limit theorem tell us?\nWhat are the assumptions for the central limit theorem?\nWhat is the asymptotic variance of the mean?\nWhat is becoming large in the central limit theorem?\nWill a sum also be normally distributed when the sample size is large?\nDoes the sampled data have to be generated from a normal distribution for the central limit theorem to hold?",
    "crumbs": [
      "Modules",
      "Module 3: Estimation, sampling distributions and resampling",
      "Central Limit Theorem"
    ]
  },
  {
    "objectID": "3-central-limit-theorem.html#control-questions",
    "href": "3-central-limit-theorem.html#control-questions",
    "title": "Central Limit Theorem",
    "section": "",
    "text": "What does the central limit theorem tell us?\nWhat are the assumptions for the central limit theorem?\nWhat is the asymptotic variance of the mean?\nWhat is becoming large in the central limit theorem?\nWill a sum also be normally distributed when the sample size is large?\nDoes the sampled data have to be generated from a normal distribution for the central limit theorem to hold?",
    "crumbs": [
      "Modules",
      "Module 3: Estimation, sampling distributions and resampling",
      "Central Limit Theorem"
    ]
  },
  {
    "objectID": "2-what-is-probability.html",
    "href": "2-what-is-probability.html",
    "title": "What is probability?",
    "section": "",
    "text": "Slides for “What is probability?”\n\n\n\nHow would you interpret an event with probability 0?\nHow would you interpret an event with probability 1?\nWhat is the key assumption behind the assigning probability using the classical method?\nHow would a frequentist interpret a probability?\nWhat is a subjective probability?\nWhat is a Bayesian probability?",
    "crumbs": [
      "Modules",
      "Module 2: Probability, random variables, probability distributions and simulations.",
      "What is probability?"
    ]
  },
  {
    "objectID": "2-what-is-probability.html#controll-questions",
    "href": "2-what-is-probability.html#controll-questions",
    "title": "What is probability?",
    "section": "",
    "text": "How would you interpret an event with probability 0?\nHow would you interpret an event with probability 1?\nWhat is the key assumption behind the assigning probability using the classical method?\nHow would a frequentist interpret a probability?\nWhat is a subjective probability?\nWhat is a Bayesian probability?",
    "crumbs": [
      "Modules",
      "Module 2: Probability, random variables, probability distributions and simulations.",
      "What is probability?"
    ]
  },
  {
    "objectID": "2-python.html",
    "href": "2-python.html",
    "title": "Python",
    "section": "",
    "text": "For module 2, the relevant Python functions are the probability distributions functions. The list of probability distributions is extensive. We will only scratch the surface in TECH3 and consider some very commonly used distributions. An important distinction is between discrete and continuous distributions.\nA discrete distribution describes outcomes that take on distinct, countable values. Typical examples include the number of successes in a series of trials or the number of arrivals in a queue. These distributions are characterized by probability mass functions (PMFs), which assign a probability to each possible outcome.\nA continuous distribution, on the other hand, models outcomes that can take on any value within an interval of real numbers. Instead of individual probabilities, they use probability density functions (PDFs), where probabilities are obtained by integrating over an interval. Examples include measurements such as time, distance, prices, or temperature.\nA probability mass function (PMF) is used for discrete distributions and gives the probability of each specific outcome. In contrast, a probability density function (PDF) is used for continuous distributions and does not give probabilities directly; instead, probabilities are obtained by integrating the density over an interval. Both types of distributions share two related functions: the cumulative distribution function (CDF), which gives the probability that a random variable is less than or equal to a given value, and the percentage point (quantile) function, which is the inverse of the CDF and returns the value corresponding to a specified cumulative probability.\nIn Python we find relevant function for (almost) all distributions you can think of in the library (complete list here).\nfrom scipy import stats\nBelow is a table of distributions, classified as either discrete or contiuous, and the name of the respective scipy.stats distribution name or class.",
    "crumbs": [
      "Modules",
      "Module 2: Probability, random variables, probability distributions and simulations.",
      "Python"
    ]
  },
  {
    "objectID": "2-python.html#continuous-uniform-distribution",
    "href": "2-python.html#continuous-uniform-distribution",
    "title": "Python",
    "section": "Continuous: Uniform distribution",
    "text": "Continuous: Uniform distribution\nThe uniform distribution is characterized by having a constant density and being defined one a closed interval. That is, if \\(X\\sim U(a,b)\\), the density is \\[f(x) = \\frac{1}{b-a},\\quad a\\le x\\le b,\\, a&lt;b.\\] The corresponding cumulative distribution function is \\[\nF(X) = \\int_a^x f(k)\\, dk = \\int_a^x \\frac{1}{b-a} dk = \\frac{k}{b-a}\\bigg|_a^x = \\frac{x-a}{b-a},\\quad a\\le x \\le b.\n\\] The CDF of a uniform distribution function is the inverse of \\(F(x)\\), which we can actually express explicitly. We need to solve \\(p=F(x)\\) for x. \\[\np= F(x) = \\frac{x-a}{b-a}\\quad \\Leftrightarrow \\quad\nx = a+(b-a)p=Q(p), \\quad 0\\le p\\le1.\n\\] Hence, the quantile function of a uniform distribution is linear in \\(p\\).\nWe plot the three functions for a U(0,1) distribution, i.e. \\(a=0\\) and \\(b=1\\). This would mean: \\(f(x) = 1/(1-0)=1\\), \\(F(x) = x\\) and \\(Q(p)=p\\).\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.linspace(0, 1, 2000)\nppts = np.linspace(0, 1, 2000)\npdf = stats.uniform.pdf(x)\ncdf = stats.uniform.cdf(x)\nppf = stats.uniform.ppf(ppts)\n\n# Plot density\nplt.plot(x,pdf)\nplt.ylabel(\"Uniform(0,1) density\")\nplt.xlabel(\"x\")\nplt.ylabel(\"f(x)\")\nplt.show()\n\n# Plot cumulative distribution function\nplt.plot(x,cdf)\nplt.xlabel(\"x\")\nplt.ylabel(\"P(X ≤ x)\")\nplt.ylabel(\"Uniform(0,1) CDF\")\nplt.show()\n# Plot percentage point function (quantile function):\nplt.plot(ppts,ppf)\nplt.xlabel(\"p\")\nplt.ylabel(\"Q(p)\")\nplt.ylabel(\"Uniform(0,1) quantile function (PPF)\")\nplt.show()",
    "crumbs": [
      "Modules",
      "Module 2: Probability, random variables, probability distributions and simulations.",
      "Python"
    ]
  },
  {
    "objectID": "2-python.html#continuous-normal-distribution",
    "href": "2-python.html#continuous-normal-distribution",
    "title": "Python",
    "section": "Continuous: Normal distribution",
    "text": "Continuous: Normal distribution\nThe normal density function is given by \\[\nf(x) = \\frac{1}{\\sqrt{2\\pi}\\,\\sigma}\n\\exp\\!\\left( -\\frac{(x - \\mu)^2}{2\\sigma^2} \\right),\\quad  x\\in\\mathcal R,\\, \\mu\\in\\mathcal R,\\, \\sigma &gt;0.\n\\] Here \\(\\mu\\) is the expected value of \\(X\\) and \\(\\sigma^2\\) is the variance. The CDF is then given by the integral: \\[\nF(x)=P(X\\le x)\n= \\frac{1}{\\sqrt{2\\pi}\\,\\sigma}\n\\int_{-\\infty}^{x}\n\\exp\\!\\left( -\\frac{(t-\\mu)^2}{2\\sigma^2} \\right)\\, dt.\n\\] The quantile function is then the inverse of \\(F\\), i.e. \\[\nQ(p) = \\inf\\{ x \\in \\mathbb{R} : F(x) \\ge p \\}.\n\\] While the CDF tells you how likely \\(X\\) is to be below a value, the quantile function tells you which value of \\(X\\) corresponds to a chosen probability.\nBelow we plot the three function for a standard normal distribution. That is, a normal distribution with expectation \\(\\mu=0\\) and variance \\(\\sigma^2=1\\).\n\nppts = np.linspace(0,1,1000)\nx = np.linspace(-5, 5, 2000)\npdf = stats.norm.pdf(x)\ncdf = stats.norm.cdf(x)\nppf = stats.norm.ppf(ppts)\n\n# Plot density\nplt.plot(x, pdf)\nplt.title(\"Standard Normal density\")\nplt.xlabel(\"x\")\nplt.ylabel(\"f(x)\")\nplt.show()\n# Plot cumulative distribution function\nplt.plot(x, cdf)\nplt.title(\"Standard Normal CDF\")\nplt.xlabel(\"x\")\nplt.ylabel(\"P(X ≤ x)\")\nplt.show()\n# Plot percentage point function (quantile function):\nplt.plot(ppts, ppf)\nplt.title(\"Standard Normal quantile function (PPF)\")\nplt.xlabel(\"p\")\nplt.ylabel(\"Q(p)\")\nplt.show()",
    "crumbs": [
      "Modules",
      "Module 2: Probability, random variables, probability distributions and simulations.",
      "Python"
    ]
  },
  {
    "objectID": "2-intro.html",
    "href": "2-intro.html",
    "title": "TECH3 Applied statistics",
    "section": "",
    "text": "Probability, random variables, probability distributions and simulations\n\nFocus: Random variables, distributions and the link to simulations.\nKey topics: Probability distributions, conditional probability, independence, the law of large numbers, pseudo-random numbers.\nSpecial Emphasis: Estimating the probability of complex outcomes through simulations.",
    "crumbs": [
      "Modules",
      "Module 2: Probability, random variables, probability distributions and simulations.",
      "Introduction"
    ]
  },
  {
    "objectID": "2-exp-var-discrete.html",
    "href": "2-exp-var-discrete.html",
    "title": "Expectation and variance of discrete random variables",
    "section": "",
    "text": "Slides for “Expectation and variance of discrete random variables”\n\n\n\nHow do you compute the expected value of discrete random variables?\nHow do you interpret an expected value?\nHow do you compute the variance of a discrete random variable?\nHow do you interpret the variance?",
    "crumbs": [
      "Modules",
      "Module 2: Probability, random variables, probability distributions and simulations.",
      "Expectation and variance of discrete random variables"
    ]
  },
  {
    "objectID": "2-exp-var-discrete.html#controll-questions",
    "href": "2-exp-var-discrete.html#controll-questions",
    "title": "Expectation and variance of discrete random variables",
    "section": "",
    "text": "How do you compute the expected value of discrete random variables?\nHow do you interpret an expected value?\nHow do you compute the variance of a discrete random variable?\nHow do you interpret the variance?",
    "crumbs": [
      "Modules",
      "Module 2: Probability, random variables, probability distributions and simulations.",
      "Expectation and variance of discrete random variables"
    ]
  },
  {
    "objectID": "2-discrete-random-variables-and-distributions.html",
    "href": "2-discrete-random-variables-and-distributions.html",
    "title": "Discrete random variables and distributions",
    "section": "",
    "text": "Slides for “Discrete random variables and distributions?”\n\n\n\nWhat is a random variable?\nWhat is a discrete random variable?\nWhat is the probability distribution of a discrete random variable?\nHow do you visualize a discrete probability distribution?\nCan you name one discrete probability distribution?",
    "crumbs": [
      "Modules",
      "Module 2: Probability, random variables, probability distributions and simulations.",
      "Discrete random variables and distributions"
    ]
  },
  {
    "objectID": "2-discrete-random-variables-and-distributions.html#controll-questions",
    "href": "2-discrete-random-variables-and-distributions.html#controll-questions",
    "title": "Discrete random variables and distributions",
    "section": "",
    "text": "What is a random variable?\nWhat is a discrete random variable?\nWhat is the probability distribution of a discrete random variable?\nHow do you visualize a discrete probability distribution?\nCan you name one discrete probability distribution?",
    "crumbs": [
      "Modules",
      "Module 2: Probability, random variables, probability distributions and simulations.",
      "Discrete random variables and distributions"
    ]
  },
  {
    "objectID": "2-conditional-probability.html",
    "href": "2-conditional-probability.html",
    "title": "Conditional probability",
    "section": "",
    "text": "Slides for “Conditional probability”\n\n\n\nHow is conditional probability \\(P(A|B)\\) defined, and what assumption must hold for it to be valid?\nWhen we condition on an event \\(B\\), how does the sample space change? Use the dice example to explain.\nExplain why the formula for conditional probability divides the probability of \\(A\\cap B\\) by the probability of \\(B\\).\nIf a die roll is known to be odd, what is the probability of rolling a 5?\nHow does the concept of conditional probability apply to real-world scenarios, such as predicting outcomes based on partial information? Provide an example.",
    "crumbs": [
      "Modules",
      "Module 2: Probability, random variables, probability distributions and simulations.",
      "Conditional probability"
    ]
  },
  {
    "objectID": "2-conditional-probability.html#controll-questions",
    "href": "2-conditional-probability.html#controll-questions",
    "title": "Conditional probability",
    "section": "",
    "text": "How is conditional probability \\(P(A|B)\\) defined, and what assumption must hold for it to be valid?\nWhen we condition on an event \\(B\\), how does the sample space change? Use the dice example to explain.\nExplain why the formula for conditional probability divides the probability of \\(A\\cap B\\) by the probability of \\(B\\).\nIf a die roll is known to be odd, what is the probability of rolling a 5?\nHow does the concept of conditional probability apply to real-world scenarios, such as predicting outcomes based on partial information? Provide an example.",
    "crumbs": [
      "Modules",
      "Module 2: Probability, random variables, probability distributions and simulations.",
      "Conditional probability"
    ]
  },
  {
    "objectID": "2-bernoulli.html",
    "href": "2-bernoulli.html",
    "title": "Bernoulli distribution",
    "section": "",
    "text": "Slides for “The Bernoulli distribution”\n\n\n\nWhat is a Bernoulli trial?\nHow is the Bernoulli distribution defined, and what role does probability p play in it?",
    "crumbs": [
      "Modules",
      "Module 2: Probability, random variables, probability distributions and simulations.",
      "Bernoulli distribution"
    ]
  },
  {
    "objectID": "2-bernoulli.html#controll-questions",
    "href": "2-bernoulli.html#controll-questions",
    "title": "Bernoulli distribution",
    "section": "",
    "text": "What is a Bernoulli trial?\nHow is the Bernoulli distribution defined, and what role does probability p play in it?",
    "crumbs": [
      "Modules",
      "Module 2: Probability, random variables, probability distributions and simulations.",
      "Bernoulli distribution"
    ]
  },
  {
    "objectID": "2-basic-probability-rules.html",
    "href": "2-basic-probability-rules.html",
    "title": "Basic probability rules",
    "section": "",
    "text": "Slides for “Basic probability rules”\n\n\n\nWhat are the three axioms of probability?\nWhat does it mean for two events to be disjoint, and how does the third axiom apply to such events?\nExplain the complement rule in your own words.\nWhen calculating the probability of the union of two events, why do we need to subtract the probability of their intersection?\nDescribe the Law of Total Probability.",
    "crumbs": [
      "Modules",
      "Module 2: Probability, random variables, probability distributions and simulations.",
      "Basic probability rules"
    ]
  },
  {
    "objectID": "2-basic-probability-rules.html#controll-questions",
    "href": "2-basic-probability-rules.html#controll-questions",
    "title": "Basic probability rules",
    "section": "",
    "text": "What are the three axioms of probability?\nWhat does it mean for two events to be disjoint, and how does the third axiom apply to such events?\nExplain the complement rule in your own words.\nWhen calculating the probability of the union of two events, why do we need to subtract the probability of their intersection?\nDescribe the Law of Total Probability.",
    "crumbs": [
      "Modules",
      "Module 2: Probability, random variables, probability distributions and simulations.",
      "Basic probability rules"
    ]
  },
  {
    "objectID": "1-working-with-data.html",
    "href": "1-working-with-data.html",
    "title": "Working with data",
    "section": "",
    "text": "Slides for “Working with data”\n\n\n\nWhat is a categorical variable?\nWhat is a numerical variable?\nIs precipitation in millimeters a real number or an integer?\nIs a binary variable numeric or categorical?\nWhat is a good measurement?\nCan measurement error arise due to typos?",
    "crumbs": [
      "Modules",
      "Module 1: Summarizing and visualizing data",
      "Working with data"
    ]
  },
  {
    "objectID": "1-working-with-data.html#controll-questions",
    "href": "1-working-with-data.html#controll-questions",
    "title": "Working with data",
    "section": "",
    "text": "What is a categorical variable?\nWhat is a numerical variable?\nIs precipitation in millimeters a real number or an integer?\nIs a binary variable numeric or categorical?\nWhat is a good measurement?\nCan measurement error arise due to typos?",
    "crumbs": [
      "Modules",
      "Module 1: Summarizing and visualizing data",
      "Working with data"
    ]
  },
  {
    "objectID": "1-textbook.html",
    "href": "1-textbook.html",
    "title": "TECH3 Applied statistics",
    "section": "",
    "text": "The following is a redistribution of chapters 1-4 of Statistical Thinking for the 21st Century by Russell A. Poldrack, 2019, under the CC BY-NC 4.0 licence. Chapters 2-4 are of main interest for this module, but chapter 1 is also part of the curriculum and gives an introduction to statistical thinking.",
    "crumbs": [
      "Modules",
      "Module 1: Summarizing and visualizing data",
      "Textbook"
    ]
  },
  {
    "objectID": "1-textbook.html#introduction",
    "href": "1-textbook.html#introduction",
    "title": "TECH3 Applied statistics",
    "section": "Introduction",
    "text": "Introduction",
    "crumbs": [
      "Modules",
      "Module 1: Summarizing and visualizing data",
      "Textbook"
    ]
  },
  {
    "objectID": "1-textbook.html#working-with-data",
    "href": "1-textbook.html#working-with-data",
    "title": "TECH3 Applied statistics",
    "section": "Working with data",
    "text": "Working with data",
    "crumbs": [
      "Modules",
      "Module 1: Summarizing and visualizing data",
      "Textbook"
    ]
  },
  {
    "objectID": "1-textbook.html#summarizing-data",
    "href": "1-textbook.html#summarizing-data",
    "title": "TECH3 Applied statistics",
    "section": "Summarizing data",
    "text": "Summarizing data",
    "crumbs": [
      "Modules",
      "Module 1: Summarizing and visualizing data",
      "Textbook"
    ]
  },
  {
    "objectID": "1-textbook.html#data-visualization",
    "href": "1-textbook.html#data-visualization",
    "title": "TECH3 Applied statistics",
    "section": "Data visualization",
    "text": "Data visualization",
    "crumbs": [
      "Modules",
      "Module 1: Summarizing and visualizing data",
      "Textbook"
    ]
  },
  {
    "objectID": "1-summarizing-data.html",
    "href": "1-summarizing-data.html",
    "title": "Summarizing data",
    "section": "",
    "text": "Slides for “Summarizing data”\nSlides for “Summarizing data using tables”\n\n\n\nWhy do we need to summarize data?\nWhat is the difference between a frequency and a relative frequency?\nWhat is a cumulative frequency?",
    "crumbs": [
      "Modules",
      "Module 1: Summarizing and visualizing data",
      "Summarizing data"
    ]
  },
  {
    "objectID": "1-summarizing-data.html#controll-questions",
    "href": "1-summarizing-data.html#controll-questions",
    "title": "Summarizing data",
    "section": "",
    "text": "Why do we need to summarize data?\nWhat is the difference between a frequency and a relative frequency?\nWhat is a cumulative frequency?",
    "crumbs": [
      "Modules",
      "Module 1: Summarizing and visualizing data",
      "Summarizing data"
    ]
  },
  {
    "objectID": "1-python.html",
    "href": "1-python.html",
    "title": "Python",
    "section": "",
    "text": "Here we will present some useful python commands relevant for what we want to do in Module 1.",
    "crumbs": [
      "Modules",
      "Module 1: Summarizing and visualizing data",
      "Python"
    ]
  },
  {
    "objectID": "1-python.html#summarizing-data-in-python",
    "href": "1-python.html#summarizing-data-in-python",
    "title": "Python",
    "section": "Summarizing data in Python",
    "text": "Summarizing data in Python\nIn this section, we will look closer at how we can calculate these summary statistics in Python, both for a single vector of numbers and for a dataset.\nA small startup tracks how many new users signed up each day (in hundreds) during a 12-day promotional campaign. The daily numbers are: \\[9.0, 5.4, 12.3, 6.6, 15.5, 7.0, 11.3, 6.6, 8.4, 14.4, 10.3, 13.2\\] We start by reading the data into Python:\n\nimport pandas as pd\n# Import numbers as vector: \nx = pd.Series((9.0, 5.4, 12.3, 6.6, 15.5, 7.0, 11.3, 6.6, 8.4, 14.4, 10.3, 13.2))\n\nWe can find the mean by doing the calculations\n\\[\\bar x = \\frac{1}n \\sum_{i=1}^n x_i = \\frac1{10}(9.0+5.4+12.3+\\cdots +13.2) = \\frac{120}{12}=10.0.\\]\nor use Python:\n\nx.mean()\n\nnp.float64(10.0)\n\n\nThe mode is the most frequent observation. This is mostly relevant for discrete observations, but in this particular example we have two observations with value 6.6, and therefore this is the mode. In Python, we would generate a frequency table and the most frequent value is the mode:\n\nx.value_counts()\n\n6.6     2\n5.4     1\n9.0     1\n12.3    1\n15.5    1\n7.0     1\n11.3    1\n8.4     1\n14.4    1\n10.3    1\n13.2    1\nName: count, dtype: int64\n\n\nHence, the mode is 6.6.\nTo find the minimum, maximum and median, we sort the numbers:\n\nx.sort_values()\n\n1      5.4\n3      6.6\n7      6.6\n5      7.0\n8      8.4\n0      9.0\n10    10.3\n6     11.3\n2     12.3\n11    13.2\n9     14.4\n4     15.5\ndtype: float64\n\n\nand find the minimum value, maximum value and middle observation. The minimum is 5.4 and the maximum is 15.5. In this case, we have two observations in the middle (9.0 and 10.3). There are several ways of calculating the median in such cases. One way is to find the middle point of these two middle observations: \\[\\text{Median} = \\frac{9.0+10.3}2 = 9.65.\\] In Python, we find it by:\n\nprint(\"Minimum:\") \nprint(x.min())\nprint(\"\\nMaximum:\")\nprint(x.max())\nprint(\"\\nMedian:\")\nprint(x.median())\n\nMinimum:\n5.4\n\nMaximum:\n15.5\n\nMedian:\n9.65\n\n\nThe median is middle observation. To find 1st and 3rd quartiles (25% and 75% percentiles), we find the observation that has 25% and 75% of the data smaller than itself. With 12 observations, this is sorted observation number \\((12-1)*25\\%+1 = 3.75\\) and \\((12-1)*75\\% +1= 9.25\\). The 25th percentile is thus between sorted observations 3 and 4, and we find it by a weighted average of the two. Since 3.75 is closer to 4, the third observation gets a higher weight. \\[\\text{1st quartile}=\\text{25th percentile}=0.25 \\cdot x_{(3)}+0.75 \\cdot x_{(4)}=0.25\\cdot 6.6+0.75\\cdot 7.0 = 6.9.\\] Likewise, \\[\\text{3rd quartile}=\\text{75th percentile}=0.75 \\cdot x_{(9)}+0.25 \\cdot x_{(10)}=0.75\\cdot 12.3+0.25\\cdot 13.2 = 12.525\\]\nIn Python, we find these quantities by\n\nprint(\"1st quartile:\")\nprint(x.quantile(.25))\nprint(\"\\n3rd quartile:\")\nprint(x.quantile(.75))\n\n1st quartile:\n6.9\n\n3rd quartile:\n12.525\n\n\nA very central measure of variability in the data is the standard deviation and variance. We can find the variance of the data by \\[s^2 = \\frac{1}{n-1}\\sum_{i=1}^n (x_i-\\bar x)^2 = \\frac1{11}((9.0-10.0)^2+ (5.4-10.0)^2+\\cdots +(13.2-10.0)^2)=\\frac{123.76}{11}=11.25\\] and then find the standard deviation \\(s\\) by taking the square root of the variance: \\[s=\\sqrt{s^2}=\\sqrt{15.61}=3.35.\\] In Python, we find this by\n\nprint(\"Variance:\")\nprint(x.var())\nprint(\"\\nStandard deviation:\")\nprint(x.std())\n\nVariance:\n11.250909090909092\n\nStandard deviation:\n3.354237482783396\n\n\nThe inter quartile range is simply the difference between the 75th and 25th percentile of the data, which we found above. Thus \\[\\text{IQR}=\\text{75th percentile}-\\text{25th percentile} = 12.525 -6.9 = 5.625\\] In Python:\n\n# interquartile range\nIQR = x.quantile(.75)-x.quantile(.25)\nprint(IQR)\n\n5.625\n\n\n\nCredit card dataset\nSay we have a simulated data with the following variables:\n\ndefault: Binary (yes/no)\nstudent: Binary (yes/no)\nbalance: Numeric. Balance on credict card after monthly payment.\nincome: Numeric. Income of customer.\n\nThe data is from the book Introduction to statistical learning. We can load the data by:\n\nimport pandas as pd\n# load the data: \ndefault = pd.read_csv(\"https://raw.githubusercontent.com/holleland/TECH3/refs/heads/main/data/Default.csv\", \n                      sep = \";\")\n# Print first rows: \ndefault.head()\n\n\n\n\n\n\n\n\nid\ndefault\nstudent\nbalance\nincome\n\n\n\n\n0\n1\nNo\nNo\n729.526495\n44361.62507\n\n\n1\n2\nNo\nYes\n817.180407\n12106.13470\n\n\n2\n3\nNo\nNo\n1073.549164\n31767.13895\n\n\n3\n4\nNo\nNo\n529.250605\n35704.49394\n\n\n4\n5\nNo\nNo\n785.655883\n38463.49588\n\n\n\n\n\n\n\nWe can then find summary statistics for the income variable similarly to what we did above:\n\nprint(\"Mean income:\")\nprint(default[\"income\"].mean())\nprint(\"Median income:\")\nprint(default[\"income\"].median())\n\nprint(\"Standard deviation of income:\")\nprint(default[\"income\"].std())\n\nMean income:\n33516.98187595744\nMedian income:\n34552.6448\nStandard deviation of income:\n13336.639562731905\n\n\nWe could also find the mode of the variable default (binary) by making a frequency table:\n\n# Frequency of defaults\ndefault[\"default\"].value_counts()\n\ndefault\nNo     9667\nYes     333\nName: count, dtype: int64\n\n\nHence, “No” is the modal value of the default column.\n\n\nGroup by\nSometimes, we want to calculate summary statistics for different groups of data. For instance, in the dataset above, say we wanted to calculate the average income of students and non-students. In the Pandas library, there is functionality for this:\n\nprint(\"Mean grouped by student status: \\n\")\nprint(default.groupby(\"student\")[\"income\"].mean())\nprint(\"Standard deviation grouped by student status: \\n\")\nprint(default.groupby(\"student\")[\"income\"].std())\n\nMean grouped by student status: \n\nstudent\nNo     40011.952857\nYes    17950.230775\nName: income, dtype: float64\nStandard deviation grouped by student status: \n\nstudent\nNo     10010.288665\nYes     4533.007954\nName: income, dtype: float64\n\n\nThus, students have a average income of 17,950 while nonstudents have an average income of 40,012. Similarly, the standard deviations are 4533 and 10,010, respectively.\n\n\nloc\nIt is also useful to filter the data, meaning that we select just a specific set of observations (rows). Say for instance, that we did not want to calculate the mean and standard deviation of income for both students and nonstudents, but only do it for the students. We can do this by first making a new data object that we call students, which only contains rows where the variable student is “Yes”, and the calculate the summary statistics for this new dataset. We do this, using the loc functionality:\n\nstudents = default.loc[default[\"student\"] == \"Yes\"]\nprint(\"Mean income of students: \")\nprint(students[\"income\"].mean())\nprint(\"\\nStandard deviation of student income:\")\nprint(students[\"income\"].std())\n\nMean income of students: \n17950.230775053467\n\nStandard deviation of student income:\n4533.00795394927\n\n\nThis was filtering based on a discrete (binary) variable, but we could also filter based on numerical comparisons. Say we wanted to find the average and standard deviation of income for students with a balances above 1900, and how many students have a balance above 1900? What is the maximum balance that students have in the dataset?\n\n# Select students with balance above 1900:\nstudents1900 = default.loc[\n   (default[\"student\"] == \"Yes\") & (default[\"balance\"] &gt; 1900)\n]\nprint(\"Mean income of students with balance &gt;1900\")\nprint(students1900[\"income\"].mean())\nprint(\"Income standard deviation for students with balance &gt;1900\")\nstudents1900[\"income\"].std()\n\nprint(\"\\nHow many students have balance above 1900?\")\nprint(len(students)) # number of rows\n\nprint(\"\\nWhat is the maximum balance of a student in the data?\")\nprint(students1900[\"balance\"].max())\n\nMean income of students with balance &gt;1900\n17859.54731112371\nIncome standard deviation for students with balance &gt;1900\n\nHow many students have balance above 1900?\n2944\n\nWhat is the maximum balance of a student in the data?\n2654.322576",
    "crumbs": [
      "Modules",
      "Module 1: Summarizing and visualizing data",
      "Python"
    ]
  },
  {
    "objectID": "1-python.html#visualization-in-python",
    "href": "1-python.html#visualization-in-python",
    "title": "Python",
    "section": "Visualization in Python",
    "text": "Visualization in Python\nWe continue using the default dataset and load the plotting libraries matplotlib and seaborn. We also need numpy.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\nData visualization in Python is usually done either using the matplotlib.pyplot (plt) library or seaborn (sns). Seaborn builds on matplotlib and many may find that it is easier to get nice looking grapghics using seaborn than with matplotlib, but this may depend on your own python programming level and preferences. I often like to start out using seaborn and then use matplotlib to add features to the figure or making graphical adjustments.\n\nHistogram\nA histogram can be plotted using the matplotlib:\n\nplt.hist(default[\"income\"]);\nplt.show(); # print figure\n\n\n\n\n\n\n\n\nWhen making a histogram, the most relevant element to tweak is perhaps the bins. This can be done by either changing the number of bins or the bin width.\n\nplt.hist(default[\"income\"], bins = 20);\nplt.show(); # print figure\n\n\n\n\n\n\n\n\nBy increasing the number of bins to 20, we now see clearer the two tops in the income distribution. We can also specify the bins as a vector if we want the splits to occur at specfic locations, e.g. every 2000 dollars, by the following code.\n\nbins = np.arange(start=0, stop=80000, step=2000)\nplt.hist(default[\"income\"], bins=bins);\nplt.show();\n\n\n\n\n\n\n\n\nNote that the y-axis here shows the frequency of observations in each bin. It is often relevant to change this to a relative frequency instead:\n\nplt.hist(default[\"income\"], bins = bins, density = True);\nplt.show(); # print figure\n\n\n\n\n\n\n\n\nWe can do this similarly using seaborn:\n\nsns.histplot(x = \"income\", \n             data = default, \n             bins = bins,\n             stat = \"density\");\nplt.show();\n\n\n\n\n\n\n\n\nSome may prefer this syntax of feeding the function a data object and specifying which column to use for the plot. We can also add a kernel density estimate (smoothed version of a histogram) on top of the histogram by the kde argument.\n\nsns.histplot(x = \"income\", \n             data = default, \n             bins = bins,\n             stat = \"density\",\n             kde = True);\nplt.show();\n\n\n\n\n\n\n\n\n\n\nBoxplot and violin plot\nBoxplot is a useful way of visualizing data.\n\nsns.boxplot(x = \"income\", data = default);\nplt.show();\n\n\n\n\n\n\n\n\nEspecially, if you are going to compare a continuous variable (income) for different groups (students vs nonstudents):\n\nsns.boxplot(x = \"student\", y = \"income\", data = default);\nplt.show();\n\n\n\n\n\n\n\n\nAlternatively, we can achieve the same thing using a violin plot:\n\nsns.violinplot(x = \"student\", y = \"income\", data = default);\nplt.show();\n\n\n\n\n\n\n\n\n\n\nScatterplot\nScatterplots are useful when you want to visualize the dependence structure of two continous variables, say balance and income in the default dataset.\n\nsns.scatterplot(x=\"balance\", y = \"income\", data = default);\nplt.show();\n\n\n\n\n\n\n\n\nIn this case, it does not seem to be a very strong relationship between the two variables.\n\n\nBar/count plot\nBar plots is a very useful way of visualizing counts or frequencies. Most graphical functions also allow you to split the plots by some grouping variable using different colors. This is done using the hue argument. In this case, we want to show the frequency of people that defaulted (default = “Yes”) or not (“No”), coloring the bars by their student status (“Yes” = student, “No” = nonstudent).\n\nsns.countplot(x= \"default\", \n              data =default, \n              stat=\"proportion\", \n              hue = \"student\");\nplt.show();\n\n\n\n\n\n\n\n\n\n\nPersonalize your figures!\nIt is important to make your graphics look good, informative and professional. The basic figure you get by using default options can be quite good, but often you may want to personalize it further. This can mean e.g. changing axis labels, adding a title, changing how the markers look, changing line widths or size of points, etc. There are lots of options you can bend and tweak when making these kinds of figures to make them look pretty and informative.\nLet us use the scatterplot from above as an example. Say I want to change the axis titles:\n\nsns.scatterplot(x=\"balance\", y = \"income\", data = default);\nplt.xlabel(\"Balance (in USD)\")\nplt.ylabel(\"Yearly gross income (USD)\")\nplt.show();\n\n\n\n\n\n\n\n\nI may also want to change how the points of the scatterplot is presented:\n\nsns.scatterplot(x=\"balance\",\n                y = \"income\", \n                data = default,\n                color = \"#B5984B\", # change color\n                marker = \"s\",  # Change marker\n                s=100); # Change size of points\nplt.xlabel(\"Balance (in USD)\");\nplt.ylabel(\"Yearly gross income (USD)\");\nplt.show();\n\n\n\n\n\n\n\n\nor perhaps I would like to color the points by their student status:\n\nsns.scatterplot(x=\"balance\",\n                y = \"income\", \n                data = default,\n                hue = \"student\",\n                marker = \"x\") ;\nplt.xlabel(\"Balance (in USD)\");\nplt.ylabel(\"Yearly gross income (USD)\");\nplt.show();\n\n\n\n\n\n\n\n\nThere are endless options. Check out the website for the seaborn package for more inspiration. It is also always a good idea to have a look the the help files for the functions for seeing more of the arguments you can play with when making graphics. Especially, this seaborn tutorial can be useful to have a look at.",
    "crumbs": [
      "Modules",
      "Module 1: Summarizing and visualizing data",
      "Python"
    ]
  },
  {
    "objectID": "1-intro.html",
    "href": "1-intro.html",
    "title": "TECH3 Applied statistics",
    "section": "",
    "text": "In this module, you will learn about summarizing and visualizing data. All (almost) applied statistical analysis starts with some data, and beeing able to summarizing visualize data is an important tool for any user of statistics.\n\nFocus: Making the data more accessible.\nKey topics: Central statistics summarizing data, histograms, scatterplots, boxplots, human perceptional limitations.\nSpecial Emphasis: What is statistical thinking? Never create a pie chart.",
    "crumbs": [
      "Modules",
      "Module 1: Summarizing and visualizing data",
      "Introduction"
    ]
  },
  {
    "objectID": "1-gapminder.html",
    "href": "1-gapminder.html",
    "title": "Gapminder example of good visualization",
    "section": "",
    "text": "Hans Rosling (deceased) is a legend when it comes data visualization. He was a professor of International Health at Karolinska Insitute and well-known for his visualizations of global data related to international health. The youtube video below is made by BBC in collaboration with Hans Rosling and can (hopefully) be inspirational for your own visualizatios. The data in the video is up to 2010, but for updated data, see gapminder.org.\n\n\n\n\n\nWhat do we call the kind of plots that Rosling uses here?\nHow many variables is visualized at the same time in Rosling’s plot?\nHow many data points did Rosling visualize in the video (according to himself)?\nWhich country had the highest life expectancy and income per person in 2010?\nWhich country had the lowest life expectancy and income per person in 2010?\nCan you see a dip in life expectancy due to Covid-19 in the updated data?",
    "crumbs": [
      "Modules",
      "Module 1: Summarizing and visualizing data",
      "Gapminder example of good visualization"
    ]
  },
  {
    "objectID": "1-gapminder.html#controll-questions",
    "href": "1-gapminder.html#controll-questions",
    "title": "Gapminder example of good visualization",
    "section": "",
    "text": "What do we call the kind of plots that Rosling uses here?\nHow many variables is visualized at the same time in Rosling’s plot?\nHow many data points did Rosling visualize in the video (according to himself)?\nWhich country had the highest life expectancy and income per person in 2010?\nWhich country had the lowest life expectancy and income per person in 2010?\nCan you see a dip in life expectancy due to Covid-19 in the updated data?",
    "crumbs": [
      "Modules",
      "Module 1: Summarizing and visualizing data",
      "Gapminder example of good visualization"
    ]
  },
  {
    "objectID": "1-data-visualization.html",
    "href": "1-data-visualization.html",
    "title": "Data visualization principles",
    "section": "",
    "text": "Slides for “Principles of good visualization”\nSlides for “Accommodating human limitations”\n\n\n\nWhy should you always show the data?\nWhat does it mean to maximize the data/ink ratio?\nYou may think that bar plots in 3D looks cool. Why should we not use that?\nWhat is distorting the data?\nIf you want to compared Gross Domestic Product (GDP) between the US and Norway, how should you adjust the data?\nWhat does it mean to be “kind to the colorblind”?\nWhy should you never use a pie chart? Ever.\nWhat would be a good alternative to a pie chart?",
    "crumbs": [
      "Modules",
      "Module 1: Summarizing and visualizing data",
      "Data visualization principles"
    ]
  },
  {
    "objectID": "1-data-visualization.html#controll-questions",
    "href": "1-data-visualization.html#controll-questions",
    "title": "Data visualization principles",
    "section": "",
    "text": "Why should you always show the data?\nWhat does it mean to maximize the data/ink ratio?\nYou may think that bar plots in 3D looks cool. Why should we not use that?\nWhat is distorting the data?\nIf you want to compared Gross Domestic Product (GDP) between the US and Norway, how should you adjust the data?\nWhat does it mean to be “kind to the colorblind”?\nWhy should you never use a pie chart? Ever.\nWhat would be a good alternative to a pie chart?",
    "crumbs": [
      "Modules",
      "Module 1: Summarizing and visualizing data",
      "Data visualization principles"
    ]
  },
  {
    "objectID": "1-exercises.html",
    "href": "1-exercises.html",
    "title": "Exercises",
    "section": "",
    "text": "Exercises\n\nProblem 1\nYou are given the following data describing final grades in a tricky statistics course. There were 20 A’s, 97 B’s, 163 C’s, 76 D’s, 31 E’s and 13 F’s.\n\nWhat are the most and least common grades?\nOrganize this data in a frequency table\nFind the relative frequency of each grade\nWhat proportion of students failed the course?\nWhat proportion of students got a B or above?\nWhat proportion of students got between a C and an E?\n\n\n\nShow solutions\n\n\nThe most common grade is C with 163 instances, and the least common grade is F with only 13 instances.\nThe table can be organised like this for instance:\n\n\n\n\nGrade\nNumber\n\n\n\n\nA\n20\n\n\nB\n97\n\n\nC\n163\n\n\nD\n76\n\n\nE\n31\n\n\nF\n13\n\n\n\n\nBy summing the amount of respective registered grades we find that there are 400 total. We can now divide the number of instances for each grade by the total of 400 to find the relative frequency. For example, we have 20 instances of A’s out of 400 total grades.\n\n\\[\\frac{20}{400}=0.05\\]\nWe can repeat this for all grades and add to the existing table:\n\n\n\nGrade\nNumber\nFrequency\n\n\n\n\nA\n20\n0.05\n\n\nB\n97\n0.2425\n\n\nC\n163\n0.4075\n\n\nD\n76\n0.19\n\n\nE\n31\n0.0775\n\n\nF\n13\n0.0325\n\n\n\n\nThe only failing grade is F. We can read from the new table that this we have 0.0325 or 3.25% of students that failed this course.\nTo find out what proportion of students got a B or above we need the cumulative frequency. We find this cumulative frequency by summing thge relative frequency of all the relevant cases. In this case “B or above” is the same as achieving a B or an A. The relative frequency of B’s is 0.2425 and for A’s it’s 0.05.\n\n\\[ 0.2425+0.05=0.2925=29.25\\%\\]\n\nTo find out what proportion of students got grades between C and E we also need to find the cumulative frequency. Like in e this requires summing the appropriate relative frequencies. From the table in b we can see that the respective relative frequencies are 0.4075 for C, 0.19 for D and 0.0775 for E.\n\n\\[ 0.4075+0.19+0.0775=0.675=67.5\\%\\]\nAlternatively we can take note of the fact that we already know the proportion of students that were below this grade range (the proportion of F’s), as well as the proportion of students that scored above this range (the proportion that scored B or above). By subtracting the proportion of students that scored outside this range from 1 (100%), we should arrive at the same answer.\n\\[1-0.0325-0.2925=0.675=67.5\\%\\]\n\n\n\nProblem 2\nYou hear that the average starting salary after finishing a master’s degree at NHH is NOK 550 000, the median salary is NOK 525 000, and the mode is NOK 500 000. What can you infer from this?\n\n\nShow solutions\n\nThough they might seem similar on a surface level, the mode, median and mean give slightly different information.\nThe mode tells us what the most common observation in the data is. In our case this means that the most common starting salary after graduating with a master’s degree from NHH is NOK 500 000.\nThe median is the “middle value” in the dataset, meaning that 50 percent of the observations should be below this value, whilst the other 50 percent should be above this value. In our case it means that half of graduates will have a starting salary that’s NOK 525 000 or below whilst the other half get a starting salary of NOK 525 000 or above.\nFinally, the mean reflects the average of the dataset. We can tell here that the average is NOK 550 000. Additionally, by looking at everything in context we can also make some minor inferences. We see that the mean is higher than the median, indicating that some data points are skewing the average upwards, i.e. some graduates are earning really well compared to their peers from the get go. Additionally we see that the mode is 500 000, compared to the mean of 550 000. As the mode is the most common observation and is below the median, this seems to suggest that those who end up with a higher starting salary start with a substantially higher one compared to their peers, for the average to skew above the median.\n\n\n\nProblem 3\nAt a retail chain Floormart the Median pay by hour is $25, but the average is $33 per hour. What does this suggest?\n\n\nShow solutions\n\nSimilar to in Problem 2, the indication here seems to be that some employees may be earning significantly more than others. Exactly how many employees are earning more and by how much is impossible to tell given our available information, but it seems likely that the executives or managers and above may have significantly higher pay than most employees in Floormart. These few employees having a much higher pay than for instance new hires or other low level workers could explain this disparity.\n\n\n\nProblem 4\nIf possible, find the mode, median and mean of the following set of dice rolls\n\\[4,6,5,5,6,2,1,6,3,6,2,1,3\\]\n\n\nShow solutions\n\nThe mode is the most common observation. As 6 appears 4 times here, more than any other roll, 6 is the mode.\nTo clearly find the median we can sort the data from lowest to highest observation. We then end up with {1,1,2,2,3,3,4,5,5,6,6,6,6}. We have 13 observations here, so the median will be given by the 7th observation from the bottom. In our case we find that 4 is the median.\nFinally we can find the mean by taking the average of the observed rolls.\nRecall the formula for averages is\n\\[\\overline{X}=\\frac{1}{n}\\sum_{i=1}^{n}X_i\\]\nIn our case we can just sum each observation and divide by the number of observations, which is 13.\n\\[\\begin{align*}\n\\overline{X}&=\\frac{1}{13}(1+1+2+2+3+3+4+5+5+6+6+6+6)\\\\\n&=\\frac{50}{13}\\approx3.846\n\\end{align*}\\]\n\n\n\nProblem 5\nIf possible, find the mode, median and mean of the following set of favorite colors\nred,blue,blue,green,yellow,green,green,blue,red,red,pink,purple,blue\n\n\nShow solutions\n\nThe mode is found by finding the most common observation. Here “blue” is the most common favorite color, with 4 instances, and thus “blue” is our mode.\nIt is not possible to find a median within this data, as we need to be able to logically sort the data to be able to have a meaningful median. I.e. as the data is not ordinal, we can’t find a median.\nIt is not possible to find a mean from this data. We need numerical data to be able to give a meaningful mean.\n\n\n\nProblem 6\nWhat level of data is required to find modes, means, and medians? (Categorical, ordinal, etc.)\n\n\nShow solutions\n\nTo find a mode we need at least categorical data.\nTo find a median we need data that is ordinal.\nTo find a mean we need data that is numerical.\n\n\n\nProblem 7\nYou get the dataset \\[1,1,1,1,1,3,3,3,3,7,7,7,7,9,9,9.\\]\n\nFind the mode, median and mean of the above set.\nFind the the first and third quartile of the set.\nWhat are the 0th and 100th percentiles of the set?\nWould the median change if you were to add a single datapoint of 11 to the set? What would the median of the new set be? What if we were to add two datapoints of 11?\n\n\n\nShow solutions\n\n\nThe most common observation is 1, thus the mode is 1. We have an even number of observations in this set, 16.\n\nBy taking the average of the two middlemost values we will find the median. Both the 8th and 9th value in the set are 3, and thus the median of the set is 3.\nWe can find the mean by taking the average of the data.\n\\[\\begin{align*}\n\\overline{X}&=\\frac{1}{16}(1+1+1+1+1+3+3+3+3+7+7+7+7+9+9+9)\\\\\n&=\\frac{72}{16}=4.5\n\\end{align*}\\]\n\nThe first quartile indicates that 25% of observations should be below this threshold. In our case we see that the first quartile would fall between the 4th and 5th observation, which are both 1, telling us the 1st qaurtile is 1.\n\nThe 3rd quartile indicates that 75% of observations should be below it. The 12th and 13th observations are both 7, letting us know that the third quartile is 7.\n\nThe 0th percentile indicates the minimum value of the set while the 100th percentile indicates the maximum value. We can then say that the 0th percentile is 1 and the 100th percentile is 9.\nIf we add a datapoint of 11 to the set, we would now have 17 observations, meaning that the 9th observation from the bottom would indicate the median. The 9th observation from the bottom is 3, which is the same median as in the previous case, so the median would not change. If we were to add two datapoints of 11 to the set, we would have 18 observations. With 18 observations we have to take the average of the two middlemost observations after sorting. The two middlemost observations are the 9th and 10th ones from the bottom, meaning 3 and 7. The average would then be:\n\n\\[\\frac{1}{2}(3+7)=\\frac{10}{2}=5\\]\nThe median in this case would be 5, so it would indeed change from the previous case.\n\n\n\nProblem 8\nYou get this dataset of a group of 11 people and their respective heights in centimetres.\n\\[184,157,168,172,198,154,192,161,186,177,165.\\]\n\nFind the mean/average height of this group.\nFind the variance in height in this group and the standard deviation.\nDoes the variance and standard deviation seem big, small or somewhere in between in this context?\nWhat would be the effect on the variance of adding another person to the sample that is of the average height we found in a. Would the variance increase, decrease or stay the same?\nWhat would be the effect of adding another person to the group that’s 200cm tall. Would the variance increase, decrease or stay the same? Why is the change intuitive?\nWhat would be the effect of adding another person to the group that’s 140cm tall. Would the variance increase, decrease or stay the same? Why is the change intuitive? Why is the change in variance here greater than the one in e.?\n\n\n\nShow solutions\n\na)We sum our observations, and divide by 11, which is the number of observations.\n\n\\[\\begin{align*}\n\\overline{X}=\\frac{1}{11}(&184+157+168+172+198+154+192+161 \\\\ &+186+177+165)=\\frac{1914}{11}=174\n\\end{align*}\\]\n\nHere we need to first recall the formula for variance.\n\n\\[S_X^2=\\frac{1}{n-1}\\sum_{i=1}^{n}(X_i-\\overline{X})^2\\]\nWe recall that the mean we found in a is 174 and that we have 11 total observations. Now we have everything we need to plug in our data and find the variance.\n\\[\\begin{align*}\nS_X^2&=\\frac{1}{11-1}\\bigg((184-174)^2+(157-174)^2+(168-174)^2 \\\\ &+(172-174)^2+(198-174)^2+(154-174)^2+(192-174)^2+(161-174)^2+ \\\\&(186-174)^2+(177-174)^2+(165-174)^2\\bigg)=\\frac{2132}{10}=213.2\n\\end{align*}\\]\nNow we only need to find standard deviation.Recall that the standard deviation is the square root of the variance.\n\\[S_X=\\sqrt{S_X^2}=\\sqrt{\\frac{1}{n-1}\\sum_{i=1}^{n}(X_i-\\overline{X})^2}\\]\nIn our case we need only plug in the variance we have already found.\n\\[S_X=\\sqrt{213.2}\\approx14.6\\]\n\nVariance is hard to interpret directly, but the standard deviation is very simple. The standard deviation tells us that the average absolute difference from the mean in our data. Here we see that the standard deviation is 14.6cm, so on average the people in this group are 14.6 away from the the mean in terms of height. This means the we would expect a random person from this group to be either about 159cm tall or 188cm tall. It’s not all that simple to say if this is a lot or not, that depends on the data collected. If this is a sample from a group of Norwegian men age 20, this could seem like a very big standard deviation and variance. However, if this is a group of men and women with an age range from 14 to 35, this might not seem like such a big variance.\nIf we were to only add more people of the average height we found (174cm) the variance would decrease. There are two effects in play here. The when we add more observations our n gets bigger, and thus we divide by a larger number, but we also have to add another squared difference from the mean. In this case we add another observation that’s exactly equal to the mean. Firstly what effect will this have on the mean itself? Intuitively, the mean will not change in this case. Notice that the squared difference between the mean and itself is 0. \\[(\\overline{X}-\\overline{X})^2=(174-174)^2=0^2=0.\\]\n\nLet’s now add this to our equation for variance. Since we have one more observation we will be dividing by (n-1)+1=n, instead of n-1.\n\\[\\begin{align*}\nS_{X1}^2&=\\frac{1}{n} \\left(\\sum_{i=1}^n(X_i-\\overline{X})^2+(\\overline{X}-\\overline{X})^2\\right)\n    =\\frac{1}{n} \\left(\\sum_{i=1}^n(X_i-\\overline{X})^2+0\\right)\n    \\\\&=\\frac{1}{n} \\left(\\sum_{i=1}^n(X_i-\\overline{X})^2\\right)\n    &lt;\\frac{1}{n-1} \\left(\\sum_{i=1}^n(X_i-\\overline{X})^2\\right)=S_X^2\n\\end{align*}\\]\nFor those that don’t find it intuitive that the mean stays the same when you add more observations the equal the mean:\nUnderneath I have shown algebraically what happens to the mean when you add one more observation in the sample that equals the mean. This can easily be extended to adding any number, m, more observations (try for yourself!).\n\\[\\begin{align*}\n\\overline{X}_1&=\\frac{1}{n+1} \\left( \\sum_{i=1}^{n}X_i+\\overline{X} \\right)=\\frac{1}{n+1}\\left( \\frac{n}{n}\\sum_{i=1}^nX_i+\\overline{X} \\right) \\\\ &=\\frac{1}{n+1}\\left(n\\overline{X}+\\overline{X}\\right) = \\frac{n+1}{n+1}\\overline{X}=\\overline{X}\n\\end{align*}\\]\nNow, let’s see what happens to the variance, when we add another observation that equals the mean.\n\\[S_{X1}^2= \\frac{1}{n-1+1}\\left(\\sum_{i=1}^n(X_i-\\overline{X})^2+(\\overline{X}-\\overline{X})^2\\right)\\]\n\nWe can amend our data by adding the new observation of 200cm, and with this we can compute the new mean and variance. We will notice that since we add an observation that is higher than the average, the mean will increase, and we will also find that the variance (and consequently the standard deviation) will both increase. The new values will be:\n\n\\[ \\overline{X}\\approx176.17 \\\\ S_X^2\\approx250.15\\]\nThe intuition is simply that since our new observation is very different from our mean, the variance will increase. Adding the new squared difference has a greater effect than dividing by a higher number. Recall that our original mean was 174 and our original standard deviation was approximately 14.6. This means that the average absolute difference between an observation and the mean is about 14.6.\n\\[|200-174|=26&gt;14.6\\]\nThis means that by adding this new observation, since it’s difference from the mean is so clearly much greater than the standard deviation, the standard deviation should increase by adding this observation. When the standard deviation increases, so will the variance. (Note that we can only intuit in this way when the absolute difference between the new observation and the mean is quite a bit greater than the standard deviation. The intuition you should build from this is that new observations that are very different from the mean will increase the variance, and consequently, observations that are close to the mean will decrease the variance.)\n\nThis case is very similar to the one in e. The only notable difference is that we add an observation that is much lower than the mean, rather than one which is much higher. Computing the the new mean and variance we get:\n\n\\[\\overline{X}\\approx171.17 \\\\ S_X^2\\approx290.15 \\]\nThe intuition here is exactly the same as in e. Since the new observation is very different from the mean, the variance should increase. Let’s compare the absolute difference between the mean and the new observation to the standard deviation.\n\\[|140-174|=|-34|=34&gt;14.6\\]\nFrom this it should also be clear why the change in variance here is greater than the one in e. Since 140 is an observation that is farther away from the mean (174), than 200, the increase in variance will be greater.\n(Extra challenge for those that can’t get enough: Can you intuit what would happen in an opposite case? Would the variance change more if we added a new observation of someone that was 177cm tall or with someone that was 172cm tall?)\n\n\n\nProblem 9.\nRecall the variance and standard deviation found in Problem 8b. Imagine that instead of the height of people, the variance and standard deviation was instead found from data on height of trees (Note: the collected data is not necessarily the same even though the variance is the same). Does the variance and standard deviation feel big, small or somewhere in between in this context?\n\n\nShow solutions\n\nThis problem is mainly about interpretation, but let’s consider the standard deviation. We have a standard deviation of 14.6cm. A height difference of 14.6cm between two people is quite large, and very noticeable. When it comes to trees, however, that is not necessarily the case. Short trees can be as short as just a few centimeters, where as the tallest trees can reach upwards of 100 meters(!). Even without considering the shortest and tallest trees, we find that there can easily be several meters of difference between completely ordinary trees. Since we only know that we are working with trees in this case, it’s hard to assume anything about the data, but looking at the standard deviation we can tell that these trees must be very similar in stature as far as trees are considered. If anything, this variance, that seems quite reasonable (or even very large), when we consider the heights of people, seems completely minuscule when we consider trees instead. It’s impossible to say whether variance and standard deviation is large or small with no context, as is illustrated with this comparison.\n\n\n\nProblem 10\nIn what scenario would we get a Variance of 0?\n\n\nShow solutions\n\nFor the variance to be 0, our observations simply can’t vary. This means that all observations have to have the same value.\nThis can be shown algebraically. Let all observations \\(X_i=X\\) be equal. Then \\[\\overline{X}=\\frac{1}{n}\\sum_{i=1}^nX_i=\\frac{1}{n}\\sum_{i=1}^nX=\\frac{n}{n}X=X \\]\nIt is clear that all observations here are equal to the mean, which gives us.\n\\[\\begin{align*}\nS_X^2&=\\frac{1}{n-1}\\sum_{i=1}^n(X_i-\\overline{X})^2=\\frac{1}{n-1}\\sum_{i=1}^n(\\overline{X}-\\overline{X})^2 \\\\ &=\\frac{1}{n-1}\\sum_{i=1}^n0=\\frac{0}{n-1}=0\n\\end{align*}\\]\nBy looking at the equation for the variance, we can tell that if any one or more observations were to deviate from the mean, there would be variance in our data.\n\n\n\nProblem 11\nYou have a dataset with six observations and a mean of 50. Show that when you have any observations outside of the interval [45;55] the variance will always be greater than 5.\n\n\nShow solutions\n\nWe have received this information:\n\\[n=6, \\ \\overline{X}=50\\]\nTo show that that our variance will always be greater than 5 when we have an observation outside of the given interval, let’s consider the situation that would create the least variance. Consider a situation where only one observation is 55, and the rest of the observations have to make up for this by all of them being a bit less than 50. Since we use squared sums when we compute the variance, we would get the least variance from the 5 last observations being equally close to to the mean. We can use an equation to find out what each of these 5 observations have to equal. For this we use the formula to calculate an average.\n\\[\\overline{X}=\\frac{1}{n}\\sum_{i=1}^nX_i\\]\nSince we know that the average we’re looking for is 50, one observation is 55 and the last 5 observations are equal, we get an equation with one variable.\n\\[\\begin{align*}\n50&=\\frac{1}{6}(5X+55) \\\\\n\\Rightarrow 6\\cdot 50&=5X+55 \\\\\n\\Rightarrow 300&=5X+55 \\\\\n\\Rightarrow 300-55&=5X \\\\\n\\Rightarrow 245&=5X \\\\\n\\Rightarrow X&=49\n\\end{align*}\\]\nNow we have a clear view of the data that would give the minimum variance. {49,49,49,49,49,55} Finally, we can compute the variance.\n\\[\\begin{align*}\nS_X^2&=\\frac{1}{6-1}\\left(5(49-50)^2+(55-50)^2 \\right) \\\\\n&=\\frac{1}{5}(5\\cdot(-1)^2+5^2)=\\frac{1}{5}(5+25) \\\\\n&=\\frac{30}{5}=6&gt;5\n\\end{align*}\\]\n\n\n\nProblem 12\nConsider the two following data sets, A and B. \\[A=\\{52, 49, 53,57,48,48,57,53\\}\\] and \\[B=\\{29,56,31,21, 79, 45, 51, 21\\}\\]\n\nFind the interquartile ranges of both A and B. Based on the ranges, which one seems to indicate a higher standard deviation?\nFind the variance and standard deviation of both data sets. Do the results fit with your guess from a).\nWhat intuition can interquartile ranges give when comparing variances? Can you always trust this intuition fully?\n\n\n\nShow solutions\n\n\nTo find the interquartile ranges we have to first find the quartiles in each data set, and to find the quartiles we need to order the sets as well. We now end up with\n\n\\[\\begin{align*}\nA=\\{48, 48, 49, 52, 53, 53, 57, 57\\} \\\\\nB=\\{21, 21, 29, 31, 45, 51, 56, 79\\}\n\\end{align*}\\]\nNow that both sets are ordered we can find the frst and third quartile in each of them by considering the 25th and 75th percentiles. In both sets the 25th percentile will fall between the second and third observations, whereas the 75th will fall between the 6th and 7th. Thus we take the average between these observations to fin the respective quatiles.\n\\[\\begin{align*}\nq_{1A}=\\frac{1}{2}(48+49)=48.5, \\ q_{3A}=\\frac{1}{2}(53+57)=55 \\\\\nq_{1B}=\\frac{1}{2}(21+29)=25, \\ q_{3B}=\\frac{1}{2}(51+56)=53.5\n\\end{align*}\\]\nFinally, we can find the interquartile ranges and compare them. Recall the formula for an interquartile range is given by the difference in quartiles.\n\\(IQR=q_3-q_1\\) Now, let’s compute the IQR for both A and B. \\[\\begin{align*}\nIQR_A=55-48.5=6.5 \\\\\nIQR_B=53.5-25=28.5\n\\end{align*}\\] Here we can see that the interquartile range for B is much larger than for A, which indicates a larger spread of data in B, which should give us a higher standard deviation.\n\nLet us compute the variances and standard deviations. By using the formulas familiar to us we will end up with.\n\n\\[\\begin{align*}\n\\overline{X}_A=52.125, \\ S^2_A\\approx13.27, \\ S_A\\approx3.64 \\\\\n\\overline{X}_B=41.625, \\ S^2_B\\approx403.70, \\ S_B\\approx20.09\n\\end{align*}\\] We can easily see that the indication from the IQR told us which data set had more variance.\n\nThe reason interquartile ranges can help give an indication of what data has more variance, is cause the interquartile range also gives an indication of how spread out the data is. The interquartile range tells us how far the 75th percentile is from the 25th percentile, i.e. a range for the middle 50% of the data. Keep in mind, however that a higher IQR does not necessarily mean a data set has a higher variance, even though it might seem that way at a first glance. When computing variance and standard deviation extreme values are very highly weighted even though they may not have any effect on the IQR.\n\nLet’s finish up by giving a quick example. We can amend A such that we change one of the data points that reads 57 to 1000 instead. We will call the amended set A’. We then get: \\[\\begin{align*}\nA=\\{48, 48, 49, 52, 53, 53, 57, 57\\}\\rightarrow A'=\\{48, 48, 49, 52, 53, 53, 57, 1000\\}\n\\end{align*}\\] Note here that the first and third quartile have not changed, and thus the IQR stays the same. However, when computing mean, variance and standard deviation we are going to see huge shifts.\n\\[\\begin{align*}\n\\overline{X}_{A'}=170, \\ S^2_{A'}=112482.86, \\ S_{A'}\\approx335.38\n\\end{align*}\\]\nThe 1000 observation is clearly an outlier here, but the point is that we always have to be careful when saying the IQR implies the variance, or anything similar. In many cases it may be true, but we should always consider the data we’re working with.\n\n\n\nProblem 13\nConsider the Boxplot below.\n\nWhich of the two groups has the higher median?\nWhich of the two groups seems to have the higher variance?\nWhich of the two groups has the higher IQR? Does this coincide with the one that has the higher variance?\nWould you think there are any outliers in the data? Why? Why not?\n\n\n\n\n\n\n\n\n\n\n\n\nShow solutions\n\n\nThe bold, black, horizontal line in each box represents the median for the data. We note that for group 1 the median is a little less than 15, and for group 2 it seems to be about 17. As such, group 2 has the higher median.\nWe note that the whiskers for group 2’s plot stretch out far wider than the ones for group 1. There are no points beyond the whiskers for either group, and as such we can conclude that group 2 probably has the higher variance. This coincides with the code used to generate the data.\nWe note that the box for group 2 is far taller than the one for group 1, which clearly shows that group 2 has a higher IQR. The lower line of the box indicates the the 1st quartile and the upper line gives the 3rd quartile. As such, the height of a box represents the IQR for the data used to generate the boxplot. This indeed coincides with the data we believe to have the higher variance.\nWe see no points beyond the whiskers of the boxes, and as such we treat none of the data as outliers. The whiskers here represent 1.5 times the IQR of the data, beyond either the 1st or 3rd quartiles. in many cases this covers all the whole range of data in a sample, however that is not always the case. In the below boxplot I have used the same data, however I added as single, clear outlier to group 1. This outlier is now the new maximum value and is indicated as a red dot. In general, when we consider outliers using boxplots, it’s most common to start by considering everything beyond the whiskers of the plot.\n\n\n\n\n\n\n\n\n\n\n\n\n\nProblem 14\nConsider the two viloinplots below.\n\nIn what ranges in each of the groups is data most dense?\nWhich group has the highest maximum, which one has the lowest minimum?\nIs it clear which group would have a higher median? Do you think this group also has a higher mean? Argue why or why not?\nWhat are some key differences to consider between a violin plot and a boxplot?\n\n\n\n\n\n\n\n\n\n\n\n\nShow solutions\n\n\nFrom a visually inspections it’s not 100% clear what the exact ranges are, but it does seem like for group 1 data is most dense in the intervals (12, 13), whereas for group 2 it’s around (13, 15). We see this as this is where the “violins” are the widest.\nA key aspect of violin plots is that they chow the distribution of the data in its entirety. This means that the top and bottom ends of the violins indicate the range of the data. Here it??s clear rthat group n2 has the higher maximum and group 1 has the lower minimum.\nThe median is the middle observation. Though we can’t see the exact point on a violin plot, we can see that the group 1 is comparatively “bottom-heavy” next to group 2, which indicates that the median should be higher in 2. We can also infer that the higher density of high numbers in group 2 indicate a higher mean as well. We can’t be 100% certain though, as we have no direct way of telling. (In an obvious case such as this though we can be at least 99% certain).\nThe key distinctions between violin and box plots are what attributes of the data they show. Whereas box plots show summary statistics such as the median, 1st and 3rd quartile and IQR, the violin plot shows how the data is distributed (i.e. the density of the data). As you should have noticed from the exdercises above, we can’t make many direct statements based on the violin plot, other than to draw semi-certain conclusions based on where we see the data is more and less dense. The boc plot also isolates ourtliers in a more clear way compared to the violin. Take a look at the below figure to see boc plots in the same data as the violins. Were the inferences from the above exercises correct?\n\n\n\n\n\n\n\n\n\n\n\n\n\nProblem 15\nYou receive data about voting intention in a Norwegian community. Each bar represents the percentage of people in this community that wants to vote for a given party. Only people who intend to vote have been counted.\n\nWhich parties are most and least popular in this community?\nWhat is the approximate percentage point difference between the most and least popular parties?\nWhat problems could have been run into by using a pie chart instead of a bar plot?\n\n\n\n\n\n\n\n\n\n\nExercises about interpreting plots\n\n\nShow solutions\n\n\nIt clear that AP (Labour party) is the most popular and R (Red party) is the least popular in this community.\nAP has a voter turnout of about 20 percent wheres R has a turnout of about 5 percent. We thus get a difference of about 15 percentage points between the most and least popular parties.\nFor one, the pie chart would not clearly indicate the relative popularities of each party. It could for instance be very hard to tell which party was more popular between MDG and R, as they are very close in size here. Secondly, this pie chart would be very distracting top look at and hard to read, as you would not be able to read the information directly from the chart. Underneath there is an example. You could for instance add percentage labels to the chart below, but that would be visually much busier tha nthe bar plot as well as would work poorly when slices get very small.\n\n\n\n\n\n\n\n\n\n\n\n\n\nProblem 16\nConsider the histogram below. The y-axis can be interpreted as percentages. The histogram shows the distributions of yearly salaries of a population in an area.\n\nWhat percentage of people in this population have salaries in the most common interval?\nWhat are key differences between histograms and bar plots.\n\n\n\n\n\n\n\n\n\n\n\n\nShow solutions\n\n\nThe tallest bin indicates the most common interval for salaries and it seems to include about 15 percent of the people in the population. The tallest bin seems to be somewhere between 600k and 700k NOK.\nIn a histogram each bin represents an interval of values, and any datapoint which falls within that interval will contribute to the height of that bin. A histogram will also necessarily represent the distribution of numerical sample data. A bar plot, however, represents the relative frequency of categorical data.\n\n\n\n\nProblem 17\nComment on the below scatterplot, focus on whether or not you believe the X and Y variables are dependent or not.\n\n\n\n\n\n\n\n\n\n\n\nShow solutions\n\nIn this scatterplot there are no clear signs of correlation between X and Y. The closest thing one might get is that there seem to be fewer X and Y observations of more extreme values, however, that still doesn’t imply dependence. Each point seems randomly scattered out and there is no pattern to see here.\n\n\n\nProblem 18\nComment on the below scatterplot, focus on whether or not you believe the X and Y variables are dependent or not.\n\n\n\n\n\n\n\n\n\n\n\nShow solutions\n\nHere there is a clear linear dependence between the X and Y variables. There correlation seems to be almost a perfect positive correlation of 1. This can very easily be illustrated by drawing a best fit line through the point. The image below illustrates the strong linear relationship.\n\n\n\n\n\n\n\n\n\n\n\n\nProblem 19\nWhich of the following histograms are closer to a normal (Gaussian) distribution?\n\n\n\n\n\n\n\n\n\n\n\nShow solutions\n\nObviously 1 is far closer to a normal distribution than 2 is. We can clearly see that 1 is very close to the characteristic bell shape and a very symmetric distribution of points around the mean. Histogram 2 on the other hand is not at all symmetric around its mean and does not have the iconic shape, so it can easily be identified as non-normally distributed.\n\n\n\nProblem 20\nDetermine whether the below histograms display distributions that are right skewed, left skewed or neither.\n\n\n\n\n\n\n\n\n\n\n\nShow solutions\n\nHistogram number 1 is clearly extremely right skewed, which we can tell from the comparative length of the right tail and that the mass is concentrated on the left side of the plot. Number 2 is also very right skewed, but not quite to the extreme of 1.\nHistogram number 3 on the other hand is very left skewed, which should be obvious as its almost the complete opposite of the number 1.\nHistogram number 4 doesn’t seem to have any particular skewness, in actuality it seems remarkably symmetric.\n\n\n\nProblem 21\nConsider the 4 following histograms. Rank them from 1 to 4, where 1 indicates the “lightest tails” and 4 indicates “the heaviest tails”\n\n\n\n\n\n\n\n\n\n\n\nShow solutions\n\nThe correct ranking is 1-D, 2-A, 3-B, 4-C\nRecall that heavier tails implies that more mass will be farther away from the mean. Once one realises this fact it should be simple to order the histograms based on weights of tails. No points reach very far away from the mean of D, a few more for A, as one can tell by the longer interval on the x axis, even more are far away from the mean in B, and a very large amount in C.\n\n\n\nProblem 22\nRank these histograms created from normally (gaussian) distributed data in 3 different ways, from 1-6. In the first ranking, 1 should be the lowest mode and 6 should be the highest mode. In the second ranking do the same with the median and in the third with the mean. As there are a lot of data points in each of these histograms, you can treat them as perfect normal distributions.\n\n\n\n\n\n\n\n\n\n\n\nShow solutions\n\nThe twist here is that since the data is all normally distributed the mode, mean and median is equal. I.e., if you’ve found one ranking you’ve found them all.\nThe correct ranking is 1-D, 2-F, 3-C, 4-B, 5-E, 6-A\nThe mode should be obvious in each one at it is indicated by the tallest bin. Furthermore, since all the histograms are so symmetric both the medians and mean should also be well within the middle bins as well. As such we end up with the same ranking in all three cases.\n\n\n\nProblem 23\nRank the following three histograms from lowest to highest variance. Consider the data in each case to eb normally distributed.\n\n\n\n\n\n\n\n\n\n\n\nShow solutions\n\nThe correct ranking is\n1-B, 2-A, 3-C\nRecall that variance increases as spread of data increases. It’s clear that the data in B all fits within a pretty narrow interval, whereas the data in C needs a much larger interval to fit all of its observatrions. It’s not even entirely clear that we see all observations of C in the given interval. A seems relatively average, it at least does not stand out in any direction like B and C do.\n\n\n\nProblem 24\nFor this and the next problem you will need to recall how you compute means an variances.\n\\[ \\overline{X}=\\frac{1}{n}\\sum^{n}_{i=1}X_i, \\\nS^2_X=\\frac{1}{n-1}\\sum^n_{i=1}\\left(X_i-\\overline{X}\\right)^2\\]\nConsider the data \\[A=\\{a_1, a_2, ..., a_n\\}\\]. Now add 5 to every observation such that you get \\[A'=\\{(a_1+5),...,(a_n+5)\\}\\]. What would happen to the mean and the variance?\n\n\nShow solutions\n\nWe now have the situation\n\\[ A=\\{a_1,...,a_n\\}\\rightarrow A'=\\{(a_1+5),...,(a_n+5)\\} \\] Let’s the old and new means and variances have the following names.\n\\[\\begin{align*}\n\\overline{X}_{A} \\rightarrow \\overline{X}_{A'} \\\\\nS^2_{A} \\rightarrow S^2_{A'} \\end{align*}\\]\nWe will first consider what happens to the mean. Let’s try to compute the mean for \\(A'\\) \\[\\begin{align*}\n\\overline{X}_{A'}=\\frac{1}{n}\\sum^n_{i=1}(a_i+5)=\\frac{1}{n}\\sum^n_{i=1}5+\\sum^n_{i=1}a_i \\\\\n=\\frac{1}{n}5n+\\sum^n_{i=1}a_i=5+\\overline{X}_{A}=\\overline{X}_{A}+5 \\end{align*}\\] From this argument it should be clear that the 5 greater for \\(A'\\) than for \\(A\\), just like the case is for each of our observations.\nWe can now move on to considering what will happen to the variance. \\[\\begin{align*}\nS_{A'}=\\frac{1}{n-1}\\sum^n_{i=1}\\left((a_i+5)-\\overline{X}_{A'}\\right)^2\n=\\frac{1}{n-1}\\sum^n_{i=1}\\left((a_i+5)-(\\overline{X}_{A}+5)\\right)^2 \\\\\n=\\frac{1}{n-1}\\sum^n_{i=1}\\left(a_i+5-\\overline{X}_{A}-5\\right)^2 =\\frac{1}{n-1}\\sum^n_{i=1}\\left(a_i+5-\\overline{X}_{A}-5\\right)^2 \\\\\n=\\frac{1}{n-1}\\sum^n_{i=1}\\left(a_i-\\overline{X}_{A}\\right)^2\n=S_{A}^2\n\\end{align*}\\] Alas, the variance has not changed at all! The reasoning here is actually quite simple. All our observations may have increased by 5, but so has the mean. This means that the absolute difference between each individual point and the mean is the same for both data sets. So even though the data is different, it doesn’t vary any more or less in one case or the other.\nExtra challenge: There is only one key difference that would come to be if you were to subtract 5 to every datapoint instead of adding 5. What is that difference?\n\n\n\nProblem 25\nConsider the data \\(B=\\{b_1, b_2, ..., b_n\\}\\). Now double every observation such that you get \\(B'=\\{2b_1,...,2b_n\\}\\). What would happen to the mean and the variance?\n\n\nShow solutions\n\nWe now have the situation\n\\[ B=\\{b_1,...,b_n\\}\\rightarrow B'=\\{2b_1,...,2b_n\\} \\] Let’s the old and new means and variances have the following names.\n\\[\\begin{align*}\n\\overline{X}_{B} \\rightarrow \\overline{X}_{B'} \\\\\nS^2_{B} \\rightarrow S^2_{B'} \\end{align*}\\]\nWe will first consider what happens to the mean. Let’s try to compute the mean for \\(B'\\) \\[\\begin{align*}\n\\overline{X}_{B'}=\\frac{1}{n}\\sum^n_{i=1}2b_i=\n2\\frac{1}{n}\\sum^n_{i=1}b_i=2\\overline{X}_B \\end{align*}\\] It’s very clear that if every datapoint was to double, so would the mean.\nNow we compute the new variance. \\[\\begin{align*}\nS_{B'}=\\frac{1}{n-1}\\sum^n_{i=1}\\left(2b_i-\\overline{X}_{B'}\\right)^2\n=\\frac{1}{n-1}\\sum^n_{i=1}\\left(2b_i-2\\overline{X}_{B}\\right)^2 \\\\\n=\\frac{1}{n-1}\\sum^n_{i=1}\\left(2(b_i-\\overline{X}_B)\\right)^2 =\\frac{1}{n-1}\\sum^n_{i=1}2^2\\left(b_i-\\overline{X}_B\\right)^2 \\\\\n=\\frac{1}{n-1}\\sum^n_{i=1}4\\left(b_i-\\overline{X}_B\\right)^2\n=4\\frac{1}{n-1}\\sum^n_{i=1}\\left(b_i-\\overline{X}_B\\right)^2\n=4S_{B}^2\n\\end{align*}\\] So, by multiplying each datapoint by 2, we end up multiplying the variance by 4 (2 squared!!). This also makes surprisingly intuitive sense. When we double each datapoint, we are also going to double the distance between that point and the mean. When computing for the variance we find the squared difference between the data points and the mean, and thus, we end up multiplying our variance with 2 squared.\nExtra challenge: What would happen to the variance if we were to halve the value of each datapoint instead of doubling them?\n\n\n\nProblem 26\nConsider the data \\[C=\\{c_1, c_2, ..., c_n\\}\\]. Now consider what would happen if you were to amend the data by adding more and more of a single observation, \\(d\\) until you’ve added an endless amount of it. What would happen to the mode, median, 1st quartile, 3rd quartile, IQR, max, min, mean, variance and standard deviation? (Note: this exercise should be solved by reasoning rather than with rigorous mathematical arguments!!)\n\n\nShow solutions\n\nTo make every point clear, let’s go summary statistic by summary statistic.\nMode: The consequence on the mode is probably the most obvious one. Recall that the mode is the most common observation, i.e. the observation that occurs most often in the data. If the observation we are adding, \\(d\\), is not already the mode, then by adding an endless amount of it, and only it, at some point it will become the mode. It is inevitable that when we have limited data, adding an endless amount of a singular observartion will eventually result in it becoming the mode. Consider a quick example \\[C=\\{1,2,1,1,1,3\\}, \\ d=5\\] If we were to add \\(d=5\\) once or twice, the mode would still be 1. However, if we keep adding \\(d=5\\), then eventually 5 will become the mode. Once \\(d\\) has become the mode, the mode will not change if we keep adding to the data.\nMedian: The consequence on the median is similar to the one on the mode, in that eventually, the median will end up equaling \\(d\\). Eventually, by adding \\(d\\) more and more to the data, more than half of the observartions will be \\(d\\), and as such, no matter what \\(d\\) equals, it will be the median. Let’s consider the an example. \\[C=\\{1,1,1,1,2,3\\}\\] If we consider \\(d=5\\), as above, we see what by adding it once, it won’t be the median. However if we were to add it 10 times for instance such that we get, more than half of the data would consist of \\(d\\), which means that the middlemost values of the data would also have to be \\(d\\). (The exact same case would play out if \\(d\\) was a much higher maximum or the minimum value of the data). The cases where it would take the most for \\(d\\) to become the median are the ones where it’s either the maximum or the minimum, in all other cases it will take less for it to become the median.\n1st and 3rd quartile: The argument for the 1st and 3rd quartiles is exactly the same as for the median. The more iterations you add, the more of the data will be \\(d\\). After a while close to all data in would equal \\(d\\), and at that point it would also cover both the 1st and 3rd quartile.\nIQR: Recall that IQR is the difference between quartiles. From the argument above we know that at some point both the 1st and 3rd quartiles will be equal to \\(d\\), and at that point the IQR will obviously be 0. Depending on the data the IQR could increase for a bit at first, but after either the 1st or 3rd quartile becomes \\(d\\), it will start approaching 0.\nMin and max: The min and max will change only if \\(d\\) is either the minimum or maximum value in the set, and only if \\(d\\) is not part of the original data. However, if there are values that are larger and smaller than \\(d\\) in the original data, they will not change, no matter how many iterations we add. Consider for example: \\(C=\\{1,2,1,1,1,3\\}, \\ d=2\\) We would then end up with $ C’={1,1,1,1,2,2,2,2,…,2,2,2,3}$ No matte how many times 2 is added, the max and min will not change.\nMean: In problem 8 we considered what would happen if we were to add a new observation with the same value as the mean, and we concluded there would be no change. So if \\(\\overline{X}_{C}=d\\), the mean would not change no matter how many times we were to iterate on the data by adding another observation, \\(d\\). Now consider what would happen if \\(d\\) is different from the. Let \\(m\\) be the number of observations in the original data, and \\(n\\) be the number we get when we add more observations. Computing the mean would then give us. \\[\\begin{align*}\n\\overline{X}_{C'}=\\frac{1}{n}\\left((n-m)d+\\sum^m_{i=1}c_i\\right)\n=\\frac{1}{n}\\left((n-m)d+m\\overline{X}_C\\right)\\\\\n=\\frac{n-m}{n}d+\\frac{m}{n}\\overline{X}_C\n\\end{align*}\\]\nNow we let \\(n\\) approach infinity.\n\\[\\begin{align*}\n\\lim_{n\\rightarrow\\infty}\\overline{X}_{C'}\n=\\lim_{n\\rightarrow\\infty}\\left(\\frac{n-m}{n}d+\\frac{m}{n}\\overline{X}_C\\right)\\\\\n=\\lim_{n\\rightarrow\\infty}\\frac{n-m}{n}d+\\lim_{n\\rightarrow\\infty}\\frac{m}{n}\\overline{X}_C\\\\\n=1*d+0*\\overline{X}_{C}=d\n\\end{align*}\\]\nThis essentially means, that after a while the mean will end up very close to \\(d\\), and it will keep approaching until it reaches \\(d\\). The nature of a limit like this means that it will end up infinitesimally close as long as we iterate a finite amount of data points, but the limit still holds true. Summed up, we can think that when a single observation makes up a vast vast majority of the data, other single observations will have basically no effect on the mean.\nVariance and standard deviation: In problem 8 we also concluded that the variance would decrease when adding more observations close or equal to the mean. Since we know the limit of the mean is \\(d\\) from above let’s argue based on that. When almost all of the data is centered around one point, and every other observation is negligible in comparison, they will contribute almost nothing until the variance reaches 0. Directly following from this, we get that the standard deviation will also approach 0, as it is the positive square root of the variance.\n(Note: the algebraic argument below is very abridged, it’s more to get the point across)\n\\[\\begin{align*}\n\\lim_{n\\rightarrow\\infty}S_{C'}^2\n=\\lim_{n\\rightarrow\\infty}\\frac{1}{n-1}\\left(\\sum_{i=m+1}^n(d-\\overline{X}_{C'})^2\n+\\sum^m_{i=1}(c_i-\\overline{X}_{C'})^2\\right) \\\\\n=\\lim_{n\\rightarrow\\infty}\\frac{1}{n-1}\\sum_{i=m+1}^n(d-\\overline{X}_{C'})^2+\n\\lim_{n\\rightarrow\\infty}\\frac{1}{n-1}\\sum^m_{i=1}(c_i-\\overline{X}_{C'})^2=\\cdots\n\\end{align*}\\]\nWe know that the first of these two sums will approach 0, as the difference between the mean and \\(d\\) will approach 0. As such we will treat this first sum as 0.\n\\[\\begin{align*}\n\\cdots=0+\\lim_{n\\rightarrow\\infty}\\frac{1}{n-1}\\sum^m_{i=1}(c_i-\\overline{X}_{C'})^2\\\\\n=\\lim_{n\\rightarrow\\infty}\\frac{1}{n-1}\\sum^m_{i=1}(c_i^2-2c_i\\overline{X}_{C'}+\\overline{X}_{C'}^2)\\\\\n=\\lim_{n\\rightarrow\\infty}\\frac{1}{n-1}*\n\\lim_{n\\rightarrow\\infty}\\sum^m_{i=1}(c_i^2-2c_i\\overline{X}_{C'}+\\overline{X}_{C'}^2)\\\\\n=\\lim_{n\\rightarrow\\infty}\\frac{1}{n-1}*\n\\left(c_i^2-2c_id+d^2\\right)=0*\\left(c_i^2-2c_id+d^2\\right)=0\n\\end{align*}\\]\n\n\n\nProblem 27\nYou are given the following dataset containing information about survival of people travelling on the Titanic in 1912. The data contains counts of how many who survived or not of groups by ticket class, sex, age (child/adult). Can you categorize the columns based on whether they are categorical, numeric, binary, integer or real?\n\n\n\n\n\nClass\nSex\nAge\nSurvived\nFreq\n\n\n\n\n1st\nMale\nChild\nNo\n0\n\n\n2nd\nMale\nChild\nNo\n0\n\n\n3rd\nMale\nChild\nNo\n35\n\n\nCrew\nMale\nChild\nNo\n0\n\n\n1st\nFemale\nChild\nNo\n0\n\n\n2nd\nFemale\nChild\nNo\n0\n\n\n3rd\nFemale\nChild\nNo\n17\n\n\nCrew\nFemale\nChild\nNo\n0\n\n\n1st\nMale\nAdult\nNo\n118\n\n\n2nd\nMale\nAdult\nNo\n154\n\n\n\n\n\n\n\nShow suggested solutions\n\n\nClass: Categorical, although could be treated as ordinal (1st &gt; 2nd &gt; 3rd &gt; Crew)\nSex: Binary (Male/female)\nAge: Binary (Child/adult)\nSurvived: Binary (no/yes)\nFreq: Numerical, integer.",
    "crumbs": [
      "Modules",
      "Module 1: Summarizing and visualizing data",
      "Exercises"
    ]
  },
  {
    "objectID": "1-idealised_representations.html",
    "href": "1-idealised_representations.html",
    "title": "Idealised representations",
    "section": "",
    "text": "Slides for “Idealised representations”\n\n\n\nHow many parameters in a Gaussian distribution?\nWhat are the characteristics of a Gaussian distribution?\nWhat does it mean that a distribution is right-skewed?\nIn the context of distributions, what is the tail?\nWhat does it mean that a distribution has a long-tail?",
    "crumbs": [
      "Modules",
      "Module 1: Summarizing and visualizing data",
      "Idealised representations"
    ]
  },
  {
    "objectID": "1-idealised_representations.html#controll-questions",
    "href": "1-idealised_representations.html#controll-questions",
    "title": "Idealised representations",
    "section": "",
    "text": "How many parameters in a Gaussian distribution?\nWhat are the characteristics of a Gaussian distribution?\nWhat does it mean that a distribution is right-skewed?\nIn the context of distributions, what is the tail?\nWhat does it mean that a distribution has a long-tail?",
    "crumbs": [
      "Modules",
      "Module 1: Summarizing and visualizing data",
      "Idealised representations"
    ]
  },
  {
    "objectID": "1-plotting-tools.html",
    "href": "1-plotting-tools.html",
    "title": "Plotting tools",
    "section": "",
    "text": "Slides for “Plotting tools”\n\n\n\nWhat is the difference between a bar plot and a histogram?\nWhat is the horizontal line inside the box of a boxplot?\nWhat do we call the height of the box in a boxplot?\nWhat plot would you use for illustrating dependence between two continuous (numeric) variables?\nWhat is a time plot?",
    "crumbs": [
      "Modules",
      "Module 1: Summarizing and visualizing data",
      "Plotting tools"
    ]
  },
  {
    "objectID": "1-plotting-tools.html#controll-questions",
    "href": "1-plotting-tools.html#controll-questions",
    "title": "Plotting tools",
    "section": "",
    "text": "What is the difference between a bar plot and a histogram?\nWhat is the horizontal line inside the box of a boxplot?\nWhat do we call the height of the box in a boxplot?\nWhat plot would you use for illustrating dependence between two continuous (numeric) variables?\nWhat is a time plot?",
    "crumbs": [
      "Modules",
      "Module 1: Summarizing and visualizing data",
      "Plotting tools"
    ]
  },
  {
    "objectID": "1-statistical-thinking.html",
    "href": "1-statistical-thinking.html",
    "title": "Statistical Thinking",
    "section": "",
    "text": "Slides for “Statistical Thinking”\n\n\n\nWhat are the three things statistics can help us with?\nWhich word is not a part of statistical thinking definitions?\n\nData\nUncertainty\nIntuition\nReasoning\n\nWhat is the “big idea” of aggregation in statistics?\nDoes more data generally imply higher uncertainty?",
    "crumbs": [
      "Modules",
      "Module 1: Summarizing and visualizing data",
      "Statistical Thinking"
    ]
  },
  {
    "objectID": "1-statistical-thinking.html#controll-questions",
    "href": "1-statistical-thinking.html#controll-questions",
    "title": "Statistical Thinking",
    "section": "",
    "text": "What are the three things statistics can help us with?\nWhich word is not a part of statistical thinking definitions?\n\nData\nUncertainty\nIntuition\nReasoning\n\nWhat is the “big idea” of aggregation in statistics?\nDoes more data generally imply higher uncertainty?",
    "crumbs": [
      "Modules",
      "Module 1: Summarizing and visualizing data",
      "Statistical Thinking"
    ]
  },
  {
    "objectID": "1-summary-statistics.html",
    "href": "1-summary-statistics.html",
    "title": "Summary statistics",
    "section": "",
    "text": "Slides for “Summary statistics”\n\n\n\nWhat does the mean, median and mode aim to describe?\nHow would you find the median if you had an even number of observations?\nWhich percentile is the median?\nWhat is the 100-percentile usually called?\nWhat is the relationship between variance and standard deviation?\nWhy prefer the standard deviation instead of the variance?",
    "crumbs": [
      "Modules",
      "Module 1: Summarizing and visualizing data",
      "Summary statistics"
    ]
  },
  {
    "objectID": "1-summary-statistics.html#controll-questions",
    "href": "1-summary-statistics.html#controll-questions",
    "title": "Summary statistics",
    "section": "",
    "text": "What does the mean, median and mode aim to describe?\nHow would you find the median if you had an even number of observations?\nWhich percentile is the median?\nWhat is the 100-percentile usually called?\nWhat is the relationship between variance and standard deviation?\nWhy prefer the standard deviation instead of the variance?",
    "crumbs": [
      "Modules",
      "Module 1: Summarizing and visualizing data",
      "Summary statistics"
    ]
  },
  {
    "objectID": "1-videos.html",
    "href": "1-videos.html",
    "title": "Working with data",
    "section": "",
    "text": "Here comes a video"
  },
  {
    "objectID": "2-basic-concepts-in-probability.html",
    "href": "2-basic-concepts-in-probability.html",
    "title": "Basic concepts in probability",
    "section": "",
    "text": "Slides for “Basic concepts in probability”\n\n\n\nWhat is an experiment?\nWhat is an outcome?\nWhat is the sample space?\nWhat is an event?\nIn a dice-throwing experiment, if event 𝐴 is “rolling a 1, 2, or 3,” what is the complement of 𝐴?\nFor events 𝐴 (rolling a 1 or 2) and 𝐵 (rolling a 2, 3, or 4), describe their intersection and how it would appear in a Venn diagram.\nUsing the same events 𝐴 and 𝐵 as above, describe their union. What does it represent, and how would you illustrate it in a Venn diagram?",
    "crumbs": [
      "Modules",
      "Module 2: Probability, random variables, probability distributions and simulations.",
      "Basic concepts in probability"
    ]
  },
  {
    "objectID": "2-basic-concepts-in-probability.html#controll-questions",
    "href": "2-basic-concepts-in-probability.html#controll-questions",
    "title": "Basic concepts in probability",
    "section": "",
    "text": "What is an experiment?\nWhat is an outcome?\nWhat is the sample space?\nWhat is an event?\nIn a dice-throwing experiment, if event 𝐴 is “rolling a 1, 2, or 3,” what is the complement of 𝐴?\nFor events 𝐴 (rolling a 1 or 2) and 𝐵 (rolling a 2, 3, or 4), describe their intersection and how it would appear in a Venn diagram.\nUsing the same events 𝐴 and 𝐵 as above, describe their union. What does it represent, and how would you illustrate it in a Venn diagram?",
    "crumbs": [
      "Modules",
      "Module 2: Probability, random variables, probability distributions and simulations.",
      "Basic concepts in probability"
    ]
  },
  {
    "objectID": "2-bayes-rule.html",
    "href": "2-bayes-rule.html",
    "title": "Bayes rule",
    "section": "",
    "text": "Slides for “Bayes rule”\n\n\n\nWhat is Bayes rule?\nWhen do we need the Bayes rule?\nWhat is the base rate fallacy?",
    "crumbs": [
      "Modules",
      "Module 2: Probability, random variables, probability distributions and simulations.",
      "Bayes rule"
    ]
  },
  {
    "objectID": "2-bayes-rule.html#controll-questions",
    "href": "2-bayes-rule.html#controll-questions",
    "title": "Bayes rule",
    "section": "",
    "text": "What is Bayes rule?\nWhen do we need the Bayes rule?\nWhat is the base rate fallacy?",
    "crumbs": [
      "Modules",
      "Module 2: Probability, random variables, probability distributions and simulations.",
      "Bayes rule"
    ]
  },
  {
    "objectID": "2-binomial.html",
    "href": "2-binomial.html",
    "title": "Binomial distribution",
    "section": "",
    "text": "Slides for “The Binomial distribution”\n\n\n\nWhat conditions must be satisfied for a random experiment to follow a binomial distribution?\nWhat do the parameters n and p represent?\nIn the binomial probability mass function, what does the first component \\(n\\choose x\\) (“n choose x”) represent?\nIn the binomial probability mass function, what does the second component \\(p^x\\,(1−p)^{n−x}\\) represent?",
    "crumbs": [
      "Modules",
      "Module 2: Probability, random variables, probability distributions and simulations.",
      "Binomial distribution"
    ]
  },
  {
    "objectID": "2-binomial.html#controll-questions",
    "href": "2-binomial.html#controll-questions",
    "title": "Binomial distribution",
    "section": "",
    "text": "What conditions must be satisfied for a random experiment to follow a binomial distribution?\nWhat do the parameters n and p represent?\nIn the binomial probability mass function, what does the first component \\(n\\choose x\\) (“n choose x”) represent?\nIn the binomial probability mass function, what does the second component \\(p^x\\,(1−p)^{n−x}\\) represent?",
    "crumbs": [
      "Modules",
      "Module 2: Probability, random variables, probability distributions and simulations.",
      "Binomial distribution"
    ]
  },
  {
    "objectID": "2-continuous-random-variables.html",
    "href": "2-continuous-random-variables.html",
    "title": "Continuous random variables and distributions",
    "section": "",
    "text": "Slides for “Continuous random variables and distributions”\n\n\n\nWhat is a continuous random variable?\nHow do we interpret a probability density function (pdf)?\nWhat is the probability that a continuous random variable takes an exact value?\nHow do you compute the probability that \\(a&lt;X&lt;b\\)?\nWhat properties must a probability density function satisfy?",
    "crumbs": [
      "Modules",
      "Module 2: Probability, random variables, probability distributions and simulations.",
      "Continuous random variables and distributions"
    ]
  },
  {
    "objectID": "2-continuous-random-variables.html#controll-questions",
    "href": "2-continuous-random-variables.html#controll-questions",
    "title": "Continuous random variables and distributions",
    "section": "",
    "text": "What is a continuous random variable?\nHow do we interpret a probability density function (pdf)?\nWhat is the probability that a continuous random variable takes an exact value?\nHow do you compute the probability that \\(a&lt;X&lt;b\\)?\nWhat properties must a probability density function satisfy?",
    "crumbs": [
      "Modules",
      "Module 2: Probability, random variables, probability distributions and simulations.",
      "Continuous random variables and distributions"
    ]
  },
  {
    "objectID": "2-exercises.html",
    "href": "2-exercises.html",
    "title": "Exercises",
    "section": "",
    "text": "Exercises\n\nProblem 1\nYou have an event your interested in studying, \\(A\\). What are the lower and upper bounds for the probability that \\(A\\) occurs? I.e.what are the lower and upper bounds of \\(P(A)\\)?\n\n\nShow solutions\n\nBy the definition of the probability of an event we know that it’s bounded both from above and below. The upper bound being that \\(A\\) is guaranteed, and the lower bound being that it is impossible. We represent this on the interval \\([0,1]\\). I.e. we know that the probability of even t \\(A\\) occurring is between \\(0\\) and \\(1\\). More compact \\(P(A)\\in[0,1]\\)\n\n\n\nProblem 2\nOne day you walk outside and overhear a person talking about the probability of an event. You hear little of what they have to say, but you do hear this person say “…this won’t have to be enough, we will have to redo this experiment many more times to be certain I am approaching the right probability.” Amazingly, some time later you hear another person mumbling to them selves about probability. “Wow, now this is a surprise. This result did not coincide with my previous beliefs at all. I will have to rethink the probability of such an event occurring”, they say.\nYou get the feeling that these two people have very different way of interpreting probability? Identify who the frequentist and who the bayesian statistician is.\n\n\nShow solutions\n\nHere, person 1 is using frequentistic methods. Person 1’s focus is entirely on getting in as many independent trials as possible to get a clear image of the probability of an event. Person 2 on the other hand has a completely different view. Person 2 obviously had an initial belief about the probability of an event, but after seeing a surprising result they were determined to update their beliefs about the probability of this event.\n\n\n\nProblem 3\nVegard is very interested in the quality of watermelons. He has decided that he wants to find out the probability of watermelons being overripe at REMA 1000. He feels he has two feasible tests he can do to find this probability, one is Bayesian in nature and one is frequentist. Vegard can either go in with an initial belief, buy a single watermelon, then update his beliefs. He can keep doing this until he is quite certain of the probability of the population. On the other hand, Vegard can buy 40 watermelons at once, check them all, and then argue for that sample being representative of the population.\nWhich method is Bayesian and which is frequentist?\n\n\nShow solutions\n\nThe first option here is Bayesian. Vegard has an initial belief on the probability of the watermelons being overripe, and then he iterates and tries to make that more exact by checking one watermelon at the time.\nThe second option is a very typical frequentist test. From the whole population, a decent chunk is tested at once, in hope that it’s a representative sample. In the representative sample overripe watermelons should have the same relative frequency as in the entire population.\n\n\n\nProblem 4\nYou have a sample space, \\(S\\), and in that sample space you have two identifiable events, \\(A\\) and \\(B\\). Can you determine a case where the probability of the intersection of \\(A\\) and \\(B\\) (\\(P(A\\cap B)\\)) occurring, equals the probability of \\(A\\) (\\(P(A)\\)) occurring? Draw a Venn diagram!\n\n\nShow solutions\n\nRecall that the intersection of two events is when they both occur. I.e. for us to be in the intersection in a Venn diagram, both shapes have to cover the same area. There are infinitely variations here, but the important part is that \\(A\\) must be contained within \\(B\\). When this is the case, all of A will be intersected by \\(B\\) (we say that \\(A\\) is a subset of \\(B\\), and the notation is \\(A\\subseteq B\\)). In this case we get: \\[P(A\\cap B)=P(A)\\]\n\n\n\nProblem 5\nYou have a sample space, \\(S\\), and in that sample space you have two identifiable events, \\(A\\) and \\(B\\). Can you determine a case where the probability of the union of \\(A\\) and \\(B\\) (\\(P(A\\cup B)\\)) occurring, equals the probability of \\(B\\) (\\(P(B)\\)) occurring? Draw a Venn diagram!\n\n\nShow solutions\n\nRecall that the union of two events is when either one or both events occur. This means that all area covered by either \\(A\\) or \\(B\\) will be part of the union. We now want the probability of the union occurring to equal the probability of \\(B\\) occurring. Since all area covered by either \\(A\\) or \\(B\\) is part of the union, we can have no excess are beyond \\(B\\), i.e. \\(A\\) should once again be a subset of \\(B\\) (\\(A\\subseteq B\\)). If this is the case we get: \\[P(A\\cup B)=P(B).\\]\n\n\n\nProblem 6\nConsider a situation where you have \\(0&lt;P(A)&lt;P(B)&lt;1\\).\n\nWhat are the lower and upper bounds of \\(P(A\\cap B)\\)?\nWhat are the lower and upper bounds of \\(P(A\\cup B)\\)?\n\n\n\nShow solutions\n\n\nThe intersection tells us that both \\(A\\) and \\(B\\) have to occur, and as such, we the lower one of the two gives us an upper bound, as the one event cant intersect with more than the cases where it itself occurs (draw a Venn diagram to confirm). We know nothing as to whether or not \\(A\\) and \\(B\\) are disjoint, and as this is still a possibility we may not have an intersection, this makes \\(0\\) our lower bound. I.e. \\[P(A\\cap B)\\in[0, P(A)]\\]\nThe union tells us that either \\(A\\) or \\(B\\) or both occurs. As all events where either one or both occurs, the unions lower bound is given by the higher of the two probabilities. We could also be in a situation where \\(A\\cup B\\) cover the whole sample space \\(S\\), and in such a case we would have an upper bound of 1. I.e: \\[P(A\\cup B)\\in[P(B),1]\\]\n\n\n\n\nProblem 7\nWhat is \\(A\\cap B\\) when events \\(A\\) and \\(B\\) are disjoint.\n\n\nShow solutions\n\nWhen the two events \\(A\\) and \\(B\\) are disjoint, there are no possible situation that would fit into the intersection \\(A \\cap B\\). In set notation we would write: \\[A\\cap B=\\emptyset\\] where \\(\\emptyset\\) is called the empty set and denotes a set with no elements.\n\n\n\nProblem 8\nYou throw a fair six-sided dice, compute the probabilities of the following events.\n\nYou roll 1\nYou roll 3 or 4\nYou roll an odd number\nYou roll a number that’s greater than 5\nYou roll a number that’s less than or equal to 10\n\n\n\nShow solutions\n\nA fair 6 sided die gives us the sample space \\(S=\\{1,2,3,4,5,6\\}\\) with each respective outcome having the same probability to be tossed. i.e. if A is an event that denotes any one number being tossed, then: \\[\nP(A)=\\frac{1}{6}\\]\n\nRolling a 1 is a single element in the sample space, i.e. \\(P(1)=\\frac{1}{6}\\)\n3 and 4 constitute 2 possible disjoint outcomes, i.e. \\[P(3\\cup 4)=P(3)+P(4)=P\\frac{1}{6}+\\frac{1}{6}=\\frac{2}{6}=\\frac{1}{3}\\]\nThere are 3 different odd numbered outcomes\n\n\\[P(odd)=3\\cdot\\frac{1}{6}=\\frac{1}{2}\\]\n\nThere is only one possible outcome greater than 5, namely 6. I.e.\n\n\\[ P(X&gt;5)=P(6)=\\frac{1}{6}\\]\n\nOur sample space tells us we can only roll integers from 1 to 6, all of which are less than 10.\n\n\\[ P(X\\leq10)=P(1\\cup2\\cup3\\cup4\\cup5\\cup6)=\\sum_{i=1}^6\\frac{1}{6}=1\\]\n\n\n\nProblem 9\nYou flip a coin 5 times as an experiment.\n\nLet event \\(A\\) be getting all heads. What is \\(P(A)\\)?\nLet event \\(B\\) be the complement of \\(A\\), i.e \\(B=A^c\\). What is the probability of getting \\(B\\)?\nWhat is \\(P(A\\cap B)\\)?\nCompute \\(P(A \\cup B)\\), how do you interpret this result?\n\n\n\nShow solutions\n\n\nWe treat each coin flip as independent of all the others, we then also let the probability of getting heads be \\(P(H)=\\frac{1}{2}\\) for each coin toss. Since each toss is independent we can simply multiply them all together to find our final probability:\n\n\\[ P(A)=P(HHHHH)=\\prod_{i=1}^5\\frac{1}{2}=\\left(\\frac{1}{2}\\right)^5=\\frac{1}{2^5}=\\frac{1}{32} \\]\n\n\\(B\\) being the complement of \\(A\\) means that it will occur whenever \\(A\\) doesn’t. It’s also simple to compute as we can use a nifty formula.\n\n\\[\\begin{align}P(A^c)=1-P(A) \\\\\n\\Rightarrow P(B)=P(A^c)=1-P(A)=1-\\frac{1}{32}=\\frac{31}{32}\n\\end{align}\\]\n\nSince \\(A\\) and \\(B\\) are complements, they are by definition disjoint, as at no point will both occur (no intersection!!!). As such, we get a clear implication that \\(A\\cap B=\\emptyset\\) Following from this \\(P(A\\cap B)=0\\)\nFinally we can compute the union. Recall that we have a formula for this.\n\n\\[P(A \\cup B)=P(A)+P(B)-P(A\\cap B) \\]\nWe know all of the relevant probabilities needed for the computation.\n\\[ P(A \\cup B)=\\frac{1}{32}+\\frac{31}{32}-0=1 \\]\nThe probability of the union being 1 means that either \\(A\\), \\(B\\) or \\(A\\cap B\\) will occur. We already know, however that the events are disjoint, and as such there is no intersection. I.e. it’s either \\(A\\) or \\(B\\) that must occur. Interpreting this, we end up at the quite banal fact that when we do this experiment, we will* either* end up with 5 heads, or we will not (any other permutation). In fact this situation illustrates a powerful concept concerning complements of events, being that if an event does not occur, then the complement will occur.\n\n\n\nProblem 10\nLet \\(S\\) be a sample space. What is the probability of the complement of \\(S\\) (\\(P(S^c)\\))? What does this imply?\n\n\nShow solutions\n\nWe know from the axioms that \\(P(S)=1\\), i.e. we can easily compute the complement.\n\\[ P(S^c)=1-P(S)=1-1=0 \\]\nThis implies that the event of being within the sample space covers all other events. I.e. when we there is no chance of any event occurring that is not in the sample space.\n\n\n\nProbelm 11\nYou roll two 6 sided dice, one blue and one red. Consider now, what would be your sample space if:\n\nyou considered the value on the red die and blue die separately?\nyou consider the sum of the two dice?\n\n\n\nShow solutions\n\n\nFor every toss, we here have to consider two separate values, and we denote them as \\((x,y)\\). Each of the two dice can roll from 1 to 6. Adding it all together we can construct the sample space\n\n\\[\nS=\\left\\{\n\\begin{array}{cccccc}\n(1,1),& (1,2),& (1,3),& (1,4),& (1,5),& (1,6), \\\\\n(2,1),& (2,2),& (2,3),& (2,4),& (2,5),& (2,6), \\\\\n(3,1),& (3,2),& (3,3),& (3,4),& (3,5),& (3,6), \\\\\n(4,1),& (4,2),& (4,3),& (4,4),& (4,5),& (4,6), \\\\\n(5,1),& (5,2),& (5,3),& (5,4),& (5,5),& (5,6), \\\\\n(6,1),& (6,2),& (6,3),& (6,4),& (6,5),& (6,6)\n\\end{array}\n\\right\\}\n\\]\n\nWe now consider the sum of the two dice. The lowest value is when both roll 1’s, and the highest possible one is when both roll 6’s. Every integer value between these two sums are achievable. We now end up with:\n\n\\[ S_{sum}=\\{2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12\\} \\]\n\n\nProblem 12\nYou throw two dice.\n\nIn how many outcomes is their sum less equals 6?\nWhat is the probability of getting a sum total of 2?\n\n\n\nShow solutions\n\n\nIf dice 1 rolls a 6, then the other dice will make the sum at least 7, i.e. no candidates. Dice 1 rolls 5, then we get 6 if dice 2 rolls 1, i.e. 1 candidate. Following similar logic if dice 1 rolls 1-4, then each produce 1 more candidate.\n\nIn total we end up with 5 outcomes that give us a sum of 6.\n\nFor us to have a sum of 2, we need both dice to roll 1. i.e. there is only one outcome that nets this event There are 36 possible outcomes. i.e. \\(P(sum \\ 2)=\\frac{1}{36}\\)\n\n\n\n\nProblem 13\nLet \\(A=\\{1,2,3,4,6\\}\\), \\(B=\\{2,4,6,8\\}\\), and \\(C=\\{2,3,4,5\\}\\).\n\nFind \\(A\\cap B\\cap C\\)\nFind \\(A\\cup B\\cup C\\)\n\n\nShow solutions\n\n\nRecall first that\n\n\\[ A\\cap (B\\cap C)=A\\cap B\\cap C \\]\nSo we can find the intersection simply by looking for the elements that appear in all of the sets. We end up with.\n\\[ A\\cap B\\cap C = \\{2,4\\} \\]\n\nRecall first that\n\n\\[ A\\cup (B\\cup C)=A\\cup B\\cup C \\] The union will now consist of any element that falls within any of the sets, and as such we get\n\\[A\\cup B\\cup C =\\{1,2,3,4,5,6,8\\} \\]\n\n\nProblem 14\nYou know that the probabilities of some intersections are given by \\(P(A\\cap B)=\\frac{1}{2}\\) and \\(P(A\\cap B^c)=\\frac{1}{3}\\). Compute \\(P(A)\\).\n\n\nShow solutions\n\nTwo things are very clear when working with complements, and that’s that they are disjoint events and that the probability of one or the other occurring sum to 1 exactly. Directly following from this is that we can always make use of the law of total probability when we know the probabilities of the intersections between an event, \\(A\\), and both complements. In this case we have exactly the right information to make use of the law of total probability. Recall the formula. \\[ P(A)=\\sum_{i=1}^kP(A\\cap B_i)=P(A\\cap B_1)+\\cdots+P(A\\cap B_2) \\] Let’s use this to compute the probability\n\\[ P(A)=P(A\\cap B)+P(A\\cap B^c)=\\frac{1}{2}+\\frac{1}{3}=\\frac{3+2}{6}=\\frac{5}{6} \\]\n\n\nProblem 15\nWe have an event, \\(A\\), and we are interested in figuring out the probability of it occurring. We know the probabilities of \\(B_1,...,B_n\\) and we know that \\(\\sum_{i=1}^nP(B_i)=1\\). Why can’t we use the law of total probability here? What could go wrong? (It might help to draw a Venn diagram if you’re struggling.)\n\n\nShow solutions\n\nThe problem here is that we don’t know if \\(B_1,...,B_n\\) are disjoint events, and since, there may be areas where they intersect with each other, we run the risk of counting some space several times if we were to sum all intersections between \\(A\\) and all \\(B_i\\). We also can’t be certain taht the entire sample space is covered by \\(B_i\\) if tehy are not all disjoint (see problem 16 for implications). We can illustrate algebraically (to simplify we reduce some generality, but this argument does extend). Let \\[ n=2, \\ P(B_1\\cap B_2)&gt;0 \\] We can now let the \\(A\\cap (B_1\\cap B_2)\\neq\\emptyset\\), i.e. \\(A\\) also occurs in the intersection. We can now compute \\(P(A)\\):\n\\[\\begin{align*}\nP(A)&=P(A\\cap B_1)+P(A \\cap B_2)\\\\\n&=P(A\\cap((B_1\\cap B_2)\\cup(B_1\\cap B_2^c)))+P(A\\cap((B_1\\cap B_2)\\cup(B_1^c\\cap B_2))) \\\\\n&=P(A\\cap(B_1\\cap B_2)) + P(A\\cap(B_1\\cap B_2^c))+ P(A\\cap(B_1\\cap B_2)) + P(A\\cap(B_1^c\\cap B_2)) \\\\\n&=2P(A\\cap(B_1\\cap B_2))+ P(A\\cap(B_1\\cap B_2^c))+ P(A\\cap(B_1^c\\cap B_2))\n\\end{align*}\\]\nAs we can see here, we have to account for an area where \\(A\\) intersects twice, and as such, this effect will have us overestimate the probability of \\(A\\) occurring. As such the equation above is necessarily incorrect.\n\n\n\nProblem 16\nWe have an event, \\(A\\), and we are interested in figuring out the probability of it occurring. We know the probabilities of \\(B_1,...,B_n\\) and we know that all \\(B_i\\) are pairwise disjoint(\\(B_i\\cap B_j=\\emptyset, \\ i\\neq j, \\ \\forall i,j\\)). Why can’t we use the law of total probability here? What could go wrong? (It might help to draw a Venn diagram if you’re struggling.)\n\n\nShow solutions\n\nThe problem here is that the sum of all probabilities \\(P(B_i)\\) may not equal 1. If the sum of probabilities \\(P(B_i)\\) is less than 1, then there are cases where no events \\(B_i\\) occur. If \\(A\\) intersects here, the law of total probability will lead us to underestimating \\(P(A)\\), as there is no \\(B_i\\) that intersects this part of \\(A\\). Let’s try to show this algebraically (we reduce generality here too, but argument holds). \\[ P((B_1\\cup...\\cup B_n)^c)&gt;0, \\ (B_1\\cup...\\cup B_n)^c\\subset A, \\ n=2 \\] If we try to compute using the law of total probability we then get\n\\[ P(A)=P(A\\cap B_1)+P(A\\cap B_2) \\ (*)\\]\nNote that \\((E_1\\cup E_2)^c=E_1^c\\cap E_2^c\\)\nFrom this we know that \\(B_1^c \\cap B_2^c\\subset A\\).\nBecause they are disjoint we also know that \\(B_1\\subset B_2^c, \\ B_2\\subset B_1^c\\), but we obviously have \\(B_1, B_2\\not\\subset B_1^c \\cap B_2^c\\), as any event will be disjoint with its complement.\nThis tells us that we don’t account for \\(B_1^c \\cap B_2^c\\subset A\\) in \\((*)\\), and thus we must be underestimating the probability, and the equation cannot hold.\n\n\n\nProblem 17\nWe are considering the probability of an event, \\(A\\). We know that \\(P(A\\cap B)=0.2\\) and that \\(P(A\\cap B^c)=0.05\\). Compute \\(P(A)\\).\n\n\nShow solutions\n\nBy definition, the events \\(B\\) and \\(B^c\\) are disjoint and \\(P(B)+P(B^c)=1\\), and as such, we can make use of the law of total probability. \\[ P(A)=\\sum_{i=1}^nP(A\\cap B_i)=P(A\\cap B)+P(A\\cap B^c)=0.2+0.05=0.25=\\frac{1}{4} \\]\n\n\n\nProblem 18\nWe get that \\(P(A)=0.7\\), \\(P(B_1)=0.3\\), \\(P(B_2)=0.2\\), \\(P(B_3)=0.5\\). All \\(B_i\\) are disjoint events. We also get that \\(P(A | B_1)=0.3\\), \\(P(A | B_2)=0.9\\). Find \\(P(A | B_3)\\) using the law of total probability.\n\n\nShow solutions\n\nWe need to rewrite the law of total probability to compute the conditional probability we want. Let’s use the definition conditional probability for this. \\[\nP(A|B)=\\frac{P(A\\cap B)}{P(B)} \\Rightarrow P(A\\cap B)=P(A | B)P(B) \\ (*)\n\\]\nRecall the law of total probability, now we can rewrite it by substituting in \\((*)\\). We can use this altered equation to solve for \\(P(A|B_3)\\)\n\\[ \\begin{align*}\nP(A)&=\\sum_{i=1}^nP(A\\cap B_i)=\\sum^n_{i=1}P(A|B_i)P(B_i) \\\\\n\\Rightarrow P(A)&=P(A|B_1)P(B_1)+P(A|B_2)P(B_2)+P(A|B_3)P(B_3) \\\\\n\\Rightarrow P(A|B_3)&=\\frac{P(A)-P(A|B_1)P(B_1)-P(A|B_2)P(B_2)}{P(B_3)}\n\\end{align*}\\]\nNow we can easily compute \\(P(A|B_3)\\)\n\\[\nP(A|B_3)=\\frac{0.7-0.3*0.3-0.2*0.9}{0.5}=\\frac{0.7-0.09-0.18}{0.5}=0.86\n\\]\n\n\n\nProblem 19\nConsider two events, \\(A\\)and \\(B\\). You get that \\(P(A)=0.1\\), \\(P(B)=0.2\\) and \\(P(A\\cap B)=0.05\\). Compute $P(A B) $ Argue for why \\(P(A\\cup B)=P(A)+P(B)-P(A\\cap B)\\) makes sense.\n\n\nShow solution\n\nWe already know the formula used for the computation.\n\\[ P(A\\cup B)=0.1+0.2-0.05=0.25 \\]\nTo argue for why the formula makes sense, let’s first divide up the union between A and B into disjoint parts. We know that the union includes any element within either A, B or both. We will have one part which is A and not B, and these will can be represented as intersections in and of themselves, i.e. \\(A\\cap B^c\\) and \\(A^c \\cap B\\). The final part of the union is when both A and B occurr, meaning our intersection \\(A\\cap B\\). With this we can find a formula for the union \\[ P(A\\cup B)=P(A\\cap B^c)+P(A^c\\cap B)+P(A\\cap B) \\quad (*)\\] Now, let’s consider \\(P(A)\\) and \\(P(B)\\). Any set \\(A\\) can necessarily be constructed by the union of it’s intersection with another set \\(B\\) and the complement of that set \\(B^c\\) on the sample space \\(S\\). This is very technical, but the idea is that every element (possible outcome) within \\(A\\) will either also be in \\(B\\), or not, hence ion \\(B^c\\). This idea let’s us rewrite \\(P(A)+P(B)\\), and we will represent them as sums similar to what we have in \\((*)\\).\n\\[\\begin{align*}\nP(A)+P(B)&=P(A\\cap B)+P(A\\cap B^c)+P(A\\cap B)+P(A^c\\cap B)\\\\\n&=2P(A\\cap B)+P(A\\cap B^c)+P(A\\cap B) \\quad (**)\\end{align*}\\]\nIt’s plain to see that \\((**)-P(A\\cap B)=(*)\\), and therefore\n\\[P(A\\cup B)=P(A)+P(B)-P(A\\cap B) \\]\n\n\n\nProblem 20\nYou get that \\(P(B)=0.5\\), \\(P(A\\cap B)=0.4\\). Compute \\(P(A|B)\\).\n\n\nShow solutions\n\nWe compute the conditional probability using the definition.\n\\[ P(A|B)=\\frac{P(A\\cap B)}{P(B)}=\\frac{0.4}{0.5}=0.8 \\]\n\n\n\nProblem 21\nYou get that \\(P(B)=0.2\\), \\(P(A | B)=0.9\\). Compute \\(P(A\\cap B)\\).\n\n\nShow solution\n\nWe make use of the definition here to compute the probability.\n\\[\n\\begin{align*}\nP(A|B)&=\\frac{P(A\\cap B)}{P(B)} \\Rightarrow P(A\\cap B)=P(A|B)P(B) \\\\\nP(A|B)&=0.9*0.2=0.18\n\\end{align*}\n\\]\n\n\n\nProblem 22\nYou find that \\(P(A)=0.6, \\ P(B)=0.3\\) and \\(P(A|B)+P(B|A)=0.9\\). Find \\(P(A\\cap B)\\).\n\n\nShow solutions\n\nLet’s try to rewrite \\(P(A|B)+P(B|A)=0.9\\) and solve for \\(P(A\\cap B)\\).\n\\[\\begin{align*}\nP(A|B)+P(B|A)=0.9 \\Leftrightarrow & \\frac{P(A\\cap B)}{P(B)}+\\frac{P(A\\cap B)}{P(A)}=\\frac{9}{10} \\\\\n\\Leftrightarrow &\\frac{10}{3}P(A\\cap B)+\\frac{10}{6}P(A\\cap B)=\\frac{9}{10} \\\\\n\\Leftrightarrow &\\frac{30}{6}P(A\\cap B)=\\frac{9}{10} \\Leftrightarrow 5P(A\\cap B)=\\frac{9}{10} \\\\\n\\therefore P(A\\cap B)&=\\frac{9}{50}=0.18\n\\end{align*}\\]\n\n\n\nProblem 23\nYou get that \\(P((A\\cup B)^c)=\\frac{1}{2}\\) and that \\(P(A\\cap B)=\\frac{3}{20}\\) \\(C\\) is the event that either \\(A\\) or \\(B\\) occurs, but not both. What is \\(P(C)\\)?\n\n\nShow solution\n\nFirst we should find a way to express \\(P(C)\\). The probability of \\(E\\) occurring is obviously related to the probabilities of \\(A\\) and \\(B\\) occrring. but we have to remove any cases where both of them occur, i.e. the intersection. Recall from problem 19 as we can construct an event \\(A\\) as all the outcomes shared with \\(B\\) (\\(P(A\\cap B)\\)) and all the outcomes which aren’t shared, or shared with not \\(B\\) (the complement \\(P(A\\cap B^c)\\)), in the same sample space. I.e.\n\\[ P(A)=P(A\\cap B)+P(A\\cap B^c) \\]\nThe same can be done for \\(B\\). Now to construct \\(P(C)\\) we can remove the respective \\(P(A\\cap B)\\) andb sum the remaining probabilities.\n\\[P(C)=P(A\\cap B^c)+P(A^c\\cap B)=P(A\\cup B)-P(A\\cap B)\\] Now we can finally start relating this to the probabilities given in the problem. We need to recall the complement rule, before we can compute however.\n\\[ P(A)=1-P(A^c) \\Rightarrow P(A\\cup B)=1-P((A\\cup B)^c)\\]\nWe can substitute this in to the expression for \\(P(C)\\)\n\\[ P(C)=1-P((A\\cup B)^c)-P(A\\cap B)=1-\\frac{1}{2}-\\frac{3}{20}=\\frac{7}{20}=0.35 \\]\n\n\n\nProblem 24\nProve that for any event \\(A\\) such that \\(A\\) is independent of another event \\(B\\), show that\n\\[P(A|B)=P(A)\\]\n\n\nShow solution\n\nConsider first the definition of independence.\n\\[ P(A\\cap B)=P(A)P(B) \\ (*)\\]\nNow let’s also consider the definition of conditional probability.\n\\[ P(A|B)=\\frac{P(A\\cap B)}{P(B)} \\]\nFinally let’s substitute in the definition given in \\((*)\\) and reduce the fraction.\n\\[ P(A|B)=\\frac{P(A)P(B)}{P(B)}=P(A) \\quad  q.e.d.\\]\n\n\n\nProblem 25\nYou find that \\(P(A)=\\frac{3}{10}\\), \\(P(B)=\\frac{1}{2}\\) and \\(P(A\\cap B)=\\frac{3}{10}\\)\nAre events \\(A\\) and \\(B\\) independent?\n\n\nShow solution\n\nWe check if the probabilities fulfill the conditions given by the definition.\n\\[ P(A)P(B)=\\frac{3}{10}\\frac{1}{2}=\\frac{3}{20}\\neq\\frac{3}{10}=P(A\\cap B) \\] I.e. A and B are not independent.\n\n\n\nProblem 26\nYou find that \\(P(A)=\\frac{5}{8}\\), \\(P(B)=\\frac{1}{5}\\) and \\(P(A\\cap B)=\\frac{1}{8}\\)\nAre events \\(A\\) and \\(B\\) independent?\n\n\nShow solution\n\nWe check if the probabilities fulfill the conditions given by the definition.\n\\[ P(A)P(B)=\\frac{5}{8}\\frac{1}{5}=\\frac{1}{8}=P(A\\cap B) \\] I.e. A and B are independent.\n\n\n\nProblem 27\nYou find that \\(P(A)=0.8\\), \\(P(B)=0.2\\) and \\(P(A\\cap B)=0.125\\)\nAre events \\(A\\) and \\(B\\) independent?\n\n\nShow solution\n\nWe check if the probabilities fulfill the conditions given by the definition.\n\\[ P(A)P(B)=0.8*0.2=0.16\\neq 0.125=P(A\\cap B) \\] I.e. A and B are not independent.\n\n\n\nProblem 28\nYou find that \\(P(A)=\\frac{3}{4}\\) and \\(P(A\\cap B)=\\frac{1}{10}\\)\nWhat must \\(P(B)\\) be for the events \\(A\\) and \\(B\\) to be independent.\n\n\nShow solution\n\nOur criterion follows from the definition.\n\\[P(A\\cap B)=P(A)P(B) \\Rightarrow P(B)=\\frac{P(A\\cap B)}{P(A)} \\] Now we can compute the probability.\n\\[ P(B)=\\frac{\\frac{1}{10}}{\\frac{3}{4}}=\\frac{1}{10}\\frac{4}{3}=\\frac{4}{30}=\\frac{2}{15} \\]\nSo \\(P(B)\\) should be \\(\\frac{2}{15}\\approx0.133\\)\n\n\n\nProplem 29\nYou have \\(P(A|B)=0.3\\), \\(P(A)=0.4\\), \\(P(B)=0.1\\). Compute the probability \\(P(B|A)\\).\n\n\nShow solution\n\nHere we can simply compute using Bayes rule!\n\\[ P(B|A)=\\frac{P(A|B)P(B)}{P(A)} \\] Let’s compute:\n\\[ P(B|A)=\\frac{0.3*0.1}{0.4}=\\frac{0.03}{0.4}=2.5*0.03=0.075\\]\nSo we conclude the probability of B given A is 7.5%\n\n\n\nProblem 30\nA certain disease affects 1% of a population. A test for the disease is 90% accurate for those who have the disease (i.e., it correctly identifies 90% of sick people) but also has a 5% false positive rate (i.e., it incorrectly identifies 5% of healthy people as having the disease).\nIf a randomly chosen person tests positive, what is the probability that they actually have the disease?\nFollowing, what would be the probability that someone testing positive is not sick?\nWould you consider this to be a good test?\n\n\nShow solution\n\nLet S be the event that a random person is suffering from the unnamed disease. Let T be the event that a random person gets a positive test. With this in mind, let’s try to identify what is given in the text.\n\\[ \\begin{align*}\nP(S)&=0.01 \\Rightarrow P(S^c)=0.99 \\\\\nP(T|S)&=0.90 \\\\\nP(T|S^c)&=0.05\n\\end{align*}\\]\nWe are now looking for then probability that a person who tests positive is sick. i.e. \\(P(S|T)\\).\n\\[\\begin{align*}\nP(S|T)&=\\frac{P(S\\cap T)}{P(S)}=\\frac{P(T|S)P(S)}{P(T)}\n\\end{align*}\\]\nWe already know the values of \\(P(T|S)\\) and \\(P(S)\\). Remains to find \\(P(T)\\).\nWe can attempt to use the law of total probability to find \\(P(T)\\).\n\\[\\begin{align*} P(T)&=P(T|S)P(S)+P(T|S^c)P(S^c)\\\\\n&=0.90*0.01+0.05*0.99\\\\\n&=0.009+0.0495=0.0585\\end{align*} \\]\nFinally; we can compute \\(P(S|T)\\)\n\\[P(S|T)=\\frac{0.90*0.01}{0.0585}\\approx0.154\\]\nLet’s now find the complement, i.e. \\(P(S^c|t)=1-P(S|T)\\).\n\\[P(S^c|T)\\approx1-0.154=0.846\\]\nThis should in no way be considered. When only 15% of cases report a true positive, that’s a bad sign. The false positive that constitutes about 85% of the positive tests is what we would call Type I errors. There is no one answer to what’s good level of Type I errors to have is when doing a test, but the most common benchmark is 5% (commonly denoted as \\(\\alpha=0.05\\), and is usually called the significance level of a test).\n\n\n\nProblem 31\nLet \\(A\\) and \\(B\\) be disjoint events. What is \\(P(A\\cap B)\\)? Feel free to draw a venn diagram to visualise this concept.\n\n\nShow solution\n\nWhen \\(A\\) and \\(B\\) disjoint, we get that\n\\[A\\cap B=\\emptyset \\Rightarrow P(A\\cap B)=0\\]\nLet’s plug this into the definition of a conditional probability.\n\\[ P(A|B)=\\frac{P(A\\cap B)}{P(B)}=\\frac{0}{P(B)}=0 \\]\n\n\nProblem 32\nIn a random draw you can end up with the respective values {3,7,9,11}, each with a respective probability of 0.25. Is this a probability distribution? Why or why not?\n\n\nShow solution\n\nThis is indeed a probability distribution. Here, each discrete possible value, is assigned a positive probability of occurring, and each of these probabilities add up to 1. Which are the requirements for a discrete probability distribution.\n\n\n\nProblem 33\nConsider now a fair dice with six faces.\n\nDo the results of a throw constitute a probability distribution?\nFind the expected value of the dice toss\nFind the variance of the dice toss\n\n\n\nShow solutions\n\n\nThis is indeed a probability distribution. Here, each discrete possible value, is assigned a positive probability of occurring, and each of these probabilities add up to 1. Which are the requirements for a discrete probability distribution.\nTo find the expected value of a randomly distributed variable we use the formula given by\n\n\\[ E[X]=\\sum_{i=1}^nx_ip(x_i) \\]\nHere, \\(X\\) denotes the random variable that has the distribution, \\(x_i\\) denotes the ith value \\(X\\) can take, and \\(p(x_i)\\) denotes the probability that \\(X\\) will take the value \\(x_i\\). Note that the sum of all \\(p(x_i)\\) should always add up to 1 exactly.\nFor a fair die with 6 faces we get possible values {1,2,3,4,5,6} and respective probabilities \\(p=\\frac{1}{6}\\) for each possible outcome. With this we can easily find the expected value.\n\\[ E[X]=\\sum_{i=1}^6x_ip(x_i)=\\sum_{i=1}^6x_i\\frac{1}{6}=\\frac{1}{6}(1+2+3+4+5+6)=\\frac{21}{6}=3.5 \\]\n\nWe have a general formula we can use to determine the variance in a distribution.\n\n\\[Var[X]=E[X^2]-(E[X])^2\\]\nFrom b) we already know \\(E[X]\\), and that can just be squared to find the second term in the equation, however, the first term \\(E[X^2]\\) still remains to be found. This term is found by squaring the values our random variable can take, i.e.\n\\[ E[X]=\\sum_{i=1}^nx_i^2p(x_i) \\] Let’s now compute this\n\\[ E[X]=\\sum_{i=1}^6x_i^2p(x_i)=\\sum_{i=1}^6x_i^2\\frac{1}{6}=\\frac{1}{6}(1+4+9+16+25+36)=\\frac{91}{6}\\approx15.167 \\]\nNow remember to finis the equation.\n\\[Var[X]=\\frac{91}{6}-(3.5)^2\\approx2.9167 \\]\n\n\n\nProblem 34\nA random variable, \\(X\\) has a binomial distribution if the three following requirements are fulfilled.\n\nThere are one or more draws which can end up in two states. Success or failure.\nFor each draw, there is the same probability for success.\nEach draw is mutually indpendent from all other draws.\n\nWe denote the total number of draws as \\(n\\), and the probability fro success on each draw as \\(p\\). For a binomial distribution we have special formulas for the expected value and variance.\n\\[ E[X]=np, \\ Var[X]=np(1-p) \\]\nFor a Binomial distribution, the probability distribution is given by\n\\[ P(X=x)=\\binom{n}{x}p^x(1-p)^{n-x} \\] Where x denotes the amount of successes drawn.\nA lightbulb manufacturing company claims that 95% of their lightbulbs pass quality control. A quality inspector randomly selects 20 lightbulbs from a day’s production line for testing. Let the amount of lightbulbs that pass quality control be given by the random variable, \\(X\\), and let \\(X\\) have a binomial distribution with \\(n=20\\) and \\(p=0.95\\).\n\nWhat is the probability that exactly 18 lightbulbs pass the quality check?\nWhat is the probability that at least 19 lightbulbs pass the quality check?\nWhat is the expected number of lightbulbs that pass the quality check?\nWhat is the variance and standard deviation (\\(\\text{SD}(X)\\)) of the number of passing lightbulbs?\n\n\n\nShow solutions\n\n\nTo find the probability that 18 bulbs pass the check we can simply plug our given information into the probability function. \\[\n\\begin{align*}\nP(X=18)&=\\binom{20}{18}0.95^{18}(1-0.95)^{20-18}\\\\&=\\frac{20!}{2!18!}0.95^{18}0.05^2\\\\&=\\frac{20*19}{2}0.95^{18}0.05^2\\\\&=\n190*0.95^{18}0.05^2\\approx1.887\n\\end{align*}\\]\nTo find the probability \\(P(X\\geq19)\\). Since we have only two cases where this is the case \\(P(X=19)\\) and \\(P(X=20)\\), as we only check 20 bulbs, this is a simple task. We need only compute these two seperate probabilities and sum them together.\n\nWe again make use pf the probability function.\n\\[ P(X=19)=\\binom{20}{19}0.95^{19}(1-0.95)^{20-19}=20*0.95^{19}*0.05\\approx0.377 \\]\n\\[ P(X=20)=\\binom{20}{20}0.95^{20}0.05^{0}=0.95^{20}\\approx0.358 \\]\nNow we need only sum these probabilities.\n\\[ P(X\\geq19)=P(X=19)+P(X=20)\\approx0.377+0.358=0.735 \\]\n\nThe expected number of lightbulubs that pass the quality check is given by the distinct fomrula\n\n\\[ E[X]=np=20*0.95=19 \\]\n\nThe variance is given by the formula\n\n\\[ Var[X]=np(1-p)=20*0.95*0.05=0.95 \\]\nThe standard deviation is always given as the postive square root of the variance.\n\\[\\text{SD}(X)=\\sqrt{Var[X]}=\\sqrt{0.95}\\approx0.975 \\]\n\n\n\nProblem 35\nA random variable, ( X ), has a Poisson distribution if the following conditions are fulfilled:\n\nEvents occur randomly and independently over time or space.\nThe events occur at a known average rate ( \\(\\lambda\\) ).\nTwo events cannot occur at exactly the same time.\n\nWe denote the average rate of occurrence by ( \\(\\lambda\\) ).\nFor a Poisson distribution we have the following formulas for the expected value and variance:\n\\[\nE[X] = \\lambda, \\quad Var[X] = \\lambda\n\\]\nFor a Poisson distribution, the probability distribution is given by:\n\\[\nP(X = x) = \\frac{e^{-\\lambda} \\lambda^x}{x!}\n\\]\nA call center receives an average of 3 customer calls per hour. Let the number of calls received in a given hour be modeled by the random variable ( X (10) ).\n\nWhat is the probability that exactly 2 calls are received in an hour?\nWhat is the probability that at less than 2 calls are received in an hour?\nWhat is the expected number of calls received per hour?\nWhat is the variance and standard deviation (( (X) )) of the number of calls per hour?\n\n\n\nShow solutions\n\n\nIn this case we can simply compute the probabilty with teh help of our distribution fuction. \\[\nP(X = 2) = \\frac{e^{-3} \\cdot 3^2}{2!} = \\frac{9e^{-3}}{2}  \\approx 0.224\n\\]\nHere we need to realise that less than 2 is the same as 0 or in this case. As such, we would like to compute both the probaility that none call in an hour and the probaility that exactly one call is received. Then finally we can sum these two proabilities.\n\n\\[ P(X=0)=\\frac{e^{-3}\\cdot3^0}{0!}=e^{-3}\\approx 0.050 \\] \\[ P(X=1)=\\frac{e^{-3}\\cdot3^1}{1!}=3e^{-3}\\approx0.149 \\]\n\\[\nP(X &lt; 2) = P(X=0)+P(X=1)\\approx0.050+0.149=0.199\n\\]\n\nFor a Poisson distribution the expected value is simply given by the parameter \\(\\lambda\\) which here represents average callers per hour. \\[\nE[X] = \\lambda = 3\n\\]\nFor a Poisson distribution in particular the variance and the expected value is the same \\[\nVar[X] = \\lambda = 3, \\quad \\text{SD}(X) = \\sqrt{3} \\approx 1.732\n\\]\n\n\n\n\nProblem 36\nA random variable, \\(X\\), has a geometric distribution if the three following requirements are fulfilled:\n\nEach trial results in either success or failure.\nThe probability of success is the same for every trial.\nEach trial is mutually independent from all other trials.\n\nThe geometric distribution models the number of trials up to and including the first success.\nWe denote the probability of success as \\(p\\). And thus it is very related to the binomial distribution. Whereas in the binomial we have decided upon a certain number of trials and then we check how many successes we’ve found, with a geometric we keep doing trials until we find a success, and then stop.\nFor a geometric distribution we have special formulas for the expected value and variance:\n\\[\nE[X] = \\frac{1}{p}, \\quad Var[X] = \\frac{1 - p}{p^2}\n\\]\nThe probability distribution is given by:\n\\[\nP(X = x) = (1 - p)^{x - 1}p\n\\]\nA machine produces small parts for watches. Each part has a 2% chance of being defective. A quality inspector examines parts one by one until the first defective part is found.\nLet the number of parts examined be given by the random variable \\(X\\), and let \\(X\\) have a geometric distribution with \\(p = 0.02\\).\n\nWhat is the probability that the first defective part is the 10th one tested?\nWhat is the probability that more than 20 parts are tested before finding a defective one? (Hint: \\(P(X\\leq x)=1-(1-p)^x\\) (this is a pretty simple to show, but could be unintuitive))\nWhat is the expected number of parts tested before finding a defective one?\nWhat is the variance and standard deviation (\\(\\text{SD}(X)\\)) of the number of parts tested?\n\n\n\nShow solutions\n\n\nThis is a simple computation with the probability function. \\[\nP(X = 10) = (1 - 0.02)^9 \\cdot 0.02 = 0.98^9 \\cdot 0.02 \\approx 0.1667\n\\]\nLet’s be a bit clever when computing this We have that\n\n\\[ P(X&gt;x)=1-P(X\\leq x) \\] By the rule of complements.\nLet’s now subsitute in for the hint we got\n\\[ P(X&gt;x)=1-(1-(1-p)^x)=1-1+(1-p)^x=(1-p)^x \\] Finally we can subsititute in for \\(p\\) and \\(x\\)\n\\[\nP(X &gt; 20) = (1 - 0.02)^{20} = 0.98^{20} \\approx 0.667\n\\]\n\nLet’s compute teh expected value with the formula! \\[\nE[X] = \\frac{1}{0.02} = 50\n\\]\nLet’s now du the same for teh variance and standard deviation! \\[\nVar[X] = \\frac{1 - 0.02}{0.02^2} = \\frac{0.98}{0.0004} = 2450\n\\]\n\\[\n\\text{SD}(X) = \\sqrt{2450} \\approx 49.5\n\\]\n\n\n\n\nProblem 37\nA random variable, \\(X\\), has a negative binomial distribution if the three following requirements are fulfilled:\n\nEach trial results in either success or failure.\nThe probability of success is the same for each trial.\nEach trial is mutually independent from all other trials.\n\nThe negative binomial distribution models the number of trials needed to get exactly \\(r\\) successes.\nWe denote the number of successes as \\(r\\) and the probability of success on each trial as \\(p\\). So it’s in a way the inverse of the binomial distribution.\nFor a negative binomial distribution we have special formulas for the expected value and variance:\n\\[\nE[X] = \\frac{r}{p}, \\quad Var[X] = \\frac{r(1 - p)}{p^2}\n\\]\nThe probability distribution is given by:\n\\[\nP(X = x) = \\binom{x - 1}{r - 1} p^r (1 - p)^{x - r}\n\\]\nA salesperson has a 30% chance of closing a sale on each call. They continue calling until they have made 3 sales.\nLet the number of calls made be given by the random variable \\(X\\), and let \\(X\\) have a negative binomial distribution with \\(r = 3\\) and \\(p = 0.3\\).\n\nWhat is the probability that the third sale occurs on the 5th call?\nWhat is the probability that more than 7 calls are needed?\nWhat is the expected number of calls made before making 3 sales?\nWhat is the variance and standard deviation (\\(\\text{SD}(X)\\)) of the number of calls made?\n\n\n\nShow solutions\n\n\nHere we can simply compute using the distribution function. \\[\nP(X = 5) = \\binom{4}{2} \\cdot (0.3)^3 \\cdot (0.7)^2 = 6 \\cdot 0.027 \\cdot 0.049 \\approx 0.079\n\\]\nNote here that since we need at least 3 sales, there is zero chance that the salesperson is done before they have made three calls. Thus, when using the rule for complements we can write \\[P(X\\leq7)=\\sum^7_{i=1}P(X=x)=0+0+\\sum_{i=3}^7P(X=x)=\\sum_{i=3}^7P(X=x) \\] Now, by computing \\(P(X=3), ..., P(X=7)\\) (same procedure as in a)), we can find our desired probability. \\[\nP(X &gt; 7) = 1 - P(X \\leq 7) = 1 - \\sum_{x=3}^{7} P(X = x) \\approx 1 - 0.353= 0.647\n\\]\nHere we simply use the fomrula \\[\nE[X] = \\frac{3}{0.3} = 10\n\\]\nHere we simply use the formula \\[\nVar[X] = \\frac{3(1 - 0.3)}{0.3^2} = \\frac{2.1}{0.09} = 23.\\overline{3}\n\\]\n\\[\n\\text{SD}(X) = \\sqrt{23.\\overline{3}} \\approx 4.83\n\\]\n\n\n\n\nProblem 38\nA random variable, \\(X\\), has a hypergeometric distribution if the three following requirements are fulfilled:\n\nWe select a sample from a finite population without replacement.\nThe population contains a known number of “successes” and “failures”.\nEach draw changes the composition of the population.\n\nWe denote: - \\(N\\) = total population size\n- \\(K\\) = number of successes in the population\n- \\(n\\) = number of draws (sample size)\nFor a hypergeometric distribution we have special formulas for the expected value and variance:\n\\[\nE[X] = n \\cdot \\frac{K}{N}, \\quad Var[X] = n \\cdot \\frac{K}{N} \\cdot \\frac{N - K}{N} \\cdot \\frac{N - n}{N - 1}\n\\]\nThe probability distribution is given by:\n\\[\nP(X = x) = \\frac{\\binom{K}{x} \\binom{N - K}{n - x}}{\\binom{N}{n}}\n\\]\nIn a school of 100 students, 40 are members of the chess club. A random sample of 10 students is selected to take part in a survey.\nLet the number of chess club members in the sample be given by the random variable \\(X\\), and let \\(X\\) have a hypergeometric distribution with \\(N = 100\\), \\(K = 40\\), and \\(n = 10\\).\n\nWhat is the probability that exactly 4 chess club members are selected?\nWhat is the probability that at least 2 chess club members are selected?\nWhat is the expected number of chess club members selected?\nWhat is the variance and standard deviation (\\(\\text{SD}(X)\\)) of the number of chess club members selected?\n\n\n\nShow solutions\n\n\n\\[\nP(X = 4) = \\frac{\\binom{40}{4} \\binom{60}{6}}{\\binom{100}{10}}=\\frac{\\frac{40!}{36!4!}\\frac{60}{54!6!}}{\\frac{100!}{90!10!}} \\approx 0.264\n\\]\nWe compute: \\[\nP(X \\geq 2) = 1 - P(X = 0) - P(X = 1)\n\\] Where: \\[\nP(X = 0) = \\frac{\\binom{40}{0} \\binom{60}{10}}{\\binom{100}{10}}\\approx 0.004, \\quad P(X = 1) = \\frac{\\binom{40}{1} \\binom{60}{9}}{\\binom{100}{10}} \\approx 0.034\n\\] Thus: \\[\nP(X \\geq 2) \\approx 1 - (0.004 + 0.038) = 0.962\n\\]\nHere we can just compute the expectation with our fomrula \\[\nE[X] = 10 \\cdot \\frac{40}{100} = 4\n\\]\nThe variance and standard deviation are just as easy to compute \\[\nVar[X] = 10 \\cdot \\frac{40}{100} \\cdot \\frac{60}{100} \\cdot \\frac{90}{99} \\approx 2.18\n\\] \\[\n\\text{SD}(X) = \\sqrt{2.18} \\approx 1.48\n\\]",
    "crumbs": [
      "Modules",
      "Module 2: Probability, random variables, probability distributions and simulations.",
      "Exercises"
    ]
  },
  {
    "objectID": "2-independent-events.html",
    "href": "2-independent-events.html",
    "title": "Independent events",
    "section": "",
    "text": "Slides for “Independent events”\n\n\n\nWhat is the definition of two independent events?\nExplain how conditional probabilities can be used to check for independence between two events.\nExplain independence with words.",
    "crumbs": [
      "Modules",
      "Module 2: Probability, random variables, probability distributions and simulations.",
      "Independent events"
    ]
  },
  {
    "objectID": "2-independent-events.html#controll-questions",
    "href": "2-independent-events.html#controll-questions",
    "title": "Independent events",
    "section": "",
    "text": "What is the definition of two independent events?\nExplain how conditional probabilities can be used to check for independence between two events.\nExplain independence with words.",
    "crumbs": [
      "Modules",
      "Module 2: Probability, random variables, probability distributions and simulations.",
      "Independent events"
    ]
  },
  {
    "objectID": "2-textbook.html",
    "href": "2-textbook.html",
    "title": "TECH3 Applied statistics",
    "section": "",
    "text": "The following is a redistribution of chapters 6 of Statistical Thinking for the 21st Century by Russell A. Poldrack, 2019, under the CC BY-NC 4.0 licence.",
    "crumbs": [
      "Modules",
      "Module 2: Probability, random variables, probability distributions and simulations.",
      "Textbook"
    ]
  },
  {
    "objectID": "2-textbook.html#probability",
    "href": "2-textbook.html#probability",
    "title": "TECH3 Applied statistics",
    "section": "Probability",
    "text": "Probability",
    "crumbs": [
      "Modules",
      "Module 2: Probability, random variables, probability distributions and simulations.",
      "Textbook"
    ]
  },
  {
    "objectID": "3-bootstrap.html",
    "href": "3-bootstrap.html",
    "title": "Bootstrap",
    "section": "",
    "text": "Slides for “Bootstrap”\nSlides for “Parametric Bootstrap”\n\n\n\nWhat is the difference between sampling with and without replacement?\nWhat is a bootstrap sample?\nWhat is the purpose of the bootstrap?\nWhat does bootstraps have to do with the bootstrap method?\nWhat is the difference between bootstrap and parametric bootstrap?\nWhat are the main purposes of parametric bootstrap?",
    "crumbs": [
      "Modules",
      "Module 3: Estimation, sampling distributions and resampling",
      "Bootstrap"
    ]
  },
  {
    "objectID": "3-bootstrap.html#control-questions",
    "href": "3-bootstrap.html#control-questions",
    "title": "Bootstrap",
    "section": "",
    "text": "What is the difference between sampling with and without replacement?\nWhat is a bootstrap sample?\nWhat is the purpose of the bootstrap?\nWhat does bootstraps have to do with the bootstrap method?\nWhat is the difference between bootstrap and parametric bootstrap?\nWhat are the main purposes of parametric bootstrap?",
    "crumbs": [
      "Modules",
      "Module 3: Estimation, sampling distributions and resampling",
      "Bootstrap"
    ]
  },
  {
    "objectID": "3-exercises.html",
    "href": "3-exercises.html",
    "title": "Exercises",
    "section": "",
    "text": "Exercises\n\nProblem 1\nA university wants to study the average number of hours students sleep during finals week. They decide to gather data by surveying students.\n\nWhat is the population of this study?\nGive an example of a possible sample.\nWhy is it important to distinguish between a population and a sample?\n\n\n\nShow solutions\n\n\nThe population is all students at the university during finals week. Recall that tthe population is the whole group we want to study.\nA sample might be 150 students selected randomly from various departments. A sample cam really be **any* subset of the population, however, that does not mean that all samples are created equally. (This will be discussed more later)\nIt’s important to distinguish between a population and a sample because we usually cannot study the entire population. Instead, we use a sample to make inferences about the population. Understanding this distinction helps us evaluate the reliability and limitations of our conclusions.\n\n\n\n\nProblem 2\nA university wants to study the average number of hours students sleep during finals week. They decide to gather data by surveying students.\nSuppose the researcher only surveys students from the university’s Mathematics department. What potential issue arises from this sampling choice?\n\n\nShow solutions\n\nSurveying only Mathematics students would certainly introduce some degree of sampling bias. We can’t be certain that the sleep habits of students in the maths department would have similar habits to the average student. There could for instance be differences in workload and culture from one department to the next. Thus, we can’t safely generalise the findings, by looking only at the maths department.\n\n\nProblem 3\nSuppose a public health organization wants to estimate the average number of daily steps taken by adults in a large city.\n\nWhat are some key features the sample should have to be considered representative of the adult population.\nIn what way could one go about achieving a representative sample?\n\n\n\nShow solutions\n\n\nA representative sample needs to reflect the characteristics, proportions, and diversity of the of the entire population. I.e. there people of different ages, sexes and lifestyles should be proportionally (at least somewhat proportionally) represented in the sample, for it to be representative of the entire population.\nThe simplest way is to just select randomly from the entire population. Even though each pick is completely random, by the law of large numbers, once enough people are picked at random, the sample distribution should will approach the population’s. This isn’t always certain to work, as it mostly relies on having a sample of adequate size, as well asd needing to ensure that one is sampling form the entire population at random, and not a subset of it.\n\nOne useful method could be stratified sampling, where the population is divided into key subgroups (e.g., age groups, districts), and individuals are randomly selected from each subgroup. This ensures balanced representation across important variables.\n\n\n\nProblem 4\nLike in Problem 3, suppose a public health organization wants to estimate the average number of daily steps taken by adults in a large city. Suppose now also that some people choose not to respond. Could this possibly cause problems for the surveyors?\n\n\nShow solutions\n\nImagine if there was some significant distinction in attributes between the people that chose to reply, and of those who chose not to. I.e., those who reply and those who don’t aren’t necessarily random. This non-randomness in response could veyr well compromise the representative sample that we are looking for. Imagine that less active adults were less likely to respond than more active ones. In a scenario like this, the public health organisation might end up overestimating the average number of daily steps.\n\n\n\nProblem 5\nA new coffee shop wants to know what people think about their service. They post a feedback form on their website and collect responses for a week.\n\nIdentify one source of sampling bias in this method.\nPropose an alternative approach that would reduce sampling bias.\n\n\n\nShow solutions\n\n\nThe main bias comes from self-selection: only people who visit the website and feel strongly (either positively or negatively) are likely to respond. This could lead to results that do not reflect the general customer base.\nA better approach would be to randomly select customers in-store and ask them to complete a short survey, or to distribute surveys with receipts to all customers during the week, encouraging broad participation. This reduces self-selection bias and captures more typical opinions.\n\n\n\n\nProblem 6\nYou are conducting a study on the eating habits of university level students in Bergen. You want your results to generalize well to the full student population.\n\nSuggest a sampling method that would help ensure your sample is representative.\nWhat steps could you take to avoid overrepresenting certain types of students (e.g. from large universities or urban areas)?\nWhat issues could you run into if you only sampled students from NHH?\n\n\n\nShow solutions\n\n\nA good choice would be stratified random sampling. You could divide the college student population by school type (e.g., large universities, community colleges, private colleges), and then randomly select students from each group proportionally.\nTo avoid overrepresentation, ensure that each subgroup is sampled in proportion to its size in the overall student population. For example, if 30% of students are from smaller schools, then 30% of your sample should come from smaller schools too. You might also apply weighting to adjust for any imbalance in response rates after collecting the data.\nThough it may be more convenient for you to only sample NHH students, we are trying to consider the eating habits of the general student in Bergen. Looking only at NHH could very well lead to biased results, especially considering that NHH is far less diverse in field of study than institutions like UiB and HVL. a random sampling of all students, or a stratified sampling, would clearly lead to much more statistically rigorous results in a case liek this.\n\n\n\n\nProblem 7\nA news article reports: “80% of people in the city support the new policy,” based on a poll conducted by a political advocacy group on their own social media page.\n\nExplain why the results of this poll may be biased.\nSuggest a way to improve the sampling process to obtain more reliable public opinion data.\nSuppose a truly random sample of city residents shows only 45% support for the policy. How does this compare to the original poll, and what does it suggest?\n\n\n\nShow solutions\n\n\nThe sample is likely biased because it seems likely that very few non-followers of the party would respond to this survey, as the poll was conducted on their page. These followers may already share similar views, making them not representative of the broader city population.\nA better approach would be to conduct a randomized phone or email survey using a list of registered voters or residents. Alternatively, partnering with an independent polling organization that uses techniques like random digit dialing or address-based sampling could provide more reliable and representative data.\nThis suggests that the original 80% figure was inflated due to sampling bias. A truly random sample showing only 45% support reveals that the earlier poll was not representative of the general population. It highlights how non-random or biased sampling can lead to misleading conclusions.\n\n\n\n\nProblem 8\nSuppose you’re studying how the number of hours a student studies per week relates to their exam score. You propose the following relationship:\n\\[\n\\text{Exam Score} = 5 \\times \\text{Study Hours} + \\varepsilon\n\\]\n\nWhat is the deterministic part (non-random part) of this model?\nWhat does the error term \\(\\varepsilon\\) represent?\n\n\n\nShow solutions\n\n\nThe deterministic part is:\n\n\\[\n\\text{Exam Score} = 5 \\times \\text{Study Hours}\n\\]\nThis is the part of the model that gives a fixed prediction based only on the number of study hours. I.e. the part of the model that is not randomly determined.\n\nThe error term \\(\\varepsilon\\) accounts for everything else that affects the exam score but isn’t included in the model — such as test anxiety, sleep, prior knowledge, or randomness. It reflects unexplained variation.\n\n\n\n\nProblem 9\nYou collect data on 5 students and fit a statistical model to predict exam scores from study hours. The model predicts:\n\\[\n\\hat{y}_i = 5 \\times x_i\n\\]\nwhere \\(x_i\\) is the number of hours student \\(i\\) studied. Karl studied for 6 hours, and his final score ended up being 35.\n\nWhat was Karl’s predicted score?\nWhat is the residual for Karl?\n\n\n\nShow solutions\n\n\nSince Karl studied for 6 hours we get that \\(x_i=6\\). Now we have everything we need to compute the predicted score. The predicted score is:\n\n\\[\n\\hat{y}_i = 5 \\times 6 = 30\n\\]\n\nThe residual is the difference between the predicted score and the actual score. Recall that the predicted score we found in a) was 30 and the actual score was 35. With this we can compute the residual. The residual is:\n\n\\[\n\\text{Residual} = y_i - \\hat{y}_i = 35 - 30 = 5\n\\]\nThis means Karl did 5 points better than the model predicted.\n\n\n\nProblem 10\nA model assumes the error terms \\(\\varepsilon_1, \\varepsilon_2, \\dots, \\varepsilon_n\\) are identically distributed.\n\nWhat does it mean for the error terms to be identically distributed?\nWhy might this assumption be important when building a statistical model?\n\n\n\nShow solutions\n\n\nIdentically distributed means that each error term comes from the same probability distribution — e.g., all errors are normally distributed with the same mean and variance.\nThis assumption ensures that the variability is consistent across observations. If errors had different distributions (e.g. wider for some groups than others), then the model’s predictions could be unreliable or biased for certain parts of the data.\n\n\n\n\nProblem 11\nNow assume the model from problem 10 also requires the error terms \\(\\varepsilon_1, \\varepsilon_2, \\dots, \\varepsilon_n\\) to be independent. This means the error terms (residuals) are now independent and identically distributed (i.i.d.).\n\nWhat does it mean for error terms to be independent?\nProvide a situation where this assumption might be violated.\n\n\n\nShow solutions\n\n\nIndependent error terms mean that the value of one error term does not provide any information about another. The errors are not correlated and have no systematic pattern between them.\nA violation might occur in time series data (e.g. daily stock prices), where today’s error may be related to yesterday’s. Another example is when data points come from the same individual (e.g. repeated measures), introducing dependency between observations.\n\n\n\n\nProblem 12\nYou take a random sample of 50 students from a large university to estimate the average number of hours they sleep per night. The sample mean is 6.8 hours, while the true population mean (which you happen to know) is 7.1 hours.\n\nWhat is the sampling error in this situation?\nWhy does sampling error occur, even when using random sampling?\n\n\n\nShow solutions\n\n\nRecall that to find the sampling error we need ot subtract the population value \\(\\mu\\) form the sample value \\(\\overline{y}\\). We see here that he nsample mean is given by the average we find through our experiment \\(\\overline{y}=6.8\\) and the population value is also given \\(\\mu=7.1\\).\n\n\\[\n\\text{Sampling error} = \\overline{y} - \\mu = 6.8 - 7.1 = -0.3\n\\] I.e. our sample underestimated the true value by 0.3 hours, which is 18 minutes.\n\nSampling error occurs because we are only using a subset of the population, and that subset may not perfectly reflect the whole population due to natural variability. Even random samples will vary from sample to sample, producing different estimates.\n\n\n\n\nProblem 13\nSuppose you repeatedly take samples of size 30 from a population with a true mean of 50 and plot the means of those samples.\n\nWhat is this collection of sample means called?\nAs you increase the number of samples, what does the shape of this distribution tend to look like?\n\n\n\nShow solutions\n\n\nThe collection of sample means is called the sampling distribution of the sample mean.\nAccording to the Central Limit Theorem, the sampling distribution will tend to become approximately normal, even if the original population is not normally distributed — provided the sample size is large enough.\n\nHave a look at the figures below to see an example of how a sample distribution may look and develop as an experiment is repeated. Note how teh figures approach a bell curve.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Set seed for reproducibility\nnp.random.seed(42)\n\n# Population: non-normal (exponential) to highlight the CLT\npopulation = np.random.exponential(scale=1.0, size=100000)\nn = 30  # Fixed sample size\n\n# Function to simulate means\ndef sample_means(num_samples):\n    return [np.mean(np.random.choice(population, n, replace=True)) for _ in range(num_samples)]\n\n# Simulate with increasing number of samples\nmeans_10 = sample_means(10)\nmeans_100 = sample_means(100)\nmeans_1000 = sample_means(1000)\n\n# Plot\nfig, axs = plt.subplots(1, 3, figsize=(15, 4))\ncolors = ['skyblue', 'orange', 'seagreen']\nsamples = [means_10, means_100, means_1000]\ntitles = [\"10 Samples\", \"100 Samples\", \"1000 Samples\"]\n\nfor ax, data, color, title in zip(axs, samples, colors, titles):\n    ax.hist(data, bins=10 if title == \"10 Samples\" else 20 if title == \"100 Samples\" else 30,\n            color=color, edgecolor='black')\n    ax.set_xlim(0, 3)\n    ax.set_title(title)\n    ax.set_xlabel(\"Sample Mean\")\n    ax.set_ylabel(\"Frequency\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nProblem 14\nYou are estimating the average test score for a population. The population has a standard deviation of \\(\\sigma = 10\\).\n\nWhat is the standard deviation of the sample mean (also called the standard error) if you take a sample of size \\(n = 25\\)?\nWhat happens to the standard error if you increase the sample size to \\(n = 100\\)?\n\n\n\nShow solutions\n\n\nRecall that so find the standard deviation of the sample mean we use the formula \\[SD\\left(\\overline{Y}_n \\right)=\\frac{\\sigma}{\\sqrt{n}} \\] Since we already know \\(n=25\\) and \\(\\sigma=10\\) we can easily compute the standard error. The standard error is given by:\n\n\\[\nSD\\left(\\overline{Y}_n \\right) = \\frac{\\sigma}{\\sqrt{n}} = \\frac{10}{\\sqrt{25}} = \\frac{10}{5} = 2\n\\]\n\nIf \\(n = 100\\):\n\n\\[\n\\text{SE} = \\frac{10}{\\sqrt{100}} = \\frac{10}{10} = 1\n\\]\nSo, increasing the sample size reduces the standard error, meaning the sample mean becomes more precise.\n\n\n\nProblem 15\nImagine you take 1,000 different random samples, each of size 40, from the same population. You compute the mean for each sample and plot a histogram.\n\nWhat does the center of the sampling distribution represent?\nHow does the spread of this histogram relate to sample size?\n\n\n\nShow solutions\n\n\nThe center of the sampling distribution represents the population mean (or it will at least be very close to the population mean). On average, the sample means will be centered around the true population mean.\nThe spread of the sampling distribution — measured by the standard error — decreases as the sample size increases. This means larger samples lead to less variability in the sample mean.I.e., we will see the the shape of the distribution close in on the population mean as the the sample size incereases.\n\nTake a look at the representation below to get an idea. The red line represents the population mean, and as the sample size increases the distribution of sample means becomes tighter around it, i.e. more observations are relatively close to the true population mean when n increases. This is the idea when we compute the standard error and notice that it decreases in n. this is also an effective illustration of the central limit theorem and the law of large numbers.\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Set seed for reproducibility\nnp.random.seed(123)\n\n# Skewed population: Exponential\npopulation = np.random.exponential(scale=1.0, size=100000)\nmu_true = np.mean(population)\n\n# Function to generate sampling distributions\ndef simulate_means_by_n(n, reps=1000):\n    sample_means = [np.mean(np.random.choice(population, n, replace=True)) for _ in range(reps)]\n    return pd.DataFrame({\n        'sample_mean': sample_means,\n        'sample_size': f'n = {n}'\n    })\n\n# Generate data\ndf = pd.concat([\n    simulate_means_by_n(5),\n    simulate_means_by_n(30),\n    simulate_means_by_n(100)\n])\n\n# Plot sampling distributions\nsns.set(style=\"whitegrid\", font_scale=1.2);\ng = sns.FacetGrid(df, col=\"sample_size\", sharey=False, height=4, aspect=1.2);\ng.map(sns.histplot, \"sample_mean\", kde=True, stat=\"density\", bins=30, color=\"lightblue\", edgecolor=\"white\");\n\n# Add vertical line for population mean\nfor ax in g.axes.flat:\n    ax.axvline(mu_true, color=\"red\", linestyle=\"--\", linewidth=1);\n\ng.set_axis_labels(\"Sample Mean\", \"Density\");\ng.fig.suptitle(\"Effect of Sample Size on the Sampling Distribution of the Mean\", y=1.05);\n\nplt.tight_layout();\nplt.show();\n\n\n\n\n\n\n\n\n\n\n\nProblem 16\nYou simulate the following experiment using software: (You don’t need to do this in Python yourself, but it could be a neat exercise to see it for yourself fully.)\n\nDraw 1,000 random samples of size \\(n = 30\\) from a population with mean \\(\\mu = 100\\) and standard deviation \\(\\sigma = 15\\).\nFor each sample, compute the sample mean and store it.\nPlot the histogram of all 1,000 sample means.\n\n\nWhat shape do you expect the histogram of sample means to have?\nApproximately where should the center of the histogram be?\nIf you were to repeat the experiment with a larger sample size \\(n = 100\\), what change would you expect in the spread of the histogram?\n\n\n\nShow solutions\n\n\nThe histogram should be approximately normal (bell-shaped) due to the Central Limit Theorem, even if the original population is not normal.\nThe center should be at or very near the population mean, which is \\(\\mu = 100\\).\nThe spread — measured by the standard error — would decrease. A larger sample size makes the sample means more concentrated around the population mean. Mathematically: (\\(SE\\) is an equivalent notation for the standard error)\n\n\\[\n\\text{SE}_{30} = \\frac{15}{\\sqrt{30}} \\approx 2.74, \\quad \\text{SE}_{100} = \\frac{15}{10} = 1.5\n\\]\n\n\n\nProblem 17\nA researcher wants to estimate the average amount of money undergraduate students at a large university spend on food per week. She:\n\nRandomly selects 40 students and records their weekly food spending.\nComputes the sample mean: \\(\\mu=\\$52.80\\).\nKnows from prior studies that the population standard deviation is approximately \\(\\sigma=\\$12\\).\n\n\n\nWhat is the sampling error if the actual population mean is $50?\nWhat is the standard error of the sample mean?\nExplain what would happen to the standard error if she used a sample of 160 students instead.\nIs this one sample mean likely to equal the population mean exactly? Why or why not?\n\n\n\nShow solutions\n\n\nWe have a sample mean \\(\\overline{y}=52.80\\), so then we should just subtract the population mean to compute the sampling error.\n\n\\[\n\\text{Sampling error} = 52.80 - 50 = 2.80\n\\]\n\nWe can easily compute this as we already know the population standard deviation \\(\\sigma=12\\), and the sample size \\(n=40\\).\n\n\\[\n\\text{SE} = \\frac{12}{\\sqrt{40}} \\approx \\frac{12}{6.32} \\approx 1.90\n\\]\n\nIf \\(n = 160\\):\n\n\\[\n\\text{SE} = \\frac{12}{\\sqrt{160}} \\approx \\frac{12}{12.65} \\approx 0.95\n\\]\nSo the standard error would shrink, improving precision.\n\nIt is unlikely that the sample mean equals the population mean exactly. Due to sampling variability, different random samples will produce different estimates, though they should cluster around the true mean if the sample is representative. It will also be **likelier* that there is less of an absolute difference between the sample mean and population mean as the sample size increases, as this will reduce standard error. As illustrated in c), this effect improves precision.\n\n\n\n\nProblem 18\nYou’re given a population distribution with unknown form. However, you’re told the population has finite mean \\(\\mu\\) and variance \\(\\sigma^2\\). You draw a simple random sample of size \\(n\\), and compute the sample mean \\(\\overline{X}_n\\).\n\nAccording to the Central Limit Theorem (CLT), what is the asymptotic distribution of \\(\\overline{X}_n\\) as \\(n \\to \\infty\\)?\nWhat does the CLT not guarantee when \\(n\\) is small?\nIllustrate the two following results given the sample of size \\(n\\) given in this problem.\n\\[E\\left[\\overline{X}_n\\right]=\\mu\\]\n\n\n\n\\[ Var \\left[\\overline{X}_n \\right]=\\frac{\\sigma^2}{n} \\]\n\n\n\nShow solutions\n\n\nThe CLT tells us that a that a sequence of independent and identically distributed (i.i.d) random variables \\(X_1,\\dots, X_n\\), with a an expected value \\(\\mu\\) and a finite variance \\(\\sigma^2\\) will have their average converge to a normal distribution, when \\(n\\) approaches infinity (\\(n\\rightarrow \\infty\\)). The average of these \\(n\\) variables we give as normal\n\n\\[\\overline{X}_n=\\frac{1}{n}\\sum^n_{i=1}X_i \\] The normal distribution is decided by two parameters; mean and variance. So to find exactly which distribution the CLT has \\(\\overline{X}_n\\) converge to we need the mean and variance. The rule from the lectures tells us that the mean of this normal distribution will remain the same as for \\(X_i\\), i.e. \\(\\mu\\) and that the variance will converge to \\(\\frac{\\sigma^2}{n}\\)\nIn short the CLT then says:\n\\[\n\\bar{X}_n \\xrightarrow{d} \\mathcal{N}\\left(\\mu, \\frac{\\sigma^2}{n}\\right)\n\\]\n\nWhen \\(n\\) is small, the sample mean may not be approximately normally distributed. The CLT is an asymptotic result, so the normal approximation improves with larger \\(n\\). If the population distribution is skewed or heavy-tailed, then the distribution of \\(\\overline{X}_n\\) is typically farther from the asymptotic normal distribution.\nThe first thing we need to do for both problems is to recall how we’ve defined \\[\\overline{X}_n\\]. By substituting and doing some algebra we will arrive at our desired results.\n\\[ E\\left[\\overline{X}_n\\right]=E\\left[\\frac{1}{n}\\sum^n_{i=1}X_i\\right] \\]\n\nFor expected values we are allowed to move constants such as \\(\\frac{1}{n}\\) outside of our brackets. We also know that the sum of an expectation is the expectation of teh sum, meaning that we can move the sum outside of our brackets as well.\n\\[ E\\left[\\frac{1}{n}\\sum^n_{i=1}X_i\\right]=\\frac{1}{n}\\sum^n_{i=1}E \\left[X_i \\right] \\] Now we can take advantage of all \\(X_i\\) being i.i.d., meaning they are identically distirbuted and will thus have the same mean, namely \\(\\mu\\).\n\\[ \\frac{1}{n}\\sum^n_{i=1}E \\left[X_i \\right]=\\frac{1}{n}\\sum^n_{i=1}\\mu=\\frac{n}{n}\\mu=\\mu\\] I.e.\n\\[E\\left[\\overline{X}_n\\right]=\\mu\\] ii) We essentially repeat the process here.\n\\[ Var \\left[\\overline{X}_n \\right] = Var \\left[\\frac{1}{n}\\sum^n_{i=1}X_i \\right] \\] For variances we are allowed to move constants such as \\(\\frac{1}{n}\\) outside of our brackets if we square them. I.e.\n\\[Var[aY]=a^2Var[Y] \\] (For those not entirely convinced by this, try to compute it bye using that \\(Var[Y]=E[Y^2]-(E[X])^2\\))\nWe also have that the sum of a variance is the variance of the sum as long as the random variables included in the sum are mutually independent of each other. In this case all our variables are i.i.d., meaning that the independence requirement is met; and thus we can move the sum outside of the brackets as well. We then get:\n\\[ Var \\left[\\frac{1}{n}\\sum^n_{i=1}X_i \\right] = \\frac{1}{n^2}\\sum^n_{i=1}Var[X_i] \\]\nAll of our \\(X_i\\) are identically distributed with the same variance; \\(\\sigma^2\\):\n\\[ \\frac{1}{n^2}\\sum^n_{i=1}Var[X_i] = \\frac{1}{n^2}\\sum^n_{i=1}\\sigma^2=\\frac{n}{n^2}\\sigma^2=\\frac{\\sigma^2}{n} \\] And we are done\n\\[ Var \\left[\\overline{X}_n \\right]=\\frac{\\sigma^2}{n} \\]\n\n\n\nProblem 19\nYou want to apply the Central Limit Theorem to a dataset of measurements collected from a machine. However, before proceeding, your colleague reminds you that “the CLT has assumptions!”\n\nList the assumptions required for the CLT to hold for sample means.\nSuppose the machine’s output is not independent from one sample to the next. Can you still apply the CLT?\n\n\n\nShow solutions\n\n\nThe main assumptions for the classical CLT are:\n\n\nThe data are independent\nThe data are identically distributed\nThe population has finite mean and finite variance\n\n\nIf the data are not independent (e.g., time-dependent or autocorrelated), then the classical CLT may not apply. There are generalized versions of the CLT for dependent data, but in this setting, using the standard version would lead to misleading conclusions about the sampling distribution.\n\n\n\n\nProblem 20\nYou simulate 10,000 observations from each of the following distributions:\n\nExponential(1)\nUniform(0, 1)\nBernoulli(0.2)\n\nFor each distribution, you compute the sample mean of many repeated samples of size \\(n = 50\\). What assumptions would you have to make in this case to be certain?\n\nFor each distribution above, is the CLT applicable? Why?\nWhat would you expect the distribution of the sample means to look like?\n\n\n\nShow solutions\n\n\nOur requirements to use CLT are:\n\n\nThe distributions have finite mean and variance\nWe have a random sample of fixed size\nThe samples are independent and identically distributed\n\nIn all cases we have that the distributions have finite means and variances. Since we’re simulating from the same distributions, we can be certain they are identical, but we still assume independence (using a random draw with Python for example this requirement is met). It’s also clear that we have random samples of a fixed size in this case as our simulation is takes the mean of assorted samples of 50.\n\nIn all three cases, the distribution of sample means should be approximately normal, centered at the true population mean, with smaller spread as \\(n\\) increases. The exponential is heavily skewed, so its sample mean distribution may look less normal than the others, but should still be roughly bell-shaped.\n\n\n\n\nProblem 21\nLet \\(X_1, \\dots, X_n\\) be i.i.d. random variables from a distribution with mean \\(\\mu = 3\\) and variance \\(\\sigma^2 = 4\\). You take a random sample of size \\(n = 100\\).\n\nUse the CLT to approximate the distribution of the sample mean \\(\\overline{X}\\).\nUse the CLT to approximate the distribution of the sum \\(S_n = \\sum_{i=1}^n X_i\\).\n\nHint: \\(S_n=n\\overline{X}\\) which can be used for computation\n\n\nShow solutions\n\n\nBy CLT:\n\n\\[\n\\overline{X} \\sim \\mathcal{N}\\left(3, \\frac{4}{100}\\right) = \\mathcal{N}(3, 0.04)\n\\]\n\nSince \\(S_n = n\\overline{X}\\), and \\(\\overline{X} \\sim \\mathcal{N}(3, 0.04)\\), then:\n\n\\[\nS_n \\sim \\mathcal{N}(300, 400)\n\\] \\[E[S_n]=\\cdots=\\frac{n}{n}\\sum^n_{i=1}\\mu=100\\mu=300 \\\\\nVar[S_n]=\\cdots=\\frac{n^2}{n^2}\\sum^n_{i=1}\\sigma=100\\sigma=400 \\]\nFor those wanting an extra challenge, try finding a general rule for how the mean and variance of a normal distribution of a sum such this would scale by multiplying with a constant \\(a\\)\n\n\n\nProblem 22\nYou are studying the average number of daily steps taken by students at a university.\n\nYou collect a random sample of 100 students and calculate the mean number of steps. What is this quantity called?\nBefore collecting data, you write down the formula you plan to use to estimate the mean from any future sample. What is this formula called?\nWhat is the term for the true, but unknown, average number of steps taken by all students at the university (for instance \\(\\theta\\))?\n\n\n\nShow solutions\n\n\nThis is an estimate — a realized numerical value from a specific sample.\nThis is an estimator — a rule or formula for calculating an estimate.\nThis is the parameter — the true population quantity we aim to estimate.\n\n\n\n\nProblem 23\nLet \\(\\hat{\\theta}\\) be an estimator for a population parameter \\(\\theta\\).\n\nDefine what it means for \\(\\hat{\\theta}\\) to be an unbiased estimator of \\(\\theta\\).\nDefine what it means for \\(\\hat{\\theta}\\) to be a consistent estimator of \\(\\theta\\).\nAre these two properties related? Can an estimator be one but not the other?\n\n\n\nShow solutions\n\n\nAn estimator \\(\\hat{\\theta}\\) is unbiased if \\(E[\\hat{\\theta}] = \\theta\\).\nAn estimator is consistent if \\(\\hat{\\theta} \\to \\theta\\) in probability as the sample size \\(n \\to \\infty\\). I.e. as the sample size \\(n\\) increases, the estimator \\(\\hat{\\theta}\\) will approach the true paramter value \\(\\theta\\).\nYes, they are distinct properties. An estimator can be:\n\n\nUnbiased but not consistent (e.g., large variance that doesn’t shrink with \\(n\\)).\nBiased but consistent (e.g., estimators that converge with increasing \\(n\\), even if not centered).\n\n\n\n\nProblem 24\nLet \\(X_1, \\dots, X_n\\) be iid with man \\(\\mu\\) and variance \\(\\sigma^2\\). Consider the sample variance:\n\\[\nS_n^2 = \\frac{1}{n} \\sum_{i=1}^n (X_i - \\bar{X})^2\n\\]\n\nIs \\(S_n^2\\) an unbiased estimator of \\(\\sigma^2\\)? Explain.\nSuggest an adjusted estimator that is unbiased, and write the formula.\nGiven that \\[E\\left[\\sum^n_{i=1}(X_i-\\overline{X})^2\\right]=(n-1)\\sigma^2 \\] Use this to show that the sample variance \\(S^2\\) is an unbiased estimator for \\(\\sigma^2\\).\n\nWhen\n\\[S^2=\\frac{1}{n-1}\\sum^n_{i=1}(X_i-\\overline{X})^2\\]\n\n\nShow solutions\n\n\nNo, \\(S_n^2\\) is biased. Its expected value is slightly less than \\(\\sigma^2\\), especially for small \\(n\\). This is due to the fact that \\(\\bar{X}\\) is estimated from the same data and introduces extra variability.\nThe unbiased estimator of variance is:\n\n\\[\nS^2 = \\frac{1}{n - 1} \\sum_{i=1}^n (X_i - \\overline{X})^2\n\\] c)\nUsing the provided identity:\n\\[\nE[S^2] = E\\left[\\frac{1}{n-1} \\sum_{i=1}^n (X_i - \\overline{X})^2\\right] = \\frac{1}{n-1} \\cdot (n - 1)\\sigma^2 = \\sigma^2\n\\]\nSo \\(S^2\\) is unbiased.\n\n\n\nProblem 25\nSuppose you are comparing two estimators of a parameter \\(\\theta\\):\n\nEstimator A is unbiased, but has high variance.\nEstimator B is biased, but the bias gets smaller as the sample size grows, and the variance is low.\n\n\nWhich estimator would you prefer if you care about consistency?\nWhich estimator would you prefer if you care most about low error in small samples?\nCan Estimator B be consistent, even though it is biased?\n\n\n\nShow solutions\n\n\nEstimator B is preferable if it is consistent, because consistency ensures convergence to the true value as \\(n \\to \\infty\\).\nIn small samples, Estimator A may perform poorly due to high variance. Estimator B may be preferred due to lower overall error.\nYes — bias and consistency are not mutually exclusive. Estimator B can be biased but consistent if the bias vanishes as \\(n \\to \\infty\\).\n\n\n\n\nProblem 26\nConsider the properties of the sample mean \\(\\overline{X}\\) and sample variance \\(S^2\\) as estimators.\n\nAre both unbiased? Under what conditions?\nAre both consistent?\nWhat happens to these properties when data are not iid?\n\n\n\nShow solutions\n\n\n\\(\\overline{X}\\) is always unbiased for \\(\\mu\\) if the mean exists. \\(S^2\\) is unbiased for \\(\\sigma^2\\) if the data are iid and \\(\\sigma^2\\) exists.\nBoth are consistent if the data are iid and the mean and variance exist (mean for \\(\\overline{X}\\), variance for \\(S^2\\)).\nIf data are not iid:\n\n\nBias may be introduced (e.g., if dependent observations).\nConsistency may fail if dependence inflates variance or reduces effective sample size.\n\n\n\n\nProblem 27\nSuppose you observe independent data points \\(X_1, X_2, \\dots, X_n\\) drawn from a distribution with a density \\(f(x; \\theta)\\), where \\(\\theta\\) is an unknown parameter.\n\nDefine the likelihood function \\(L(\\theta)\\) based on this data.\nExplain why, when maximizing the likelihood, we often instead maximize the log-likelihood.\n\n\n\nShow solutions\n\n\nThe likelihood is the joint probability (or density) of the observed data, viewed as a function of \\(\\theta\\):\n\n\\[\nL(\\theta) = \\prod_{i=1}^n f(X_i; \\theta)\n\\]\n\nWe often maximize the log-likelihood because:\n\n\nIt turns a product into a sum: \\(\\log L(\\theta) = \\sum_{i=1}^n \\log f(X_i; \\theta)\\)\nIt’s easier to differentiate and solve\nIt avoids numerical issues caused by multiplying many small probabilities\n\n\n\n\nProblem 28\nLet \\(X_1, \\dots, X_n \\sim \\text{iid Geometric}(p)\\), where \\(p \\in (0,1)\\) is the probability of success. The pmf is:\n\\[\nP(X = k) = (1 - p)^{k - 1} p, \\quad k = 1, 2, 3, \\dots\n\\]\n\nWrite the log-likelihood function for the observed sample.\nDerive the maximum likelihood estimator \\(\\hat{p}\\).\n\n\n\nShow solutions\n\n\n\n\n\\[\nL(p) = \\prod_{i=1}^n (1 - p)^{X_i - 1} p = (1 - p)^{\\sum (X_i - 1)} p^n\n\\]\n\\[\n\\Rightarrow \\ell(p) = \\log L(p) = (n \\log p) + \\left( \\sum^n_{i=1} (X_i - 1) \\right) \\log(1 - p)\n= n \\log p + (n \\bar{X} - n) \\log(1 - p)\n\\]\n\nDifferentiate:\n\n\\[\n\\frac{d\\ell}{dp} = \\frac{n}{p} - \\frac{n \\bar{X} - n}{1 - p}\n\\Rightarrow \\frac{1}{p} = \\frac{\\bar{X} - 1}{1 - p}\n\\Rightarrow \\hat{p} = \\frac{1}{\\bar{X}}\n\\]\nSo, the MLE is \\(\\hat{p} = \\dfrac{1}{\\overline{X}}\\).\n\n\n\nProblem 29\nSuppose you observe \\(X_1, \\dots, X_n \\sim f(x; \\theta)\\), where \\(f(x; \\theta)\\) is a known family of distributions.\n\nWhy must the data be for the data to be iid when using MLE?\nWhat could go wrong if the data are not iid?\nSuppose the true distribution of the data is not in the family \\(f(x; \\theta)\\). What consequences might this have?\n\n\n\nShow solutions\n\n\nIf the data are iid:\n\n\nThe joint probability factorizes nicely as it will be a product of identical identities\nThe likelihood simplifies\nThe MLE behaves well and has known properties (e.g. consistency)\n\n\nIf data are not iid:\n\n\nThe likelihood expression is invalid or more complicated\nThe MLE may be biased or inconsistent\nInference based on the MLE may be misleading\n\n\nIf the model is wrong (i.e., the true distribution isn’t in the family):\n\n\nThe MLE still finds the “best-fitting” parameter within the family, but it’s not estimating the true parameter\nThis is called model misspecification\nThe MLE may converge to a value that minimizes some distance, but it’s not consistent for the true data-generating parameter\n\n\n\n\nProblem 30\nLet \\(U \\sim \\text{Uniform}(0,1)\\). You are told that it is possible to generate a random variable \\(X\\) from another distribution by transforming \\(U\\). This technique is called inverse transform sampling.\nSuppose you want to generate random variables from an exponential distribution with parameter \\(\\lambda &gt; 0\\), which has CDF\n\\[\nF(x) = 1 - e^{-\\lambda x}, \\quad x \\ge 0\n\\]\n\nSolve for the inverse of the CDF, \\(F^{-1}(u)\\), where \\(u \\in (0,1)\\).\nLet \\(U \\sim \\text{Uniform}(0,1)\\). Show that if we define \\(X = F^{-1}(U)\\), then \\(X \\sim \\text{Exp}(\\lambda)\\).\nWhat general steps does this illustrate about how inverse transform sampling works?\n\n\n\nShow solutions\n\n\nStart with:\n\n\\[\nF(x) = 1 - e^{-\\lambda x}\n\\Rightarrow u = 1 - e^{-\\lambda x}\n\\Rightarrow e^{-\\lambda x} = 1 - u\n\\Rightarrow -\\lambda x = \\log(1 - u)\n\\Rightarrow x = -\\frac{1}{\\lambda} \\log(1 - u)\n\\]\nSince \\(1 - u \\sim \\text{Uniform}(0,1)\\) as well, we often write:\n\\[\nF^{-1}(u) = -\\frac{1}{\\lambda} \\log u\n\\]\n\nLet \\(U \\sim \\text{Uniform}(0,1)\\) and define \\(X = -\\frac{1}{\\lambda} \\log U\\). We want to show that \\(X \\sim \\text{Exp}(\\lambda)\\). Compute the CDF:\n\n\\[\nP(X \\le x) = P\\left(-\\frac{1}{\\lambda} \\log U \\le x\\right) = P\\left(\\log U \\ge -\\lambda x\\right)\n= P\\left(U \\ge e^{-\\lambda x}\\right)\n= 1 - P(U &lt; e^{-\\lambda x}) = 1 - e^{-\\lambda x}\n\\] For the last step you must recall the CDF of a uniform distribution.\\[U\\sim \\text{Uniform(a,b)}\\]. In our case we have \\(a=0\\) and \\(b=1\\).\n\\[F_U(u)=P(U\\leq u)=\\frac{u}{b-a}=\\frac{u}{1}=u\\]\nSo the CDF of \\(X\\) matches the exponential distribution. Hence, \\(X \\sim \\text{Exp}(\\lambda)\\).\n\nThe inverse transform sampling procedure works as follows:\n\n\n\nStart with a known distribution with invertible CDF, \\(F\\)\nSample \\(U \\sim \\text{Uniform}(0,1)\\)\nSet \\(X = F^{-1}(U)\\)\n\nThen \\(X\\) has the distribution defined by \\(F\\). This method is particularly useful when direct sampling is hard, but the inverse CDF is known or easy to compute.\n\n\n\nProblem 31\nIn this exercise, you will implement inverse transform sampling to generate samples from an exponential distribution with rate \\(\\lambda = 2\\), and compare the result to samples generated by R’s built-in exponential sampler. (Feel free to use Python or any other language to do the same)\n\nUse the inverse CDF method to generate 10,000 samples from an \\(\\text{Exp}(\\lambda = 2)\\) distribution in R. Recall the inverse CDF is\n\n\\[\nX = -\\frac{1}{\\lambda} \\log(U)\n\\]\n\nUse R’s built-in function rexp(n, rate = 2) to generate another 10,000 samples from the same distribution.\nCreate a histogram of both sample sets on the same scale. Overlay the theoretical exponential density curve for comparison.\nComment on the similarity between the two sample sets and the theoretical distribution. What does this illustrate about inverse transform sampling?\n\n\n\nShow solutions\n\n\nGenerate using inverse CDF:\n\n\nimport numpy as np\n# Set seed for reproducibility\nnp.random.seed(123)\nn = 10000\nlambda_ = 2\n# Generate uniform random numbers\nu = np.random.uniform(0, 1, n)\n# Inverse transform sampling\nx_inv = -np.log(u) / lambda_\n\n\nGenerate using built-in function:\n\n\nx_builtin = np.random.exponential(scale=1/lambda_, size=n)\n\n\nPlot histo}grams with density curve:\n\n\n# Plot histogram for inverse transform samples\nplt.hist(x_inv, bins=50, density=True, color=(0, 0, 1, 0.5), range=(0, 4),\n         label='Inverse CDF', edgecolor='black');\n\n# Overlay histogram for built-in samples\nplt.hist(x_builtin, bins=50, density=True, color=(1, 0, 0, 0.5), range=(0, 4),\n         label='Built-in', edgecolor='black');\n\n# Theoretical density curve\nx_vals = np.linspace(0, 4, 500)\ny_vals = lambda_ * np.exp(-lambda_ * x_vals)\nplt.plot(x_vals, y_vals, color='black', linewidth=2, label='Theoretical Density');\n\n# Labels and legend\nplt.title(\"Inverse Transform vs Built-in Sampling\");\nplt.xlabel(\"x\");\nplt.ylabel(\"Density\");\nplt.ylim(0, 2);\nplt.legend(loc='upper right');\nplt.tight_layout();\nplt.show();\n\n\n\n\n\n\n\n\n\nThe histograms of both sample sets are visually indistinguishable and both closely match the theoretical density curve of the exponential distribution. This illustrates that inverse transform sampling correctly reproduces the distribution, even when sampling is done indirectly via uniform draws.\n\n\n\n\nProblem 32\nMonte Carlo simulation is a technique used to approximate mathematical quantities through repeated random sampling.\n\nWhat is the purpose of Monte Carlo simulation in statistics and data analysis?\nHow does the Law of Large Numbers justify the use of Monte Carlo methods?\n\n\n\nShow solutions\n\n\nMonte Carlo simulation is used to estimate expectations, probabilities, or other quantities that are hard (or impossible) to compute analytically. By simulating many random samples and averaging the results, we approximate the desired value.\nThe Law of Large Numbers states that the sample average converges to the expected value as the number of samples increases. In Monte Carlo methods, this justifies approximating:\n\n\\[\nE[f(X)] \\approx \\frac{1}{n} \\sum_{i=1}^n f(X_i)\n\\]\nwhere \\(X_i \\sim \\text{Distribution of } X\\).\n\n\n\nProblem 33\nLet \\(X \\sim \\text{Uniform}(0, 1)\\). You are interested in estimating \\(E[X^2]\\) using Monte Carlo simulation.\n\nGenerate three values: \\(X_1 = 0.2, X_2 = 0.6, X_3 = 0.9\\). Use these to compute a Monte Carlo estimate of \\(E[X^2]\\).\nCompute the exact expected value of \\(X^2\\) when \\(X \\sim \\text{Uniform}(0, 1)\\), and compare.\n\n\n\nShow solutions\n\n\nEstimate:\n\n\\[\nE[X^2] \\approx \\frac{1}{3}(0.2^2 + 0.6^2 + 0.9^2) = \\frac{1}{3}(0.04 + 0.36 + 0.81) = \\frac{1.21}{3} \\approx 0.403\n\\]\n\nTo compute \\(E[X^2]\\) exactly, recall that for a continuous distribution like \\(\\text{Uniform}(0,1)\\), we define \\(E[g(X)]\\), where \\(g(X)\\) is some function of the random variable \\(X\\), as\n\n\\[ \\int^\\infty_{-\\infty}g(x)f(x)dx \\] Where \\(f(x)\\) is the probility density function of the random variable \\(X\\).\nFor our distribution in this case \\(g(x)=x^2\\), and since it’s uniform on the support \\((0,1)\\), the pdf if given by:\n\\[f(x)= \\begin{cases}\n  1, \\quad for \\ x\\in(0,1) \\\\\n  0, \\quad other  \n\\end{cases} \\]\nNow we can use this to compute the exact value. Exact value:\n\\[\nE[X^2] = \\int_0^1 x^2 dx =\\frac{1}{3}\\left[x^3\\right]^1_0= \\frac{1}{3} \\approx 0.333\n\\]\nThe estimate \\(0.403\\) is fairly close given the very small sample size.\n\n\n\nProblem 34\nYou want to estimate the area under the curve \\(y = \\sqrt{1 - x^2}\\) for \\(x \\in [0,1]\\), which corresponds to a quarter of a unit circle. This will allow you to approximate \\(\\pi\\).\n\nExplain how you can estimate this area using Monte Carlo simulation.\nWithout using a computer, simulate with the following 5 random pairs:\n\n\\[\n(x, y) \\in \\{ (0.1, 0.5), (0.4, 0.2), (0.6, 0.9), (0.3, 0.7), (0.9, 0.1) \\}\n\\]\nUse these to estimate the area.\n\nWhat is the true area? How good is the estimate?\n\n\n\nShow solutions\n\n\nWe sample points \\((x, y)\\) uniformly in the unit square \\([0,1] \\times [0,1]\\), and count how many fall below the curve. The proportion of points under the curve estimates the area.\nWe check whether each point satisfies \\(y \\le \\sqrt{1 - x^2}\\), as only those will fall withing the quarter of the unit circle:\n\n\n\\((0.1, 0.5)\\): \\(\\sqrt{1 - 0.01} = \\sqrt{0.99} \\approx 0.995 &gt; 0.5\\): ✅\n\\((0.4, 0.2)\\): \\(\\sqrt{1 - 0.16} = \\sqrt{0.84} \\approx 0.916 &gt; 0.2\\): ✅\n\\((0.6, 0.9)\\): \\(\\sqrt{1 - 0.36} = \\sqrt{0.64} = 0.8 &lt; 0.9\\): ❌\n\\((0.3, 0.7)\\): \\(\\sqrt{1 - 0.09} = \\sqrt{0.91} \\approx 0.954 &gt; 0.7\\): ✅\n\\((0.9, 0.1)\\): \\(\\sqrt{1 - 0.81} = \\sqrt{0.19} \\approx 0.436 &gt; 0.1\\): ✅\n\nSo, 4 out of 5 points were below the curve:\n\\[\n\\text{Estimated area} = \\frac{4}{5} = 0.8\n\\] We know the area of the unit circle is \\(\\pi\\), so the quarter we’ve estimated now should be a fourth of \\(\\pi\\). As such we get:\n\\[\\text{Estimated } \\pi = 4 \\times 0.8 = 3.2\\]\n\nThe true area is \\(\\frac{\\pi}{4} \\Rightarrow \\pi \\approx 3.14\\). Our estimate of 3.2 is quite reasonable for only 5 points!\n\n\n\n\nProblem 35\nUse Monte Carlo simulation to estimate the integral:\n\\[\n\\int_0^1 \\frac{1}{1 + x^2} dx\n\\]\nThis integral equals \\(\\arctan(1) = \\frac{\\pi}{4}\\). You should use Python to approximate this value.\n\nWrite python code to generate 100,000 samples \\(X_i \\sim \\text{Uniform}(0, 1)\\), and compute \\(\\frac{1}{100{,}000} \\sum \\frac{1}{1 + X_i^2}\\).\nCompare your result to the true value \\(\\frac{\\pi}{4}\\).\nHow would increasing the number of samples affect the estimate?\n\n\n\nShow solutions\n\n\nPython code:\n\n\nimport numpy as np\n# Set seed for reproducibility\nnp.random.seed(123)\nn = 100000\nx = np.random.uniform(0, 1, n)\n# Monte Carlo estimate\nestimate = np.mean(1 / (1 + x**2))\nprint(estimate)\n\n0.7852382622958511\n\n\n\n\n\n\ntrue_value = np.pi / 4\nabs_error = abs(estimate - true_value)\nprint(true_value, abs_error)\n\n0.7853981633974483 0.00015990110159713744\n\n\nOutput is typically very close, e.g., around 0.7854.\n\nAs the number of samples increases, the estimate becomes more accurate and more stable, thanks to the Law of Large Numbers. The standard error of the mean decreases like \\(1/\\sqrt{n}\\).\n\n\n\n\nProblem 36\nSuppose you observe the following dataset of 5 values:\n\\[\n\\{4.1,\\ 5.3,\\ 2.8,\\ 6.0,\\ 3.9\\}\n\\]\n\nWhat is meant by a bootstrap sample? How is it created?\nGenerate two bootstrap samples from the dataset above by sampling with replacement.\nWhy is it important that bootstrap sampling is done with replacement?\n\n\n\nShow solutions\n\n\nA bootstrap sample is a new sample of the same size drawn with replacement from the original data. It mimics the idea of repeated sampling from the population by reusing the observed data.\nExample bootstrap samples:\n\n\nSample 1: {5.3, 2.8, 4.1, 4.1, 3.9}\nSample 2: {6.0, 3.9, 5.3, 6.0, 2.8}\n\n(Answers may vary depending on random sampling.)\n\nSampling with replacement is crucial in bootstrap methods because we only have one sample from the population. Replacement introduces variability and simulates the act of drawing new samples from the population.\n\n\n\n\nProblem 37\nLet \\(\\hat{\\theta} = \\overline{X}\\) be the estimator of the mean from the original sample:\n\\[\n\\{2.0,\\ 4.0,\\ 6.0,\\ 8.0\\}\n\\]\nSuppose you compute the means of three bootstrap samples (each of size 4 with replacement):\n\nSample 1 mean: 4.5\nSample 2 mean: 5.0\nSample 3 mean: 3.5\n\n\nWhat is the original estimate \\(\\hat{\\theta}\\)?\nWhat is the bootstrap estimate of the bias?\nWhat is the bias-corrected estimate?\n\n\n\nShow solutions\n\n\nOriginal estimate:\n\n\\[\n\\hat{\\theta} = \\frac{2 + 4 + 6 + 8}{4} = 5\n\\]\n\nBootstrap mean estimate:\n\n\\[\n\\hat{\\theta}^* = \\frac{4.5 + 5.0 + 3.5}{3} = 4.33\\ldots\n\\]\nBias estimate:\n\\[\n\\text{Bias} = \\hat{\\theta}^* - \\hat{\\theta} = 4.33 - 5 = -0.67\n\\]\n\nBias-corrected estimate:\n\n\\[\n\\hat{\\theta}_{\\text{corr}} = \\hat{\\theta} - \\text{Bias} = 5 - (-0.67) = 5.67\n\\]\n\n\n\nProblem 38\nSuppose you have the following data that you believe follows a Poisson distribution:\n\\[\n\\{2,\\ 1,\\ 0,\\ 3,\\ 2,\\ 1,\\ 4\\}\n\\]\n\nDescribe how you would perform a nonparametric bootstrap to estimate the sampling distribution of the sample mean.\nDescribe how you would perform a parametric bootstrap in this case.\nWhy might the parametric bootstrap be more appropriate here?\n\n\n\nShow solutions\n\n\nNonparametric bootstrap:\n\n\n\nResample (with replacement) from the data to create many new samples.\nCompute the sample mean for each resample.\nUse the distribution of means to estimate uncertainty.\n\n\n\nParametric bootstrap:\n\n\n\nEstimate the parameter of the assumed model: \\(\\hat{\\lambda} = \\text{mean of the data} = \\frac{2+1+0+3+2+1+4}{7} = 1.86\\)\nGenerate new samples from \\(\\text{Poisson}(\\hat{\\lambda})\\) of size 7.\nRepeat and compute the sample mean for each.\nAnalyze the distribution of those means.\n\n\n\nIf we believe the data is truly Poisson-distributed, the parametric bootstrap can capture the shape of the population better than the finite empirical distribution, especially with small sample sizes.\n\n\n\n\nProblem 39\nConsider the dataset:\n\nimport numpy as np\ndata = np.array([2.3, 1.9, 3.1, 2.8, 3.5])\n\nYou are interested in the standard error of the median.\n\nUse Pyhon to create 1,000 bootstrap samples of the same size and compute the median for each.\nUse the bootstrap medians to estimate the standard error.\nPlot the bootstrap distribution of the median.\n\n\n\nShow solutions\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Set seed for reproducibility\nnp.random.seed(123)\n\n# Original data\ndata = np.array([2.3, 1.9, 3.1, 2.8, 3.5])\nn = len(data)\nB = 1000\nboot_medians = np.empty(B)\n\n# Bootstrap loop\nfor i in range(B):\n    sample_i = np.random.choice(data, size=n, replace=True)\n    boot_medians[i] = np.median(sample_i)\n\n# Standard error of the median\nse_median = np.std(boot_medians)\n\n# Plot\nplt.hist(boot_medians, bins=20, density=True, color='skyblue', edgecolor='black');\nplt.title(\"Bootstrap Distribution of Median\");\nplt.xlabel(\"Median\");\nplt.ylabel(\"Density\");\nplt.tight_layout();\nplt.show();\n\n\n\n\n\n\n\nprint(se_median)\n\n0.39077603560095653\n\n\nThis gives an estimate of the sampling variability of the median using only the observed data.\n\n\n\nProblem 40\nThe diameter of a randomly selected mechanical nut is a random variable with mean 10 mm and standard deviation 0.04 mm.\n\nIf \\(\\bar X\\) is the sample mean diameter of a random sample of \\(n=16\\) nuts, where is the sampling distribution of \\(\\bar X\\) centered, and what is the standard deviation of the \\(\\bar X\\) distribution?\nAnswer the question in (a) for a sample size of \\(n=64\\) nuts.\nFor which of the two random samples from (a) or (b), is \\(\\bar X\\) more likely to be within 0.01 mm from 10 mm? Explain your reasoning.\n\nSuppose the distrution of the diameter is normal.\n\nCalculate \\(P(9.99\\le \\bar X \\le 10.01)\\) when \\(n=16\\).\nHow likely is it that the sample mean diameter exceeds 10.01 when \\(n=25\\).\n\n\n\nShow solutions\n\n\nCentered at 10 with standard deviation \\(\\sigma/\\sqrt{n}=0.04/\\sqrt{16}= 0.01\\).\nCentered at 10 with standard deviation \\(\\sigma/\\sqrt{n}=0.04/\\sqrt{64}= 0.005\\).\nWith less variability, the second sample is more closely centered near 10.\n\\(P(9.99\\le \\bar X \\le 10.01)=P(\\bar X \\le 10.01)-P(\\bar X \\le 9.99) = 0.84-0.16= 0.68\\), getting the numbers from the code below:\n\n\nfrom scipy import stats\nimport numpy as np\nprint(\n  stats.norm.cdf(10.01, loc = 10, scale = 0.04/np.sqrt(16))-\n  stats.norm.cdf(9.99, loc = 10, scale = 0.04/np.sqrt(16))\n  )\n\n0.6826894921370756\n\n\n\n\\(P(\\bar X_{25}&gt;10.01)=0.106\\).\n\n\nprint(\n  1-stats.norm.cdf(10.01, loc = 10, scale = 0.04/np.sqrt(25))\n  )\n\n0.10564977366686001\n\n\n\n\n\nProblem 41\nThe tip percentage at a restaurant has a mean value of 18% and standard deviation of 6%.\n\nWhat is the apporixmate probability that the sample mean tip percentage for a random sample of 40 bills is between 16% and 19%?\nIf the sample size had been rather 15 than 40, could the probability requested in part (a) be calculated from the given information?\n\n\n\nShow solutions\n\n\nWith \\(n=40\\) observations, we can approximate the probability using the central limit theorem (CLT). That is \\(\\bar X_{40}\\sim N(0.18, 0.06/\\sqrt{40})\\): \\[P(16\\%\\le \\bar X_{40}\\le 19\\%)=P(\\bar X_{40}\\le 19\\%)-P(\\bar X_{40}\\le 16\\%)=0.8366\\]\n\n\nfrom scipy import stats\nmean = 0.18\nsd = 0.06/np.sqrt(40)\nprint(stats.norm.cdf(0.19, loc=mean, scale =sd)-stats.norm.cdf(0.16, loc=mean, scale = sd))\n\n0.8365722369182748\n\n\n\nIn (a) we used the Central Limit Theorem (CLT) to approximate the probability using a normal distribution. This relies on a large enough sample size. With only \\(n=15\\) observations, the sample size is likely too low for the CLT. If we use it anyway, we get 0.6423 for the probability.\n\n\nmean = 0.18\nsd = 0.06/np.sqrt(15)\nprint(stats.norm.cdf(0.19, loc=mean, scale =sd)-stats.norm.cdf(0.16, loc=mean, scale = sd))\n\n0.6423446905561638\n\n\n\n\n\nProblem 42\nSuppose the sugar content (g per cm³) of a randomly selected fruit from a certain orchard is normally distributed with a mean of 2.65 and a standard deviation of 0.85.\n\nIf a random sample of 25 fruits is selected, what is the probability that the sample average sugar content is at most 3.00? Between 2.65 and 3.00?\nHow large a sample size would be required to ensure that the first probability in part (a) is at least 0.99?\n\n\n\nShow solutions\n\n\nLet \\(X_i\\), \\(i=1,\\ldots, 25\\) denote the sugar content of 25 randomly selected fruits. We then have from the CLT that \\(\\overline X_{25}=\\frac{1}{25}\\sum_{i=1}^{25}X_i\\) is approximately \\(N(\\mu,\\sigma^2/n)=N(2.65, 0.85^2/25)\\).\n\nWe can thus find the probabilities by\n\\[P(\\overline X_{25} \\le 3) = P\\left(\\frac{\\overline X_{25}-\\mu}{\\sigma/\\sqrt{n}}\\le\\frac{3-2.65}{0.85/\\sqrt{25}}\\right)=P(Z\\le 2.059)=0.9802,\\] where the latter equality comes from\n\nfrom scipy.stats import norm\nprint(norm.cdf(2.059))\n\n0.9802528807563333\n\n\n\\[P(\\overline X_{25} \\le 2.65) = P\\left(\\frac{\\overline X_{25}-\\mu}{\\sigma/\\sqrt{n}}\\le\\frac{2.65-2.65}{0.85/\\sqrt{25}}\\right)=P(Z\\le 0)=0.5,\\] since Z is N(0,1) and the normal distribution is symmetric around its mean. We thus have that \\[\\begin{align*}\nP(2.65\\le\\overline X_{25}\\le 3)&=P(0\\le Z\\le 2.059)=P(Z\\le 2.059)-P(Z\\le 0)\\\\\n&= 0.9802-05=0.4802.\n\\end{align*}\\]\nWe could also find the probabilities without finding the Z-scores:\n\nimport numpy as np\nprint(\"P(Xbar &lt;= 3) = \", norm.cdf(3, loc = 2.65, scale = np.sqrt(0.0289)))\n\nP(Xbar &lt;= 3) =  0.980244426611219\n\nprint(\"P(Xbar &lt;= 2.65) = \", norm.cdf(3, loc = 2.65, scale = np.sqrt(0.0289)) - norm.cdf(2.65, loc = 2.65, scale = np.sqrt(0.0289)))\n\nP(Xbar &lt;= 2.65) =  0.480244426611219\n\n\n\nWe then need to find the value of n that satisfies: \\[P(\\overline X_{n} \\le 3) = P\\left(\\frac{\\overline X_{n}-\\mu}{\\sigma/\\sqrt{n}}\\le\\frac{3-2.65}{0.85/\\sqrt{n}}\\right)=P(Z\\le \\frac{0.35}{0.85}\\sqrt{n})=0.99.\\] The us then find the z-score that give a probability of 0.99 using the quantile function or percent point function (the inverse of cdf):\n\n\nprint(norm.ppf(0.99))\n\n2.3263478740408408\n\n\nThat is \\(P(Z\\le 2.3263) = 0.99\\), so to find \\(n\\) we need to solve \\[\\frac{0.35}{0.85}\\sqrt n = 2.3263\\Leftrightarrow n=2.3263^2\\frac{0.85^2}{0.35^2}=31.91\\approx 32.\\] Note that we round off \\(n\\) to an integer and we should here always round up to ensure that the probability is not less than 0.99.\n\n\n\nProblem 43\nThe Central Limit Theorem states that the sample mean \\(\\bar X\\) follows an approximately normal distribution when the sample size is sufficiently large. More precisely, the theorem asserts that the standardized version of \\(\\bar X\\) given by \\[\\frac{\\bar X-\\mu}{\\sigma/\\sqrt{n}},\\] converges in distribution to the standard normal distribution as \\(n\\) increases.\nHow does this relate to the Law of Large Numbers? If the standardized \\(\\bar X\\) follows an approximate standard normal distribution, what does this imply about the distribution of \\(\\bar X\\) itself?\n\n\nShow solutions\n\nThe Law of Large numbers states that \\(\\bar X\\) will approach \\(\\mu\\) when \\(n\\) becomes large. The central limit theorem implies that \\(\\bar X\\) will follow a \\(N(\\mu, \\sigma^2/n)\\) distribution asymptotically.\n\n\n\nProblem 44\nSuppose \\(X_1, X_2, \\dots, X_n\\) are independent and identically distributed (i.i.d.) random variables from an exponential distribution with parameter \\(\\lambda &gt; 0\\), i.e.,\n\\[f(x \\mid \\lambda) = \\lambda e^{-\\lambda x}, \\quad x &gt; 0.\\]\nFind the Maximum Likelihood Estimator (MLE) for \\(\\lambda\\).\n\n\nShow solutions\n\nThe likelihood function is: \\[L(\\lambda) = \\prod_{i=1}^{n} \\lambda e^{-\\lambda X_i} = \\lambda^n e^{-\\lambda \\sum X_i}.\\]\nThe log-likelihood function is: \\[\\ell(\\lambda) = n \\log \\lambda - \\lambda \\sum X_i.\\]\nDifferentiating with respect to \\(\\lambda\\): \\[\\frac{d\\ell}{d\\lambda} = \\frac{n}{\\lambda} - \\sum X_i.\\]\nSetting it equal to zero: \\[\\frac{n}{\\lambda} = \\sum X_i.\\]\nSolving for \\(\\lambda\\): \\[\\hat{\\lambda} = \\frac{n}{\\sum X_i}.\\]\n\n\n\nProblem 45\nLet \\(X_1, X_2, \\dots, X_n\\) be i.i.d. random variables from a normal distribution with unknown mean \\(\\mu\\) and known variance \\(\\sigma^2\\). That is, \\[X_i \\sim N(\\mu, \\sigma^2),\\] such that the density of the X’s is \\[f(x,\\mu,\\sigma)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp \\left( -\\frac{(x - \\mu)^2}{2\\sigma^2}\\right).\\]\nFind the MLE of \\(\\mu\\).\n\n\nShow solutions\n\nThe likelihood function is: \\[L(\\mu) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp \\left( -\\frac{(X_i - \\mu)^2}{2\\sigma^2} \\right).\\]\nThe log-likelihood function is: \\[\\ell(\\mu) = -\\frac{n}{2} \\log (2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (X_i - \\mu)^2.\\]\nDifferentiating with respect to \\(\\mu\\): \\[\\frac{d\\ell}{d\\mu} = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} (X_i - \\mu).\\]\nSetting it equal to zero: \\[\\sum_{i=1}^{n} (X_i - \\mu) = 0.\\]\nSolving for \\(\\mu\\): \\[\\hat{\\mu} = \\frac{1}{n} \\sum_{i=1}^{n} X_i.\\]\n\n\n\nProblem 46\nLet \\(X_1, X_2, \\dots, X_n\\) be i.i.d. random variables from a Bernoulli distribution with unknown parameter \\(p\\). That is, \\[P(X_i = 1) = p, \\quad P(X_i = 0) = 1 - p.\\] Find the MLE for \\(p\\).\nHint: We can also write the Bernoulli distribution as \\(P(X=x) =p^x(1-p)^{1-x}\\), where \\(x\\in \\{0,1\\}\\).\n\n\nShow solutions\n\nThe likelihood function is: \\[L(p) = \\prod_{i=1}^{n} p^{X_i} (1 - p)^{1 - X_i}.\\]\nThe log-likelihood function is: \\[\\ell(p) = \\sum_{i=1}^{n} X_i \\log p + (1 - X_i) \\log (1 - p).\\]\nDifferentiating with respect to \\(p\\): \\[\\frac{d\\ell}{dp} = \\sum_{i=1}^{n} \\frac{X_i}{p} - \\sum_{i=1}^{n} \\frac{1 - X_i}{1 - p}.\\]\nSetting it equal to zero: \\[\\sum X_i \\frac{1}{p} - \\sum (1 - X_i) \\frac{1}{1 - p} = 0.\\] Rearranging: \\[\\sum X_i (1 - p) = (n - \\sum X_i) p.\\]\nSolving for \\(p\\): \\[\\hat{p} = \\frac{1}{n} \\sum X_i.\\]\n\n\n\nProblem 47\nLet \\(\\hat{\\theta}\\) be an estimator of a parameter \\(\\theta\\). The bootstrap estimate of the bias is given by:\n\\[\\hat{\\text{Bias}} = \\frac{1}{B} \\sum_{b=1}^{B} \\hat{\\theta}^*_b - \\hat{\\theta},\\]\nwhere \\(\\hat{\\theta}^*_b\\) is the estimate from bootstrap sample \\(b\\).\nUse bootstrap resampling to estimate the bias of the maximum likelihood estimator of the variance;\\[s^2=\\frac1n \\sum_{i}(x_i-\\bar x)^2,\\] for a set of data \\(x_1,\\ldots, x_n\\).\nDescribe the procedure and discuss the sign of the bias. If you want to implement the code, use the following data set:\n\\[X = \\{4.2, 3.9, 5.1, 4.8, 4.3, 5.0, 3.7, 4.5, 4.9, 4.6\\}.\\]\n\n\nShow solutions\n\n\nCompute the sample variance \\(s^2\\) from the original dataset.\nGenerate 1000 bootstrap samples and compute \\(s^2\\) for each.\nCompute the bootstrap estimate of bias using the formula above.\nInterpret the result: If the bias is positive, \\(s^2\\) is overestimating the true variance; if negative, it is underestimating.\n\n\n\n\nProblem 48\nWhen playing the casino game american roulette, there are 38 possible outcomes: 0,00, and 1-36. The 0 and 00 are colored green, while odd numbers from 1-35 are black and even numbers from 2-36 are red.\n\nWhat is the probability of winning if you bet on a black outcome?\n\nA gambler has $100 and bets $1 each time he plays. He plays until he is either out of money or has $150.\n\nHow would you construct a Monte Carlo simulation of a situation where you always bet on black, and want to find the probability of going bankrupt.\nWhat would the probability be of the gambler reaches his goal of $150?\n\n\n\nShow solutions\n\n\nThe probability of winning when betting on black is \\(18/38=0.474\\)\nIt would make sense to use a while loop and continue to simulate unless the gambler is bankrupt or has $200. To simulate the game, use a Bernoulli distribution for each game with success probability \\(18/38=0.474\\). If the Bernoulli variable is 1, increase the “bank” of the gambler by 1 dollar. If it is 0, decrease it by 1 dollar. Repeat the game 10 000 times and calculate the relative frequency of going bankrupt.\n\n\nimport random\nrandom.seed(123)\n\ndef simulate_game_simple(starting_money=100, goal_money=150, bet_amount=1):\n    money = starting_money\n    while 0 &lt; money &lt; goal_money:\n        # Simulate the outcome: +1 with prob 18/38, -1 with prob 20/38\n        outcome = random.choices([-1, 1], weights=[20, 18])[0]\n        money += outcome * bet_amount\n    return money == 0  # True if bankrupt\n\ndef monte_carlo_simulation(n_simulations=10000):\n    bankrupt_count = sum(simulate_game_simple() for _ in range(n_simulations))\n    return bankrupt_count / n_simulations\n\n# Run the simplified simulation\nestimated_prob = monte_carlo_simulation(10000)\nprint(f\"Estimated probability of going bankrupt: {estimated_prob:.4f}\")\n\nEstimated probability of going bankrupt: 0.9949\n\n\n\nIn the end, there are only two outcomes of the experiment. Either the gambler ends up with $0 (bankrupt) or $150. The probability achieving $150 would therefore be 1-the probability of going bankrupt.\n\nFrom the python simulation above, it would then be \\[1-P(bankrupt)=1-0.9949=0.0051\\]\n\n\n\nProblem 49\nLet \\(X_1,\\ldots, X_n\\) be a random sample from the probability density function \\[f(x|\\theta) = \\theta x^{\\theta -1},\\quad 0&lt;\\theta&lt;\\infty,\\quad 0\\le x\\le1.\\] Find the MLE of \\(\\theta\\).\n\n\nShow solutions\n\n\\[L(\\theta)=f(x_1,\\ldots, x_n|\\theta) = \\theta^n \\prod_{i=1}^nx_i^{\\theta -1}\\] \\[\\log L = n\\log \\theta + (\\theta-1)\\sum_{i=1}^n\\log x_i\\] \\[\\frac{\\partial \\log L}{\\partial \\theta} = \\frac{n}{\\theta} + \\sum_{i=1}^n\\log x_i=0\\] \\[\\widehat \\theta = \\frac{-n}{\\sum_{i=1}^n \\log x_i}.\\]",
    "crumbs": [
      "Modules",
      "Module 3: Estimation, sampling distributions and resampling",
      "Exercises"
    ]
  },
  {
    "objectID": "3-maximum-likelihood.html",
    "href": "3-maximum-likelihood.html",
    "title": "Maximum likelihood",
    "section": "",
    "text": "Slides for “Maximum Likelihood Estimation”\nSlides for “Maximum Likelihood Estimator of mu”\nSlides for “Maximum Likelihood Estimator for Bernoulli distribution”\n\n\n\nWhat is the likelihood?\nWhy do we use the log-likelihood?\nWhat are the assumptions for MLE?\nWhat is the Maximum Likelihood Estimator of the mean in a normal distribution?\nWhat is the Maximum Likelihood Estimator of the success probability in a Bernoulli distribution?",
    "crumbs": [
      "Modules",
      "Module 3: Estimation, sampling distributions and resampling",
      "Maximum likelihood"
    ]
  },
  {
    "objectID": "3-maximum-likelihood.html#control-questions",
    "href": "3-maximum-likelihood.html#control-questions",
    "title": "Maximum likelihood",
    "section": "",
    "text": "What is the likelihood?\nWhy do we use the log-likelihood?\nWhat are the assumptions for MLE?\nWhat is the Maximum Likelihood Estimator of the mean in a normal distribution?\nWhat is the Maximum Likelihood Estimator of the success probability in a Bernoulli distribution?",
    "crumbs": [
      "Modules",
      "Module 3: Estimation, sampling distributions and resampling",
      "Maximum likelihood"
    ]
  },
  {
    "objectID": "3-population-vs-sample.html",
    "href": "3-population-vs-sample.html",
    "title": "Population vs sample",
    "section": "",
    "text": "Slides for “Population vs sample”\n\n\n\nWhat is the difference between a population and sample?\nWhat do we call numbers derived from a population?\nWhat do we call numbers derived from a sample?\nWhat does it mean that a sample is representative?\nWhat is sampling bias?",
    "crumbs": [
      "Modules",
      "Module 3: Estimation, sampling distributions and resampling",
      "Population vs sample"
    ]
  },
  {
    "objectID": "3-population-vs-sample.html#control-questions",
    "href": "3-population-vs-sample.html#control-questions",
    "title": "Population vs sample",
    "section": "",
    "text": "What is the difference between a population and sample?\nWhat do we call numbers derived from a population?\nWhat do we call numbers derived from a sample?\nWhat does it mean that a sample is representative?\nWhat is sampling bias?",
    "crumbs": [
      "Modules",
      "Module 3: Estimation, sampling distributions and resampling",
      "Population vs sample"
    ]
  },
  {
    "objectID": "3-random-number-generation.html",
    "href": "3-random-number-generation.html",
    "title": "Random number generation",
    "section": "",
    "text": "Slides for “Random number generation”\n\n\n\nWhat is random?\nWhat do we call random numbers generated from an algorithm?\nAre these numbers truly random?\nWhat do mean when we say that numbers generated by the algorithms are deterministic?\nWhat does a setting a seed do?\nCan you explain the inverse transform sampling procedure?",
    "crumbs": [
      "Modules",
      "Module 3: Estimation, sampling distributions and resampling",
      "Random number generation"
    ]
  },
  {
    "objectID": "3-random-number-generation.html#control-questions",
    "href": "3-random-number-generation.html#control-questions",
    "title": "Random number generation",
    "section": "",
    "text": "What is random?\nWhat do we call random numbers generated from an algorithm?\nAre these numbers truly random?\nWhat do mean when we say that numbers generated by the algorithms are deterministic?\nWhat does a setting a seed do?\nCan you explain the inverse transform sampling procedure?",
    "crumbs": [
      "Modules",
      "Module 3: Estimation, sampling distributions and resampling",
      "Random number generation"
    ]
  },
  {
    "objectID": "3-textbook.html",
    "href": "3-textbook.html",
    "title": "TECH3 Applied statistics",
    "section": "",
    "text": "The following is a redistribution of chapters 5, 7 and 8 of Statistical Thinking for the 21st Century by Russell A. Poldrack, 2019, under the CC BY-NC 4.0 licence.",
    "crumbs": [
      "Modules",
      "Module 3: Estimation, sampling distributions and resampling",
      "Textbook"
    ]
  },
  {
    "objectID": "3-textbook.html#fitting-models-to-data",
    "href": "3-textbook.html#fitting-models-to-data",
    "title": "TECH3 Applied statistics",
    "section": "Fitting models to data",
    "text": "Fitting models to data",
    "crumbs": [
      "Modules",
      "Module 3: Estimation, sampling distributions and resampling",
      "Textbook"
    ]
  },
  {
    "objectID": "3-textbook.html#sampling",
    "href": "3-textbook.html#sampling",
    "title": "TECH3 Applied statistics",
    "section": "Sampling",
    "text": "Sampling",
    "crumbs": [
      "Modules",
      "Module 3: Estimation, sampling distributions and resampling",
      "Textbook"
    ]
  },
  {
    "objectID": "3-textbook.html#resampling-and-simulation",
    "href": "3-textbook.html#resampling-and-simulation",
    "title": "TECH3 Applied statistics",
    "section": "Resampling and Simulation",
    "text": "Resampling and Simulation",
    "crumbs": [
      "Modules",
      "Module 3: Estimation, sampling distributions and resampling",
      "Textbook"
    ]
  },
  {
    "objectID": "3-what-is-an-estimator.html",
    "href": "3-what-is-an-estimator.html",
    "title": "What is an estimator?",
    "section": "",
    "text": "Slides for “What is an estimator?”\n\n\n\nWhat is an estimator?\nWhat is an estimate?\nWhat does it mean that an estimator is unbiased?\nWhat about a consistent estimator?\nIs the sample mean \\(\\bar{X}_n\\) unbiased and/or consistent?\nIs the sample variance \\(S_n^2\\) unbiased and/or consistent?",
    "crumbs": [
      "Modules",
      "Module 3: Estimation, sampling distributions and resampling",
      "What is an estimator?"
    ]
  },
  {
    "objectID": "3-what-is-an-estimator.html#control-questions",
    "href": "3-what-is-an-estimator.html#control-questions",
    "title": "What is an estimator?",
    "section": "",
    "text": "What is an estimator?\nWhat is an estimate?\nWhat does it mean that an estimator is unbiased?\nWhat about a consistent estimator?\nIs the sample mean \\(\\bar{X}_n\\) unbiased and/or consistent?\nIs the sample variance \\(S_n^2\\) unbiased and/or consistent?",
    "crumbs": [
      "Modules",
      "Module 3: Estimation, sampling distributions and resampling",
      "What is an estimator?"
    ]
  },
  {
    "objectID": "4-hypothesis-testing-in-python.html",
    "href": "4-hypothesis-testing-in-python.html",
    "title": "Hypothesis testing in Python",
    "section": "",
    "text": "In any given hypothesis testing situation, we need to decide",
    "crumbs": [
      "Modules",
      "Module 4: Designing studies, hypothesis testing, and quantifying effects",
      "Hypothesis testing in Python"
    ]
  },
  {
    "objectID": "4-hypothesis-testing-in-python.html#inference-about-one-population",
    "href": "4-hypothesis-testing-in-python.html#inference-about-one-population",
    "title": "Hypothesis testing in Python",
    "section": "Inference About One Population",
    "text": "Inference About One Population\n\n\n\n\n\n\n\n\n\n\nTest\nNull Hypothesis\nTest Statistic\nDistribution Under \\(H_0\\)\nComments\n\n\n\n\nTest for a population mean\n\\(H_0: \\mu = \\mu_0\\)\n\\(T = \\frac{\\bar X - \\mu_0}{s / \\sqrt{n}}\\)\nt-distribution, \\(n-1\\) df\nOne- or two-sided\n\n\nTest for a population variance\n\\(H_0: \\sigma = \\sigma_0\\)\n\\(\\chi^2 = \\frac{(n-1)s^2}{\\sigma_0^2}\\)\n\\(\\chi^2\\)-distribution, \\(n-1\\) df\nOne- or two-sided; skewed\n\n\nTest for a population proportion\n\\(H_0: p = p_0\\)\n\\(Z = \\frac{\\hat p - p_0}{\\sqrt{p_0(1-p_0)/n}}\\)\nStandard normal\nOne- or two-sided",
    "crumbs": [
      "Modules",
      "Module 4: Designing studies, hypothesis testing, and quantifying effects",
      "Hypothesis testing in Python"
    ]
  },
  {
    "objectID": "4-hypothesis-testing-in-python.html#inference-about-two-populations",
    "href": "4-hypothesis-testing-in-python.html#inference-about-two-populations",
    "title": "Hypothesis testing in Python",
    "section": "Inference About Two Populations",
    "text": "Inference About Two Populations\n\n\n\n\n\n\n\n\n\n\nTest\nNull Hypothesis\nTest Statistic\nDistribution Under H₀\nComments\n\n\n\n\nEqual means (independent, equal variances)\n\\(H_0: \\mu_1 = \\mu_2\\)\n\\(T = \\frac{\\bar X_1 - \\bar X_2}{\\sqrt{s_p^2(1/n_1 + 1/n_2)}}\\)\nt-distribution, \\(n_1 + n_2 - 2\\) df\nPooled variance\n\n\nEqual means (independent, unequal variances — Welch)\n\\(H_0: \\mu_1 = \\mu_2\\)\n\\(T = \\frac{\\bar X_1 - \\bar X_2}{\\sqrt{s_1^2/n_1 + s_2^2/n_2}}\\)\nWelch’s t-distribution: \\(\\text{df}=\\frac{(S_1^2/n_1+S_2^2/n_2)^2}{\\frac{(S_1^2/n_1)^2}{n_1-1}+\\frac{(S_2^2/n_2)^2}{n_2-1}}\\)\nRobust to unequal variances\n\n\nEqual means (paired samples)\n\\(H_0: \\mu_D = 0\\)\n\\(T = \\frac{\\bar D}{s_D / \\sqrt{n}}\\)\nt-distribution, \\(n-1\\) df\nUse differences\n\n\nEqual variances\n\\(H_0: \\sigma_1^2 = \\sigma_2^2\\)\n\\(F = \\frac{s_1^2}{s_2^2}\\)\nF-distribution, \\((n_1-1, n_2-1)\\) df\nPlace larger variance in numerator\n\n\nEqual proportions\n\\(H_0: p_1 = p_2\\)\n\\(Z = \\frac{\\hat p_1 - \\hat p_2}{\\sqrt{\\hat p(1-\\hat p)(1/n_1 + 1/n_2)}}\\)\nStandard normal\nUses pooled proportion \\(\\hat p\\)",
    "crumbs": [
      "Modules",
      "Module 4: Designing studies, hypothesis testing, and quantifying effects",
      "Hypothesis testing in Python"
    ]
  },
  {
    "objectID": "4-hypothesis-testing-in-python.html#paired-t-test",
    "href": "4-hypothesis-testing-in-python.html#paired-t-test",
    "title": "Hypothesis testing in Python",
    "section": "Paired t-test",
    "text": "Paired t-test\nWe have measured the weight of five people before and after a diet.\n\\[H_0: \\mu_\\text{before}=\\mu_\\text{after}\\quad \\text{vs}\\quad H_A: \\mu_\\text{before} &gt;\\mu_\\text{after}.\\]\n\nimport numpy as np\nfrom scipy import stats\nbefore = np.array((85, 76, 72, 92, 79)) # weight in kg\nafter  = np.array((78, 73, 83, 89, 71)) \npaired_test = stats.ttest_rel(before, after, alternative=\"greater\")\nprint(paired_test)\n\nTtestResult(statistic=np.float64(0.5872202195147035), pvalue=np.float64(0.29430108085972617), df=np.int64(4))\n\n\nAs you can see, the default print of the test object is not very pretty. Let us format the printout a bit nicer:\n\nprint(f\"Paired t-test (one-sided)\\n\"\n      f\"-------------------------\\n\"\n      f\"t-statistic : {paired_test.statistic:.4f}\\n\"\n      f\"p-value     : {paired_test.pvalue:.4f}\\n\")\n\nPaired t-test (one-sided)\n-------------------------\nt-statistic : 0.5872\np-value     : 0.2943\n\n\n\nThis test is equivalent to doing a one-sample t-test on the weight difference. \\[H_0: \\mu_d=0\\quad \\text{vs}\\quad H_A: \\mu_d &gt;0.\\]\n\ndiff = before-after\nonesample_test = stats.ttest_1samp(diff, popmean = 0, \n                                   alternative = \"greater\")\nprint(onesample_test)\nprint(f\"One-sample t-test (one-sided)\\n\"\n      f\"-------------------------\\n\"\n      f\"t-statistic : {onesample_test.statistic:.4f}\\n\"\n      f\"p-value     : {onesample_test.pvalue:.4f}\\n\")\n\nTtestResult(statistic=np.float64(0.5872202195147035), pvalue=np.float64(0.29430108085972617), df=np.int64(4))\nOne-sample t-test (one-sided)\n-------------------------\nt-statistic : 0.5872\np-value     : 0.2943\n\n\n\nAs you can see, the test statistics and corresponding p-values are equal. Since the p-value is high, the evidence against the null-hypothesis is weak, and for a given significance level (say 10%), we would not reject the null of change in expected weight following the diet. The expected weight is equal before and after the diet. Five people is very few for testing a diet, so we could suggest that the investigators collect more data.",
    "crumbs": [
      "Modules",
      "Module 4: Designing studies, hypothesis testing, and quantifying effects",
      "Hypothesis testing in Python"
    ]
  },
  {
    "objectID": "4-hypothesis-testing-in-python.html#proportions-test---one-sample",
    "href": "4-hypothesis-testing-in-python.html#proportions-test---one-sample",
    "title": "Hypothesis testing in Python",
    "section": "Proportions test - one sample",
    "text": "Proportions test - one sample\nTo evaluate whether an observed proportion differs from a specified benchmark, we can carry out a one-sample test of proportions in Python.\nAs an example, let’s say \\(p\\) is the probability of heads for a given coin. We want to test if the coin is fair, i.e.\n\\[H_0: p = 0.5\\quad \\text{vs}\\quad H_A:p\\neq 0.5\\] with a significance level of 5%. We toss the coin ten times and observe 4 heads.\n\nfrom scipy import stats\nfrom statsmodels.stats.proportion import proportions_ztest\nprop_test = proportions_ztest(count = 4,\n                  nobs = 10,\n                  value = 0.5,\n                  alternative = \"two-sided\")\nprint(prop_test)\n\n(np.float64(-0.6454972243679027), np.float64(0.5186050164287257))\n\n\nThis test is based on a normal approximation of the binomial distribution. We could do the corresponding exact test, using a binomial test:\n\nstats.binomtest(4, 10, 0.5, alternative = \"two-sided\")\n\nBinomTestResult(k=4, n=10, alternative='two-sided', statistic=0.4, pvalue=0.75390625)\n\n\nA common rule of thumb is that when \\(np&gt;5\\), the approximation is good. In this case we are exactly on this common threshold.\nAnother alternative is to use a bootstrap approach for testing.\n\nnp.random.seed(12345)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsample_size = 10\ncount = 4\nphat = stats.binom.rvs(n=sample_size, p = 0.5, size = 10000)/sample_size\nz = (phat-0.5)/np.sqrt(0.5*0.5/sample_size)\nz_stat = (count/sample_size - 0.5)/np.sqrt(0.5*0.5/sample_size)\nsns.histplot(z, binwidth = .3, stat = \"density\")\nplt.axvline(x=z_stat, color = \"red\")\nplt.show()\nprint(\"Statistic: \", np.round(z_stat,3),\n      \"\\nNumber exceeding: \", (z&gt;z_stat).sum(),\n      \"\\nPercentage: \", np.round((z&gt;z_stat).mean()*100,3),\"%\")\n\n\n\n\n\n\n\n\nStatistic:  -0.632 \nNumber exceeding:  6261 \nPercentage:  62.61 %",
    "crumbs": [
      "Modules",
      "Module 4: Designing studies, hypothesis testing, and quantifying effects",
      "Hypothesis testing in Python"
    ]
  },
  {
    "objectID": "4-hypothesis-testing-in-python.html#two-saple-test-of-proportions",
    "href": "4-hypothesis-testing-in-python.html#two-saple-test-of-proportions",
    "title": "Hypothesis testing in Python",
    "section": "Two-saple test of proportions",
    "text": "Two-saple test of proportions\nInstead of comparing a proportion to a benchmark value, we can compare proportions between two groups.\nExample: A web store wants to test whether changing the color of the “buy” button effects sales (e.g. increases the rate of clicking “buy”). When customers load the website, they assign a 50% chance that the customer sees a blue (group A) or green (group B) buy button. Does changing the color from blue to green increase the rate of customers clicking “buy”?\n\\[H_0: p_A=p_B\\quad \\text{vs}\\quad p_A &lt; p_B.\\] We can accept a high significance level here, since we are not afraid of making type I error (incorrectly rejecting a true null hypothesis). Let us use \\(\\alpha = 10\\%\\).\n\n\n\nGroup\nSample size\nClicks\nRate\n\n\n\n\nA (blue button)\n2359\n543\n23%\n\n\nB (green button)\n2523\n606\n24%\n\n\n\nThe observed rate is higher for green button, but lets perform the test in Python:\n\nfrom statsmodels.stats.proportion import test_proportions_2indep\ntest = test_proportions_2indep(count1=543, \n                               nobs1=2359,\n                               count2=606,\n                               nobs2=2523,\n                               value=0,\n                               method=\"wald\",\n                               compare='diff',\n                               alternative='smaller')\nprint(test)\n\nstatistic = -0.8241828933434447\npvalue = 0.2049178227694594\ncompare = diff\nmethod = wald\ndiff = -0.010007969075350343\nratio = 0.9583331584536157\nodds_ratio = 0.9458744057225107\nvariance = 0.0001474499797394375\nalternative = smaller\nvalue = 0\ntuple = (np.float64(-0.8241828933434447), np.float64(0.2049178227694594))\n\n\nThe experimental setup described above if often called an AB-test. An A/B test is any randomized experiment comparing two versions (A and B) of something to see which performs better on some outcome.\nWe can do the test also “by hand”. The test statistic is \\[Z=\\frac{\\widehat p_A-\\widehat p_B}{\\text{SE}},\\] where \\[\n\\widehat p_\\text{pool} = \\frac{\\text{Total clicks}}{\\text{Total customers}} = \\frac{545+606}{2359+2523}=23.58\\%\n\\] and \\[\\text{SE}=\\sqrt{\\widehat p_\\text{pool}(1-\\widehat p_\\text{pool})(\\frac1{n_1}+\\frac1{n_2})}=0.01216\\] Thus, \\[\nZ = \\frac{0.23-0.24}{0.01216}=-0.82\n\\] Note that, if the alternative hypothesis is true, \\(p_A\\) is smaller than \\(p_B\\), making \\(Z\\) negative. So we reject the null hypothesis for small values of Z (i.e. large negative values). To find the p-value,\n\\[P(Z\\le z)=P(Z\\le -0.82) = 0.2061\\] based on\n\nfrom scipy.stats import norm\nprint(round(norm.cdf(-0.82),4))\n\n0.2061\n\n\nWith \\(\\alpha = 10\\%\\) we would not reject the null hypothesis, since the p-value exceeds the significance level. The evidence does not suggest any difference between the clicking rates for green versus blue buying buttons.",
    "crumbs": [
      "Modules",
      "Module 4: Designing studies, hypothesis testing, and quantifying effects",
      "Hypothesis testing in Python"
    ]
  },
  {
    "objectID": "4-intro.html",
    "href": "4-intro.html",
    "title": "TECH3 Applied statistics",
    "section": "",
    "text": "Designing studies, hypothesis testing, and quantifying effects\n\nFocus: Modern hypothesis testing using bootstrap methods.\nKey topics: classical test statistics, confidence intervals, effect size, power analysis, AB-testing.\nSpecial Emphasis: Statistical significance versus practical significance. Describe the proper interpretations of a p-value and common misinterpretations",
    "crumbs": [
      "Modules",
      "Module 4: Designing studies, hypothesis testing, and quantifying effects",
      "Introduction"
    ]
  },
  {
    "objectID": "4-python.html",
    "href": "4-python.html",
    "title": "Python",
    "section": "",
    "text": "Python\nHere we will present some useful python commands relevant for what we want to do in Module 4.",
    "crumbs": [
      "Modules",
      "Module 4: Designing studies, hypothesis testing, and quantifying effects",
      "Python"
    ]
  },
  {
    "objectID": "4-textbook.html",
    "href": "4-textbook.html",
    "title": "TECH3 Applied statistics",
    "section": "",
    "text": "The following is a redistribution of chapters 9 and 10  of Statistical Thinking for the 21st Century by Russell A. Poldrack, 2019, under the CC BY-NC 4.0 licence.",
    "crumbs": [
      "Modules",
      "Module 4: Designing studies, hypothesis testing, and quantifying effects",
      "Textbook"
    ]
  },
  {
    "objectID": "4-textbook.html#hypothesis-testing",
    "href": "4-textbook.html#hypothesis-testing",
    "title": "TECH3 Applied statistics",
    "section": "Hypothesis testing",
    "text": "Hypothesis testing",
    "crumbs": [
      "Modules",
      "Module 4: Designing studies, hypothesis testing, and quantifying effects",
      "Textbook"
    ]
  },
  {
    "objectID": "4-textbook.html#quantifying-effects-and-designing-studies",
    "href": "4-textbook.html#quantifying-effects-and-designing-studies",
    "title": "TECH3 Applied statistics",
    "section": "Quantifying effects and designing studies",
    "text": "Quantifying effects and designing studies",
    "crumbs": [
      "Modules",
      "Module 4: Designing studies, hypothesis testing, and quantifying effects",
      "Textbook"
    ]
  },
  {
    "objectID": "4-textbook.html#comparing-means",
    "href": "4-textbook.html#comparing-means",
    "title": "TECH3 Applied statistics",
    "section": "Comparing means",
    "text": "Comparing means",
    "crumbs": [
      "Modules",
      "Module 4: Designing studies, hypothesis testing, and quantifying effects",
      "Textbook"
    ]
  },
  {
    "objectID": "5-assessing-the-model.html",
    "href": "5-assessing-the-model.html",
    "title": "Assessing the model",
    "section": "",
    "text": "“Goodness of fit”\n“Criticizing the model and checking assumptions”\n\n\n\n\n\nWhat is the interpretation of \\(R^2=0.80\\)?\nIf the correlation between \\(X\\) and \\(Y\\) is \\(0.5\\), what is \\(R^2\\) in the regression \\(Y=\\beta_0+\\beta_1 X+\\epsilon\\)?\nWhat are the assumptions of a linear regression?\nHow do we assess whether the linearity assumption is fulfilled?\nHow do we assess whether the residuals are homoskedastic (have constant variance)?\nWhat is QQ-plot and what do we use it for?\nDo we need normality of the errors when estimating coefficients?\nDo we need normality when making prediction intervals?",
    "crumbs": [
      "Modules",
      "Module 5: Measuring relationships and fitting models",
      "Assessing the model"
    ]
  },
  {
    "objectID": "5-assessing-the-model.html#slides",
    "href": "5-assessing-the-model.html#slides",
    "title": "Assessing the model",
    "section": "",
    "text": "“Goodness of fit”\n“Criticizing the model and checking assumptions”",
    "crumbs": [
      "Modules",
      "Module 5: Measuring relationships and fitting models",
      "Assessing the model"
    ]
  },
  {
    "objectID": "5-assessing-the-model.html#control-questions",
    "href": "5-assessing-the-model.html#control-questions",
    "title": "Assessing the model",
    "section": "",
    "text": "What is the interpretation of \\(R^2=0.80\\)?\nIf the correlation between \\(X\\) and \\(Y\\) is \\(0.5\\), what is \\(R^2\\) in the regression \\(Y=\\beta_0+\\beta_1 X+\\epsilon\\)?\nWhat are the assumptions of a linear regression?\nHow do we assess whether the linearity assumption is fulfilled?\nHow do we assess whether the residuals are homoskedastic (have constant variance)?\nWhat is QQ-plot and what do we use it for?\nDo we need normality of the errors when estimating coefficients?\nDo we need normality when making prediction intervals?",
    "crumbs": [
      "Modules",
      "Module 5: Measuring relationships and fitting models",
      "Assessing the model"
    ]
  },
  {
    "objectID": "5-contingency-tables.html",
    "href": "5-contingency-tables.html",
    "title": "Contingency tables",
    "section": "",
    "text": "Slides for “Contingency tables”\n\n\n\nWhat is a contingency table?\nWhat do we test when doing a chi-squared test on contingency tables?\nWhat do we \\(P(S\\cup B)=P(S)\\cdot P(B)\\) under the null hypothesis?",
    "crumbs": [
      "Modules",
      "Module 5: Measuring relationships and fitting models",
      "Contingency tables"
    ]
  },
  {
    "objectID": "5-contingency-tables.html#control-questions",
    "href": "5-contingency-tables.html#control-questions",
    "title": "Contingency tables",
    "section": "",
    "text": "What is a contingency table?\nWhat do we test when doing a chi-squared test on contingency tables?\nWhat do we \\(P(S\\cup B)=P(S)\\cdot P(B)\\) under the null hypothesis?",
    "crumbs": [
      "Modules",
      "Module 5: Measuring relationships and fitting models",
      "Contingency tables"
    ]
  },
  {
    "objectID": "5-correlation-and-causation.html",
    "href": "5-correlation-and-causation.html",
    "title": "Correlation and causation",
    "section": "",
    "text": "Slides for “Correlation and causation”\n\n\n\nIs heavier rain correlated with more umbrellas in the street?\nWhat is the causal relationship between heavier rain and proportion of umbrellas in the street?\nWhy do we use controlled experiments?\nWhy is randomization important in this context?\nWhat is the difference between an observational study and an experimental one?\nWhat is a confounder?",
    "crumbs": [
      "Modules",
      "Module 5: Measuring relationships and fitting models",
      "Correlation and causation"
    ]
  },
  {
    "objectID": "5-correlation-and-causation.html#control-questions",
    "href": "5-correlation-and-causation.html#control-questions",
    "title": "Correlation and causation",
    "section": "",
    "text": "Is heavier rain correlated with more umbrellas in the street?\nWhat is the causal relationship between heavier rain and proportion of umbrellas in the street?\nWhy do we use controlled experiments?\nWhy is randomization important in this context?\nWhat is the difference between an observational study and an experimental one?\nWhat is a confounder?",
    "crumbs": [
      "Modules",
      "Module 5: Measuring relationships and fitting models",
      "Correlation and causation"
    ]
  },
  {
    "objectID": "5-exercises.html",
    "href": "5-exercises.html",
    "title": "Exercises",
    "section": "",
    "text": "The Chi-square test for independence is used to determine whether two categorical variables are independent. Given a contingency table of observed counts \\(O_{ij}\\), compute expected counts under independence:\n\\[\n\\begin{align*}\nE\\_{ij} &= \\frac{\\text{(row total)}_i \\times\\text{(column  total)}_j}{\\text{grand total}} \\\\\n\\chi^2& = \\sum_{ij} \\frac{(O_{ij} - E_{ij})^2}{E_{ij}}\n\\end{align*}\n\\]\nWhere we let \\(O_{ij}\\) denote observed in each cell ij and \\(E_{ij}\\) denote expected. \\(df = (rows − 1)(columns − 1)\\)\n\n\n\nA survey records whether people prefer coffee or tea in two regions:\n\n\n\n\nCoffee\nTea\nTotal\n\n\n\n\nRegion A\n30\n20\n50\n\n\nRegion B\n40\n10\n50\n\n\nTotal\n70\n30\n100\n\n\n\n\nSet up the hypotheses.\nCompute the expected counts.\nCompute the chi-square statistic.\nTest independence at \\(\\alpha=0.05\\)\nWhat do you conclude?\n\n\n\nShow solution\n\n\n\\(H_0\\): Coffee preference is independent of region. \\(H_a\\): They are dependent.\nE.g., \\(E_{11} = \\frac{50*70}{100} = 35\\)\n\nExpected table:\n\n\n\n\nCoffee\nTea\n\n\n\n\nRegion A\n35\n15\n\n\nRegion B\n35\n15\n\n\n\n\nNow let’s compute the test statistic\n\n\\[\n\\chi^2 = \\frac{(30-35)^2}{35} + \\frac{(20-15)^2}{15} + \\frac{(40-35)^2}{35} + \\frac{(10-15)^2}{15} = 4.76\n\\]\n\n\\(df = (2−1)(2−1) = 1\\), critical value is \\(\\chi^2=3.84\\). This critical value can be found in any \\(\\chi^2\\) table. We can also check in Python.\n\n\nfrom scipy import stats\nprint(stats.chi2.ppf(0.95, df = 1))\n\n3.841458820694124\n\n\n\nSince \\(7.62 &gt; 3.84\\), reject \\(H_0\\). We can not say conclude that preferences are independent of region.\n\n\n\n\n\nTests whether observed frequencies match a specified distribution (e.g., uniform, Poisson, etc.).\nThe test statistic is found as below\n\\[\n\\chi^2 = \\sum_i \\frac{(O_i - E_i)^2}{E_i}\n\\]\n\\[\n\\text{df} = \\text{number of categories} - 1\n\\]\n\n\n\nA six-sided die is rolled 60 times. The results:\nFace:   1  2  3  4  5  6  \nFreq:  10 12 9 11 8 10\nWe want to find out if these results are consistent with a fair die?\n\nState the hypotheses.\nCompute the expected count.\nCompute the test statistic and p-value.\nInterpret the result.\n\n\n\nShow solution\n\n\n\\(H_0\\): The die is fair. \\(H_a\\): The die is not fair.\nExpected = 60/6 = 10 per face\n\n\n\\[\n\\chi^2 = \\sum \\frac{(O_i - 10)^2}{10} = \\frac{(0)^2 + (2)^2 + (-1)^2 + (1)^2 + (-2)^2 + (0)^2}{10} = \\frac{0+4+1+1+4+0}{10} = 1\n\\] First we will find the degrees of freedom, such that we can have a proper critical value.\n\\(df = 6 − 1 = 5\\), critical value is then \\(\\chi^2=11.07\\).\nWe can find the exact critical value and compute the p-value of the test in Python.\n\nprint(stats.chi2.ppf(0.95, df = 5))\n\n11.070497693516351\n\n## Note for that chi square test we find the p-value by looking at the right tail\nprint(1-stats.chi2.cdf(1, df = 5))\n\n0.9625657732472964\n\n\n\nSince \\(1 &lt; 11.07\\), and we have a p-value as high as 96%, we fail to reject \\(H_0\\): die could be fair.\n\n\n\n\n\nA survey was conducted to determine if there’s an association between gender (Male, Female) and preference for a new product (Like, Dislike).\nData\n\n\n\n\nLike\nDislike\nTotal\n\n\n\n\nMale\n30\n20\n50\n\n\nFemale\n20\n30\n50\n\n\nTotal\n50\n50\n100\n\n\n\n\nState the null and alternative hypotheses.\nCalculate the expected frequencies.\nCompute the chi-squared test statistic.\nDetermine the degrees of freedom.\nAt \\(\\alpha=0.05\\), determine the critical value and conclude the test.\n\n\n\nShow solution\n\n\n\\(H_0\\): Gender and product preference are independent.\n\n\\(H_1\\): Gender and product preference are not independent.\n\nExpected frequencies:\n\n\nMale-Like: (50×50)/100 = 25\nMale-Dislike: (50×50)/100 = 25\nFemale-Like: (50×50)/100 = 25\nFemale-Dislike: (50×50)/100 = 25\n\n\nLet’s find the test statistic first, by using the formula for the chi-square test for independence.\n\n\\[\n\\chi^2=\\sum^n_{ij}\\frac{(O_{ij}-E_{ij})^2}{E_{ij}}=\\frac{5^2+(-5)^2+5^2+(-5^2)}{25}=\\frac{100}{25}=4\n\\]\n\nWe can now compute the degrees of freedom simply by looking at the table. \\(df=(rows-1)(columns-1)=(2-1)(2-1)=1\\), which nets \\(\\chi_{1,0.05}^2=3.841\\)\nCritical value at \\(\\alpha=0.05\\) and df = 1 is 3.841. Since 4 &gt; 3.841, we reject \\(H_0\\). There is a significant association between gender and product preference.\n\n\n\n\n\nExplain how the chi-squared distribution is related to the standard normal distribution.\n\nDescribe the relationship between the standard normal distribution and the chi-squared distribution.\nIf \\(Z\\) is a standard normal variable, what is the distribution of \\(Z^2\\)?\n\n\n\nShow solutions\n\n\nThe chi-squared distribution with k degrees of freedom is the distribution of the sum of the squares of k independent standard normal variables. That is, if \\(Z_1,\\dots,Z_k\\) are independent standard normal variables, then\n\n\\[\nX=\\sum^k_{i=1}Z_i^2\\sim\\chi^2_k\n\\] I.e. a chi-square distribution with k degrees of freedom.\n\nConsider a standard normal \\(Z\\) Then \\[\nX=\\sum_{i=1}^kz_i^2=\\sum^1_{i=1}Z_i^2\\sim\\chi_1^2\n\\] i.e. the square of \\(Z\\) will be chi-square distributed with 1 degree of freedom.\n\n\n\n\n\n\nExplain what a contingency table shows in the context of testing independence.\nWhy are expected counts calculated, and how do they relate to independence?\n\n\n\nShow solutions\n\n\nA contingency table displays the frequency distribution of variables and helps test whether two categorical variables are independent.\nExpected counts are computed under the assumption of independence. Large deviations between observed and expected suggest dependence.\n\n\n\n\n\nA six-sided die is rolled 120 times. The outcomes are: \\(1: 10, \\ 2: 14, \\ 3: 19, \\ 4: 24, \\ 5: 25, \\ 6: 28\\)\n\nSet up hypotheses.\nCompute expected frequencies.\nPerform the chi-squared test at \\(\\alpha=0.05\\).\nInterpret the results of the test. How would changing \\(\\alpha\\) change the results of the test?\n\n\n\nShow solutions\n\n\n\\(H_0\\): Die is fair. \\(H_1\\): Die is not fair.\nExpected: \\(120/6 = 20\\) for each face.\nWe find the test statistic. \\[\n\\chi^2 =\\frac{100+36+1+16+25+64}{20} =\\frac{242}{20}=12.1\n\\] We can easily find that \\(df=6-1=5\\)\n\nNow we complete the rest of the computation with Python or with a table.\n\nfrom scipy import stats\nimport numpy as np\n## Critical value at alpha=0.05 and df=5\nprint(\"Critical value: \", stats.chi2.ppf(0.95, df = 5))\n\nCritical value:  11.070497693516351\n\n## p-value\nprint(\"p-value: \", 1-stats.chi2.cdf(12.1, df = 5))\n\np-value:  0.033442946211414415\n\n## alternatively we can use the chisqtest in r\ndata=np.array([10, 14, 19, 24, 25, 28])\nexpected = np.ones(6)/6*120\n\nchi2, pvalue = stats.chisquare(f_obs=data, f_exp = expected)\n\nprint(\"Chi-squared statistic:\", chi2)\n\nChi-squared statistic: 12.099999999999998\n\nprint(\"p-value:\", pvalue)\n\np-value: 0.033442946211414415\n\nprint(\"Expected frequencies:\\n\", expected)\n\nExpected frequencies:\n [20. 20. 20. 20. 20. 20.]\n\n\nWith a p-value of about 3%, we can reject \\(H_0\\) at \\(\\alpha=0.05\\)\n\nSince we reject \\(H_0\\), we can conlcude that the dice is likely unfair. BY looking at our data, the die really seems to favour higher numbers. If \\(\\alpha\\) were to be heightened, then we would more easily be convinced that teh dice is unfair, i.e., the result would not need to be as skewed as shown above. And vice versa of course.\n\n\n\n\n\nUse Python to simulate a 3x2 table, given by the code below, and conduct a chi-squared test. Interpret the results. Use \\(\\alpha=0.05\\) as the significance level.\n\nnp.random.seed(1)\n# Create 3x2 matrix of random integers from 1 to 100\ndata = np.random.choice(np.arange(1, 101), \nsize=6, replace=False).reshape((3, 2))\nchi2, p, dof, expected = stats.chi2_contingency(data)\n# Output\nprint(\"Data:\\n\", data)\n\nData:\n [[81 85]\n [34 82]\n [94 18]]\n\nprint(\"Chi-squared statistic:\", chi2)\n\nChi-squared statistic: 70.32890811890222\n\nprint(\"Degrees of freedom:\", dof)\n\nDegrees of freedom: 2\n\nprint(\"p-value:\", p)\n\np-value: 5.348988237051145e-16\n\nprint(\"Expected frequencies:\\n\", expected)\n\nExpected frequencies:\n [[88.05583756 77.94416244]\n [61.53299492 54.46700508]\n [59.41116751 52.58883249]]\n\n\n\n\nShow solutions\n\nChi-squared test returns statistic, df, and p-value. Interpretation depends on p-value vs alpha. If our p-value is below that of 0.05, which is our current significance level, then we can reject the null hypothesis, \\(H_0\\).\n\n\n\n\n\nDescribe the distribution of a chi-squared variable with 4 degrees of freedom. (What are the mean and variance, can you find a general rule?).\n\nHint: When \\(Y\\sim\\mathcal{N}(\\mu, \\sigma^2)\\), we have that \\(E[Y^4]=3\\sigma^4\\)\n\nExplain how the chi-squared distribution arises from the standard normal distribution.\n\n\n\nShow solutions\n\n\nPositively skewed distribution, where \\(E[X]=\\mu=k\\), and \\(Var[X]=\\sigma^2= 2k\\). Let’s see how we can reach these. First, recall that if \\(X\\sim\\chi^2_k\\), then\n\n\\[\nX=\\sum^k_{i=1}Z^2_i\n\\] Where \\(Z_i\\sim iid \\mathcal{N}(0,1)\\)\nNow we can begin computing the expectation:\n\\[\nE[X]=E\\left[\\sum^k_{i=1}Z^2_i\\right]=\\sum^k_{i=1}E[Z_i^2]\n\\] Now, we can be a bit clever here. Since \\(Z\\sim\\mathcal{N}(0,1)\\), and \\(Var[Y]=E[Y^2]-(E[Y])^2\\), we get\n\\[\nE[Z^2]=Var[Z]+(E[Z])^2 \\quad \\Rightarrow \\quad E[Z^2]=1+0=1\n\\] This lets us fully compute the expected value for \\(X\\). We use the fact that we have identical distribution to simpify the notation.\n\\[\nE[X]=\\cdots=kE[Z^2]=k*1=k\n\\] Now we can do the same procedure with the Variance. Recall, since \\(Z_1,...,Z_k\\) are independent, we have that the variance of the sum is the sum of the variances.\n\\[\nVar[X]=Var\\left[\\sum^k_{i=1}Z^2_i\\right]=\\sum^k_{i=1}Var[Z_i^2]\n\\] Now, we again use the ralationship that \\(Var[Y]=E[Y^2]-(E[Y])^2\\). We already know that \\(E[Z^2]=1 \\Rightarrow (E[Z^2])^2=1\\). Now we need only determine \\(E[(Z^2)^2]=E[Z^4]\\). Using the hint, we have that \\(E[Z^4]=3\\sigma^4=3*1^2=3\\), and we then get:\n\\[\nVar[Z^2]=3-1=2\n\\] Now we use that we have k identical distributions to get the variance of \\(X\\) \\[\nVar[X]=\\cdots=kVar[Z^2]=k*2=2k\n\\] Since we know that \\(k=4\\), we can then conclude on the expected value and variance of \\(X\\)\n\\[\nE[X]=k=4, \\quad Var[X]=2*k=2*4=8\n\\]”\n\nThe chi-sqaured distribution, as we’ve just looked at in depth, comes from the sum of squares of independent standard normal variables.\n\n\n\n\n\nA researcher models expected frequencies of customer types and compares to observed values. Expected: \\(\\{40, 35, 25\\}\\), Observed: \\(\\{38, 30, 32\\}\\) Conduct a goodness-of-fit test by hand.\n\n\nShow solutions\n\n\\[\n\\chi^2 = \\frac{(38-40)^2}{40} + \\frac{(30-35)^2}{35} + \\frac{(32-25)^2}{25} = 0.1 + 0.714 + 1.96 ≈ 2.53\n\\] \\(df=3-1=2\\)\nAt a significance level of \\(\\alpha=0.05\\) we then get a critical value of 5.99, which is higher than the test statistic. I.e. it seems the data fits the distribution well enough.\n\n\n\n\nUniversity admits 30% of male applicants and 25% of female applicants overall. In Engineering, 90% of women and 70% of men were accepted. In Arts, 10% of women and 5% of men were accepted.\n\nHow could this occur.\nExplain how aggregated vs. disaggregated data leads to paradox.\n\n\n\nShow soltuions\n\n\nIf way more female students were to apply to arts than to engineering, the total acceptance rate of women would decrease drastically. And if for instance, only 20 men applied to arts, and just one got accepted, then this would hardly have any effect if the engineering department was popular among men.\nAggregation can mask subgroup differences. Simpson’s paradox highlights lurking confounders.\n\n\n\n\n\nUse Python to simulate two groups and plot overall vs subgroup relationships. Create a scenario in which Simpson’s paradox would apply. The script underneath gives an idea for two simulated groups where Simpson’s paradox could apply.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set seed\nnp.random.seed(42)\n\n# Create data\ngroup = np.repeat(['A', 'B'], 100)\nx = np.concatenate([np.random.normal(3, 1, 100), np.random.normal(7, 1, 100)])\ny = np.concatenate([\n    2 * x[:100] + np.random.normal(size=100),\n    0.5 * x[100:] + np.random.normal(size=100)\n])\ndata = pd.DataFrame({'x': x, 'y': y, 'group': group})\n\n# Plot\nplt.figure(figsize=(8, 6));\nsns.scatterplot(data=data, x='x', y='y', hue='group');\nsns.regplot(data=data[data['group'] == 'A'], x='x', y='y', scatter=False, label=None);\nsns.regplot(data=data[data['group'] == 'B'], x='x', y='y', scatter=False, label=None);\nsns.regplot(data=data, x='x', y='y', scatter=False, color='black',  line_kws={\"linestyle\": \"--\"});\n\nplt.title('Scatterplot with Group-specific and Pooled Regression Lines');\nplt.grid(True); \nplt.show();\n\n\n\n\n\n\n\n\n\n\nShow solutions\n\nExample: Drug Effectiveness Study In a clinical trial, Drug A appears less effective than Drug B overall:\nDrug A: 60% recovery\nDrug B: 70% recovery\nBut when you break it down by age group:\nUnder 50:\nDrug A: 90%\nDrug B: 80%\nOver 50:\nDrug A: 30%\nDrug B: 20%\nDespite being better in both age groups, Drug A looks worse overall because more older, high-risk patients were given Drug A — a classic Simpson’s Paradox.\n\n\n\n\n\nExplain difference between correlation and covariance.\nInterpret \\(r = -0.8\\).\n\n\n\nShow solutions\n\n\nCovariance is is not is not constrained by any upper or lower bound, whereas correlation is a normalized version bounded on \\([−1,1]\\), which show how close two variables are to be “perfectly related”.\n\\(r=-0.8\\) would indicate a strong negative linear raltionship between two variables. If you were to plot their points in a two-dimensional plain they would be quite close to creating a neat line.\n\nsee plot below\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set seed\nnp.random.seed(123)\n\n# Define mean and covariance matrix\nmu = [0, 0]\nSigma = [[1, -0.8],\n         [-0.8, 1]]\n\n# Generate multivariate normal data\ndata = np.random.multivariate_normal(mu, Sigma, size=200)\ndf = pd.DataFrame(data, columns=['x', 'y'])\n\n# Check correlation\nprint(\"Correlation:\", df['x'].corr(df['y']))\n\nCorrelation: -0.7974792951296004\n\n# Plot\nplt.figure(figsize=(8, 6));\nsns.scatterplot(data=df, x='x', y='y', alpha=0.6);\nsns.regplot(data=df, x='x', y='y', scatter=False, color='blue');\nplt.title(\"Scatterplot of Two Variables with Correlation ≈ -0.8\");\nplt.grid(True);\nplt.tight_layout();\nplt.show();\n\n\n\n\n\n\n\n\n\n\n\n\nGiven pairs: (1, 2), (2, 3), (3, 5)\n\nCompute sample means.\nCompute covariance and correlation.\n\n\n\nShow solutions\n\n\nWe call our variables \\(X\\) and \\(Y\\) respectively.\n\n\\[ \\mu_X=\\frac{1+2+3}{3}=2, \\quad \\mu_Y=\\frac{2+3+5}{3}=\\frac{10}{3} \\]\n\nNow to compute covariance (\\(Cov(X,Y)\\)) and correlation (\\(r_{XY}\\) or \\(\\rho_{XY}\\)) We have that \\[\nCov(X,Y)=\\frac{1}{n-1}\\sum_{i=1}^n(x_i-\\bar{x})^2(y_i-\\bar{y})^2, \\quad r_{XY}=\\frac{Cov(X,Y)}{s_Xs_Y}\n\\] Where \\(s\\) are respective standard deviations.\n\n\\[\nCov(X,Y)=\\frac{(1-2)(2-3)+(2-2)(3-3)+(3-2)(5-3)}{2}=\\frac{1+2}{2}=\\frac{3}{2}\n\\] Now we need the respective standard deviations, which turn out to be, after not much work\n\\[\ns_X= 1, \\quad s_Y=\\sqrt{14/6}=\\sqrt{7/3}\n\\] Finally, we can compute \\(r_{XY}\\)\n\\[\nr_{XY}=\\frac{3/2}{\\sqrt{7/3}}=\\approx 0.98\n\\]\n\n\n\n\nSimulate two variables and visualize correlation. What kind of correlation do you see? (e.g. Weak negative, strong positive). Example below.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Simulate data\nnp.random.seed(42)  # Optional: for reproducibility\nx = np.random.normal(size=100)\ny = 2 * x + np.random.normal(size=100)\n\n# Plot;\nplt.scatter(x, y);\nplt.title(\"Scatterplot of x vs y\");\nplt.xlabel(\"x\");\nplt.ylabel(\"y\");\nplt.grid(True);\nplt.show();\n\n\n\n\n\n\n\n\n\n\nShow solutions\n\nWith the simulation above you should get a very strong positive correlation between X and Y. You can see this visually in that it would be very easy to draw a positive linear relationship between them that seems representative for the points given.\n\n\n\n\nAssuming we have two normally distributed random variables \\(X\\) and \\(Y\\), we can use a variant of a t-test to test for their correlation. The test statistic is given by:\n\\[\nT_r=\\frac{r\\sqrt{n-2}}{\\sqrt{1-r^2}}\n\\] Our null hypothesis is given by \\(H_0: \\ r=0\\). The test statistic will follow a t-distribution with \\(df=n-2\\)\n\n\n\nA professor collects data on 12 students to investigate whether time spent studying is correlated with their exam scores. The data are:\n\n\n\nStudy Hours (X)\nExam Score (Y)\n\n\n\n\n2\n65\n\n\n4\n70\n\n\n3\n68\n\n\n5\n75\n\n\n1\n60\n\n\n6\n80\n\n\n4\n72\n\n\n2\n66\n\n\n5\n78\n\n\n3\n67\n\n\n6\n82\n\n\n7\n85\n\n\n\n\nCompute the sample correlation coefficient \\(r\\).\nTest the null hypothesis \\(H_0: \\rho = 0\\) at the 5% significance level.\nInterpret the result.\n\n\n\nShow solutions\n\n\n\n\nYou can use Python, or work by hand similarly to problem 13:\n\nimport numpy as np\nx = np.array([2,4,3,5,1,6,4,2,5,3,6,7])\ny = np.array([65,70,68,75,60,80,72,66,78,67,82,85])\nr = np.corrcoef(x, y)[0, 1]\nprint(r)\n\n0.9857360065184915\n\n\nThis yields \\(r \\approx 0.986\\)\n\n\n\nUse the test statistic:\n\\[\nT_r = \\frac{r \\sqrt{n - 2}}{\\sqrt{1 - r^2}}\n\\]\n\\[\nt = \\frac{0.986 \\sqrt{12 - 2}}{\\sqrt{1 - 0.986^2}} = \\frac{0.986 \\cdot \\sqrt{10}}{\\sqrt{1 - 0.972}} \\approx \\frac{0.986 \\cdot 3.162}{\\sqrt{0.028}} \\approx \\frac{3.118}{0.167} \\approx 18.67\n\\]\nDegrees of freedom = \\(n - 2 = 10\\)\nLook up the critical value for \\(t_{0.025, 10} \\approx 2.228\\)\nSince \\(t = 15.9 \\gg 2.228\\), we reject the null hypothesis.\nFor good measure, lets double check the critical and p-values in R.\n\n## crtical value (this test is two sided, so at a 5 percent significance level we look at the quantiles for 0.025 and 0.975)\nprint(\"critical value: \", stats.t.ppf(0.975, df=10))\n\ncritical value:  2.2281388519649385\n\nprint(\"p-value: \", 1-stats.t.cdf(18.67, df=10))\n\np-value:  2.1004691319603808e-09\n\n\nWe VERY clearly reject the null hypothesis here. This should not be surprising, as a) gave us a VERY strong positive correlation.\n\n\n\nThere is a statistically significant and strong positive correlation between study hours and exam score. The data support the hypothesis that more study time is associated with higher performance.\n\n\n\n\n\nGive two examples where correlation does not imply causation.\nWhy are randomized experiments used?\n\n\n\nShow solutions\n\n\nReasonable examples:\n\nIce cream sales and drowning could very easily seem positively correlated. When ice cream sales go up we see more drowning, but does that mean ice cream sales lead to drowning? Seems unlikely. Let’s consider, when, both are at their peak. In the summer more people go swimming, and as such more accidents can happen, and this also happens to be the exact time of year when ice cream sales peak as well, due to heat. The scorching sun leads to higher ice cream sales and far more bathing (and consequently accidents).\n\n\n\nShoe size and reading ability, is probably positively correlated. This may not seem obvious, but consider the lower extreme. The ones that have the smallest shoes would be infants, who don’t yet have the ability to read, and at the other end you will have full grown adults that more than likely can read well. It seems far fetched to think that the size of your feet would determine your reading ability, but your age determining both your shoe size and your reading ability could veyr well be the case.\n\nThese effects on both the exposure and the outcome, we call confounders.\n\nRandomization eliminates selection bias and confounding.\n\n\n\n\n\nDescribe a scenario where a confounding variable could lead to misleading correlation.\n\n\nShow solutions\n\nExample: You may find yourself regressing prevalence of heart disease on coffee consumption, and seem to get a very strong connection. You, however, haven’t controlled for smoking habits. Without controlling for smoking, all the effects of smoking would be attempted explained by coffee-consumption, which could lead to it seeming like a far more serious contributor than what we see in practice.\n\n\n\n\nExplain the importance of random assignment in identifying causal relationships.\n\n\nShow solution\n\nRandom assignment in experiments let’s us account for both the confounders we are aware of and those we aren’t. When we randomly select enough data points we will find that known and unknown confounders will distribute more evenly across groups. Essentially we can more effectively isolate the relationship between the dependent and the independent variable in a regression.\n\n\n\n\nGiven: \\(X = (1, 2, 3)\\) and \\(Y = (2, 2.5, 4).\\)\n\nEstimate intercept and slope.\nPredict Y when X = 4.\n\n\n\nShow solutions\n\n\nWe essentially treat the best fit line in a linear regression like we would any linear function, where we have a set formula for the slope, \\(\\hat{\\beta}_1=\\frac{\\hat{Cov(X,Y)}}{S^2_X}\\), and \\(\\hat{\\beta}_0=\\bar{Y}-\\hat{\\beta}_1\\bar{X}\\), and gives the intercept.\n\nWe can start by finding the averages and for \\(X\\) and \\(Y\\), before we compute variance and covariacne.\n\\[\n\\begin{align*}\n\\bar{X}=\\frac{1+2+3}{3}=2, \\quad \\bar{Y}=\\frac{2+5/2+4}{3}=\\frac{17}{6} \\\\\nS^2_X=\\frac{1}{2}\\left((1-2)^2+(2-2)^2+(3-2)^2\\right)=1 \\\\\n\\widehat{Cov}(X,Y)=\\frac{1}{2}(1*\\frac{5}{6}+0+1*\\frac{7}{6})=\\frac{1}{2}\\frac{12}{6}=1\n\\end{align*}\n\\] Now we can compute \\(\\hat{\\beta_1}\\) and \\(\\hat{\\beta_0}\\)\n\\[\n\\hat{\\beta_1}=\\frac{1}{1}=1, \\quad \\hat{\\beta_0}=\\frac{17}{6}-1*2=\\frac{5}{6}\n\\] And with that we have estimates for the intercept, 5/6, and slope, 1.\n\nGiven our estimated model we have\n\n\\[\nY_i=\\frac{5}{6}+X_i+\\varepsilon\n\\] What we would like now is the conditional expected value of \\(Y\\) when \\(X=4\\). Recall recall that \\(E[\\varepsilon]=0\\), and \\(\\varepsilon\\) is also independent of \\(X\\), meaning \\(E[\\varepsilon|X]=E[\\varepsilon]\\)\nWe then get\n\\[\nE[Y|X=4]=E\\left[\\frac{5}{6}+X_i+\\varepsilon|X=4\\right]=\\frac{5}{6}+E[X_i|X=4]+E[\\varepsilon]=\\frac{5}{6}+4=\\frac{29}{6}\n\\]\n\n\n\n\nYou fit a linear model (it doesn’t really matter in this case what you are looking at, just think of it as a simple linear regression):\n\nimport pandas as pd\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\n# Load the mtcars dataset\nmtcars = sm.datasets.get_rdataset(\"mtcars\", \"datasets\").data\n# Fit the linear model\nmodel = smf.ols('mpg ~ wt', data=mtcars).fit()\n\n# Augment the model with predictions and residuals\naugmented = mtcars.copy()\naugmented['fitted'] = model.fittedvalues\naugmented['residuals'] = model.resid\n\n# Print the model summary\nprint(model.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                    mpg   R-squared:                       0.753\nModel:                            OLS   Adj. R-squared:                  0.745\nMethod:                 Least Squares   F-statistic:                     91.38\nDate:                Fri, 20 Feb 2026   Prob (F-statistic):           1.29e-10\nTime:                        22:23:17   Log-Likelihood:                -80.015\nNo. Observations:                  32   AIC:                             164.0\nDf Residuals:                      30   BIC:                             167.0\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     37.2851      1.878     19.858      0.000      33.450      41.120\nwt            -5.3445      0.559     -9.559      0.000      -6.486      -4.203\n==============================================================================\nOmnibus:                        2.988   Durbin-Watson:                   1.252\nProb(Omnibus):                  0.225   Jarque-Bera (JB):                2.399\nSkew:                           0.668   Prob(JB):                        0.301\nKurtosis:                       2.877   Cond. No.                         12.7\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nCreate the following diagnostic plots using Python. Underneath I have done so using ggplot.\n\n\nResiduals vs Fitted Values\nNormal Q-Q Plot\nHistogram of Residuals\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\n\n# Residuals vs fitted:\nplt.figure();\nsns.scatterplot(x='fitted', y='residuals', data=augmented);\nplt.axhline(0, color='red', linestyle='dashed');\nplt.title('Residuals vs Fitted');\nplt.xlabel('Fitted Values');\nplt.ylabel('Residuals');\nplt.grid(True);\nplt.show();\n\n\n\n\n\n\n\n# QQ-plot\nplt.figure()\nstats.probplot(augmented['residuals'], dist=\"norm\", plot=plt);\nplt.title('Normal Q-Q Plot');\nplt.xlabel('Theoretical Quantiles');\nplt.ylabel('Standardized Residuals');\nplt.grid(True);\nplt.show();\n\n\n\n\n\n\n\n# histogram:\nplt.figure()\nsns.histplot(augmented['residuals'], bins=10, kde=False, color='steelblue', edgecolor='white');\nplt.title('Histogram of Residuals');\nplt.xlabel('Residuals');\nplt.ylabel('Count');\nplt.grid(True);\nplt.show();\n\n\n\n\n\n\n\n\n\nFor each plot, describe which regression assumption it assesses, and what pattern would indicate the assumption holds.\nBased on the plots, assess whether the assumptions of linear regression are likely satisfied in this model.\nGive your thoughts on the model.\n\n\n\nShow solutions\n\n\ndo as above\n\n\n\nResiduals vs Fitted: Tests for linearity and equal variance. You should see no clear pattern — just a random scatter.\nNormal Q-Q Plot: Assesses normality of residuals. Points should lie along the diagonal line if residuals are normal.\nHistogram: Another way to check residual normality. A roughly bell-shaped, symmetric distribution supports the assumption.\n\n\n\n\n\nThe residuals in plot (1) are randomly scattered around 0 without fan shape → linearity and homoscedasticity are likely met.\nQ-Q plot in (2) shows points close to the line → residuals are approximately normal, though it seems that the tails may a be abbit heavy.\nHistogram in (3) is roughly symmetric and bell-shaped → supports normality assumption, but we also see the same indication asnin teh QQ plot, that there are heavy tail tendencies.\n\nWe don’t have that many points that are used in our regression, but they stil give some idea.\nIf any of these show clear violations (e.g., funnel shape, skewed histogram), mention the likely issue and suggest remedies like transformation or robust methods.\n\nFrom the summary it seems very clear that the intercept and slope are statistcally significant, whihc is cause to suspect a high correlation between the X and Y variables. We also find an R-squared of about 75%, which indicated that the variation in X explaines 75% of teh variation in Y. Generally, this seems pretty alrigth at this point, especially after looking at the residuals and finding that they seem to indicate our assumptions of linearity homoskedasticity and normality being fulfilled.\n\n\n\n\n\nSimulate and fit the following model in R:\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\n\n# Simulate data\nnp.random.seed(0)\nx = np.arange(1, 101)\ny = 3 * x + np.random.normal(scale=x / 5, size=100)\n\n# Create DataFrame\ndf = pd.DataFrame({'x': x, 'y': y})\n\n# Fit linear model\nX = sm.add_constant(df['x'])  # Add intercept\nmodel = sm.OLS(df['y'], X).fit()\n\n# Print summary\nprint(model.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.985\nModel:                            OLS   Adj. R-squared:                  0.985\nMethod:                 Least Squares   F-statistic:                     6614.\nDate:                Fri, 20 Feb 2026   Prob (F-statistic):           9.19e-92\nTime:                        22:23:18   Log-Likelihood:                -379.15\nNo. Observations:                 100   AIC:                             762.3\nDf Residuals:                      98   BIC:                             767.5\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         -2.5259      2.183     -1.157      0.250      -6.858       1.806\nx              3.0521      0.038     81.325      0.000       2.978       3.127\n==============================================================================\nOmnibus:                        3.233   Durbin-Watson:                   1.969\nProb(Omnibus):                  0.199   Jarque-Bera (JB):                3.079\nSkew:                           0.159   Prob(JB):                        0.215\nKurtosis:                       3.799   Cond. No.                         117.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n# Residuals vs Fitted plot\nfitted = model.fittedvalues\nresiduals = model.resid\n\nplt.scatter(fitted, residuals);\nplt.axhline(0, color='red', linestyle='dashed');\nplt.xlabel('Fitted Values');\nplt.ylabel('Residuals');\nplt.title('Residuals vs Fitted');\nplt.grid(True);\nplt.show();\n\n\n\n\n\n\n\n\n\nWhat does the residual plot suggest about the assumptions?\nWhy is this a problem for inference?\nSuggest a fix.\n\n\n\nShow solutions\n\n\nResidual variance clearly increases with fitted values, which is a classic sign of heteroskedasticity.\nStandard errors may be incorrect, leading to misleading confidence intervals and p-values, which again can lead a researcher to wild or misleading conclusions.\nUse log(y), square-root(y), or weighted least squares to stabilize variance.\n\n\n\n\n\nYou model test scores using study hours and gender:\n\nimport pandas as pd\nimport statsmodels.formula.api as smf\n\n# Create the DataFrame\ndf = pd.DataFrame({\n    'score': [80, 85, 70, 90],\n    'hours': [4, 5, 3, 6],\n    'female': pd.Categorical([0, 1, 0, 1])  # treated as a categorical variable\n})\n\n# Fit the linear model\nmodel = smf.ols('score ~ hours + C(female)', data=df).fit()\n\n# Print the model summary\nprint(model.summary())\n\nC:\\Users\\s15052\\ONEDRI~3\\DOKUME~1\\VIRTUA~1\\R-RETI~1\\Lib\\site-packages\\statsmodels\\stats\\stattools.py:74: ValueWarning: omni_normtest is not valid with less than 8 observations; 4 samples were given.\n  warn(\"omni_normtest is not valid with less than 8 observations; %i \"\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  score   R-squared:                       0.971\nModel:                            OLS   Adj. R-squared:                  0.914\nMethod:                 Least Squares   F-statistic:                     17.00\nDate:                Fri, 20 Feb 2026   Prob (F-statistic):              0.169\nTime:                        22:23:18   Log-Likelihood:                -6.5683\nNo. Observations:                   4   AIC:                             19.14\nDf Residuals:                       1   BIC:                             17.30\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==================================================================================\n                     coef    std err          t      P&gt;|t|      [0.025      0.975]\n----------------------------------------------------------------------------------\nIntercept         48.7500      8.927      5.461      0.115     -64.676     162.176\nC(female)[T.1]    -2.5000      5.590     -0.447      0.732     -73.530      68.530\nhours              7.5000      2.500      3.000      0.205     -24.266      39.266\n==============================================================================\nOmnibus:                          nan   Durbin-Watson:                   1.000\nProb(Omnibus):                    nan   Jarque-Bera (JB):                0.667\nSkew:                           0.000   Prob(JB):                        0.717\nKurtosis:                       1.000   Cond. No.                         39.9\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nNote that with so few observations, Python will not calculate the Omnibus test.\n\nExplain the meaning of each coefficient.\nWhat is the baseline category for gender, and how is the effect of gender represented?\n\n\n\nShow solutions\n\n\n\n\n\nIntercept: Estimated score for a male who studied 0 hours.\nhours: Change in score per additional hour studied.\nfemale: Difference in score between females and males, controlling for hours.\n\n\nThe baseline is male. Female is the additional effect for females compared to males, controlling also for the amount of hours spent.\n\nIf we were to interpret this model, it would seem being a female had a negative effect, but if we look closer at the data we may see why. Notice that the females did better than the males, but that they also spent more hours studying. The regression tells us that we dont have very conclusive data here, but one could suspect that the first few hours of studying would grant you more of a point increase than the next few, i.e. a diminishing return.\n\n\n\n\nYou have a fitted model: \\(y = 10 + 2x\\) with residual standard error of 3.\n\nWhat is the difference between a 95% confidence interval and a 95% prediction interval for a given x-value?\nWhich interval is wider and why?\nWrite expressions for both intervals at x = 5.\n\n\n\nShow solutions\n\n\nCI estimates the mean value of \\(y\\) at a given \\(x\\), at some range of values. PI estimates the value of a new individual at that \\(x\\).\nThe PI is wider because it includes variability both from the estimate of the mean and from individual variation.\n\n\n\nCI: \\(\\hat{y} \\pm t \\cdot SE_{\\text{mean}}\\)\nPI: \\(\\hat{y} \\pm t \\cdot \\sqrt{SE_{\\text{mean}}^2 + \\sigma^2}\\)\n\nAssuming \\(x = 5\\): \\(\\hat{y} = 10 + 2(5) = 20\\)\nNow we could theoretically use estimated SE and residual error to calculate intervals.\n\n\n\n\nA company fits a logistic regression model to understand how the price of a product affects the likelihood that a customer will purchase it. The model output is:\n\\[\n\\log\\left(\\frac{p}{1 - p}\\right) = 3 - 0.5 \\times \\text{price}\n\\]\nwhere \\(p\\) is the probability that a customer buys the product.\n\nInterpret the coefficient of \\(price\\) in context.\nWhat is the odds ratio associated with a 1-unit increase in price?\nCalculate the predicted probability that a customer will buy the product when the price is $10.\nSketch (or describe) what the logistic curve would look like based on this model.\nBased on the model, what would happen to the predicted probability of purchase as the price increases toward $20?\n\n\n\nShow solutions\n\n\nThe coefficient of -0.5 means that for every 1-unit increase in price, the log-odds of purchase decreases by 0.5. In other words, as price increases, the probability of purchase decreases.\nThe odds ratio is \\(e^{-0.5} \\approx 0.61\\). This means the odds of purchasing are multiplied by 0.61 for each $1 increase in price.\nAt price = 10:\n\n\\[\n\\log\\left(\\frac{p}{1 - p}\\right) = 3 - 0.5 \\times 10 = -2 \\Rightarrow p = \\frac{1}{1 + e^{2}} \\approx 0.12\n\\]\n\nThe logistic curve is an S-shaped curve that starts near 1 when price is low (e.g., close to $0), and decreases smoothly toward 0 as price increases.\nAs the price increases toward 20 USD, the exponent in the model becomes more negative, and the probability \\(p\\) approaches 0. Customers are highly unlikely to purchase at very high prices under this model.\n\n\n\n\n\nA company wants to understand how the price of a product affects the probability that a customer will purchase it. The following dataset shows simulated purchase behavior for 100 customers, where \\(buy = 1\\) means the customer made a purchase and \\(buy = 0\\) otherwise.\n\nFit a logistic regression model using \\(buy ~ price\\).\n\n\nimport numpy as np\nimport pandas as pd\n\n# Set seed for reproducibility\nnp.random.seed(42)\n\n# Generate data\nprice = np.random.uniform(1, 20, 100)\nlog_odds = -0.5 * price + 3\nprob_buy = 1 / (1 + np.exp(-log_odds))\nbuy = np.random.binomial(1, prob_buy)\n\n# Create DataFrame\ndf = pd.DataFrame({'price': price, 'buy': buy})\n\n\nPredict the probability of purchase when the price is $10.\nPlot the logistic regression curve and observed data points.\n\n\n\nShow solutions\n\n\n\n\n\nimport statsmodels.formula.api as smf\n# Fit logistic regression model\nmodel = smf.glm(formula='buy ~ price', data=df,\n                family=sm.families.Binomial()).fit()\nprint(model.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                    buy   No. Observations:                  100\nModel:                            GLM   Df Residuals:                       98\nModel Family:                Binomial   Df Model:                            1\nLink Function:                  Logit   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -35.867\nDate:                Fri, 20 Feb 2026   Deviance:                       71.734\nTime:                        22:23:18   Pearson chi2:                     72.1\nNo. Iterations:                     6   Pseudo R-squ. (CS):             0.4151\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      2.6312      0.640      4.111      0.000       1.377       3.886\nprice         -0.4299      0.087     -4.922      0.000      -0.601      -0.259\n==============================================================================\n\n\n\n\n\n\n# Predict probability of purchase at price = 10\nprediction = model.predict(pd.DataFrame({'price': [10]}))\nprint(prediction[0])\n\n0.15873234194196\n\n\n\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Plot points and logistic regression curve\nsns.scatterplot(x='price', y='buy', data=df, alpha=0.5);\n\n# Generate a smooth curve for predicted probabilities\nprice_range = np.linspace(df['price'].min(), df['price'].max(), 300);\nprob_curve = model.predict(pd.DataFrame({'price': price_range}));\n\nplt.plot(price_range, prob_curve, color='red', linewidth=1.2);\nplt.title('Logistic Regression: Purchase vs Price');\nplt.xlabel('Price');\nplt.ylabel('Probability of Purchase');\nplt.grid(True);\nplt.show();\n\n\n\n\n\n\n\n\nThe negative coefficient for price implies that as price increases, the likelihood of purchase decreases.\n\n\n\n\nA researcher is studying how the number of hours a student studies and whether they attend a prep course affect their exam score. They collect data from 10 students and fit the following model:\n\\[\n\\text{score} = 55 + 5 \\cdot \\text{hours} + 8 \\cdot \\text{prep}\n\\]\nWhere:\n\nhours = number of hours studied\nprep = 1 if the student attended a prep course, 0 otherwise\nscore = predicted exam score\n\n\nInterpret the coefficient for hours.\nInterpret the coefficient for prep.\nPredict the score of a student who studied for 4 hours and attended the prep course.\nPredict the score of a student who studied for 4 hours and did not attend the prep course.\n\n\n\nShow results\n\n\nFor every additional hour studied, the expected exam score increases by 5 points, holding prep course attendance constant.\nStudents who attended the prep course are predicted to score 8 points higher than those who did not, for the same number of study hours.\nFor a student with 4 hours and prep = 1:\n\n\\[\n\\text{score} = 55 + 5 \\cdot 4 + 8 \\cdot 1 = 55 + 20 + 8 = 83\n\\]\n\nFor a student with 4 hours and prep = 0:\n\n\\[\n\\text{score} = 55 + 5 \\cdot 4 + 8 \\cdot 0 = 55 + 20 = 75\n\\]\n\n\n\n\nA marketing analyst builds the following regression model to predict weekly sales (in $1000s) based on the amount spent on online ads and whether the store is in an urban location:\n\\[\n\\text{sales} = 20 + 3.5 \\cdot \\text{ad-spend} + 7 \\cdot \\text{urban}\n\\]\nWhere:\n\nad-spend = amount spent on online advertising in $1000s\nurban = 1 if the store is in an urban location, 0 otherwise\n\n\nInterpret the coefficient for ad_spend.\nInterpret the coefficient for urban.\nPredict sales for a store that spends $2000 on ads and is in an urban location.\nPredict sales for a store with the same ad spend but in a non-urban location.\n\n\n\nShow solutions\n\n\nEach additional $1000 in ad spending increases expected weekly sales by $3,500.\nUrban stores are expected to generate $7,000 more in sales, all else equal.\n\\(\\text{sales} = 20 + 3.5 \\cdot 2 + 7 \\cdot 1 = 20 + 7 + 7 = 34\\) → $34,000\n\\(\\text{sales} = 20 + 7 = 27\\) → $27,000\n\n\n\n\n\nYou are given a fitted model:\n\\[\n\\text{income} = 15000 + 2500 \\cdot \\text{education}\n\\]\nWhere education is years of schooling.\nYou receive student-generated residual plots: a histogram of residuals, a QQ plot, and a scatterplot of residuals vs fitted values.\n\nThe histogram is left-skewed. What might this indicate?\nThe residual vs fitted plot shows a funnel shape. What does that imply?\n\n\n\nShow solutions\n\n\nLeft-skewed histogram suggests residuals are not normally distributed, which in and of itself may not be a huge problem, but it at a certain magnitude a left skew also makes the distribution of residuals vary far from being symmetrical. Major left skewness can significantly affect the estimation by leading to a positive bias.\nFunnel shape implies heteroskedasticity; variance increases with predicted values.\n\n\n\n\n\nA model is fit to assess how advertising budget affects product demand:\n\\[\n\\text{demand} = 10 + 4 \\cdot \\text{budget} - 0.2 \\cdot \\text{budget}^2\n\\]\n\nInterpret the model.\nAt what budget is demand maximized?\nCompute expected demand when budget = 5.\nShould a linear or quadratic model be preferred here, and why?\n\nUnderneath is some r code for visualisation help\n\n\nShow solution\n\n\nInitial increases in budget raise demand, but at higher levels the negative squared term causes diminishing returns.\nVertex of the parabola: \\(x = \\frac{-b}{2a} = \\frac{-4}{2(-0.2)} = 10\\)\n\nWe treat this like any parabola.\n\\[\ndemand'=4-0.4\\cdot budget=0 \\quad \\Rightarrow \\quad budget=10\n\\] This is clearly a maximum when you consider that the squre term s negative.\n\n\\(\\text{demand} = 10 + 4(5) - 0.2(25) = 10 + 20 - 5 = 25\\)\nQuadratic is better if diminishing returns are evident in data (verified via residual plots or model comparison).\n\n\n# Simulate data\nimport numpy as np\nimport pandas as pd\n\nnp.random.seed(123)\nbudget = np.arange(0, 20.5, 0.5)\ndemand = 10 + 4 * budget - 0.2 * budget**2 + np.random.normal(0, 1, len(budget))\ndata = pd.DataFrame({'budget': budget, 'demand': demand})\n\nimport statsmodels.formula.api as smf\n\n# Linear model\nmodel0 = smf.ols('demand ~ budget', data=data).fit()\nprint(\"Linear Model Summary:\")\n\nLinear Model Summary:\n\nprint(model0.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 demand   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.025\nMethod:                 Least Squares   F-statistic:                   0.01279\nDate:                Fri, 20 Feb 2026   Prob (F-statistic):              0.911\nTime:                        22:23:19   Log-Likelihood:                -135.36\nNo. Observations:                  41   AIC:                             274.7\nDf Residuals:                      39   BIC:                             278.1\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     23.1001      2.066     11.181      0.000      18.921      27.279\nbudget        -0.0201      0.178     -0.113      0.911      -0.380       0.340\n==============================================================================\nOmnibus:                        4.299   Durbin-Watson:                   0.093\nProb(Omnibus):                  0.117   Jarque-Bera (JB):                2.974\nSkew:                          -0.495   Prob(JB):                        0.226\nKurtosis:                       2.128   Cond. No.                         22.9\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n# Quadratic model\nmodel1 = smf.ols('demand ~ budget + I(budget**2)', data=data).fit()\nprint(\"\\nQuadratic Model Summary:\")\n\n\nQuadratic Model Summary:\n\nprint(model1.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 demand   R-squared:                       0.970\nModel:                            OLS   Adj. R-squared:                  0.969\nMethod:                 Least Squares   F-statistic:                     622.6\nDate:                Fri, 20 Feb 2026   Prob (F-statistic):           9.08e-30\nTime:                        22:23:19   Log-Likelihood:                -63.211\nNo. Observations:                  41   AIC:                             132.4\nDf Residuals:                      38   BIC:                             137.6\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==================================================================================\n                     coef    std err          t      P&gt;|t|      [0.025      0.975]\n----------------------------------------------------------------------------------\nIntercept          9.6512      0.524     18.402      0.000       8.589      10.713\nbudget             4.1180      0.121     33.945      0.000       3.872       4.364\nI(budget ** 2)    -0.2069      0.006    -35.282      0.000      -0.219      -0.195\n==============================================================================\nOmnibus:                        0.548   Durbin-Watson:                   1.979\nProb(Omnibus):                  0.760   Jarque-Bera (JB):                0.663\nSkew:                          -0.129   Prob(JB):                        0.718\nKurtosis:                       2.432   Cond. No.                         532.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n# Visualize both models\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Linear model plot\nsns.lmplot(x='budget', y='demand', data=data,\n           line_kws={\"color\": \"blue\"},\n           scatter_kws={\"s\": 40, \"alpha\": 0.6},\n           ci=None);\nplt.title('Linear Model: Demand vs Budget');\nplt.xlabel('Budget');\nplt.ylabel('Demand');\nplt.show();\n\n\n\n\n\n\n\n# Quadratic model plot\n# Need to manually generate the fitted curve\ndata['budget_squared'] = data['budget'] ** 2\ndata['fitted_quad'] = model1.predict(data[['budget', 'budget_squared']])\n\nplt.figure();\nplt.scatter(data['budget'], data['demand'], alpha=0.6);\nplt.plot(data['budget'], data['fitted_quad'], color='red');\nplt.title('Quadratic Model: Demand vs Budget');\nplt.xlabel('Budget');\nplt.ylabel('Demand');\nplt.grid(True);\nplt.show();\n\n\n\n\n\n\n\n\n\n\n\n\nYou fit a model and check these diagnostics:\n\nResiduals show non-linear pattern in residuals vs fitted plot.\nHistogram of residuals is roughly normal.\nResiduals increase in spread with fitted values.\n\n\nWhich regression assumptions are violated?\nWhat could be done to address these?\nWould transforming variables help? Explain.\n\n\n\nShow solutions\n\n\nLinearity and homoscedasticity assumptions are violated.\nConsider polynomial terms for non-linearity, and weighted least squares or transformations for heteroskedasticity.\nYes log-transforming the response or predictors can stabilize variance or linearize relationships.\n\n\n\n\n\nYou build two models:\n\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\n# Simulate data\nnp.random.seed(123)\nn = 100\nx = np.random.normal(size=n)\ny = 3 * x + np.random.normal(size=n)\ndata = pd.DataFrame({'x': x, 'y': y})\n\nimport statsmodels.formula.api as smf\n\n# Simple linear model A\nmodel_a = smf.ols('y ~ x', data=data).fit()\nprint(\"Model A: Simple Linear Regression\")\n\nModel A: Simple Linear Regression\n\nprint(model_a.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.923\nModel:                            OLS   Adj. R-squared:                  0.923\nMethod:                 Least Squares   F-statistic:                     1180.\nDate:                Fri, 20 Feb 2026   Prob (F-statistic):           1.84e-56\nTime:                        22:23:20   Log-Likelihood:                -138.83\nNo. Observations:                 100   AIC:                             281.7\nDf Residuals:                      98   BIC:                             286.9\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     -0.0191      0.098     -0.195      0.846      -0.214       0.175\nx              2.9834      0.087     34.357      0.000       2.811       3.156\n==============================================================================\nOmnibus:                        5.027   Durbin-Watson:                   1.860\nProb(Omnibus):                  0.081   Jarque-Bera (JB):                5.131\nSkew:                          -0.308   Prob(JB):                       0.0769\nKurtosis:                       3.924   Cond. No.                         1.13\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n# Model B: \n# Manually add polynomial terms up to degree 10\nfor degree in range(2, 11):\n    data[f'x^{degree}'] = data['x'] ** degree\n\n# Define predictor variables (x, x^2, ..., x^10)\nX = data[['x'] + [f'x^{i}' for i in range(2, 11)]]\nX = sm.add_constant(X)  # add intercept\n\n# Fit model\nmodel = sm.OLS(data['y'], X).fit()\n\n# Print summary\nprint(model.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.927\nModel:                            OLS   Adj. R-squared:                  0.919\nMethod:                 Least Squares   F-statistic:                     113.8\nDate:                Fri, 20 Feb 2026   Prob (F-statistic):           2.94e-46\nTime:                        22:23:20   Log-Likelihood:                -136.05\nNo. Observations:                 100   AIC:                             294.1\nDf Residuals:                      89   BIC:                             322.8\nDf Model:                          10                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          0.1046      0.233      0.448      0.655      -0.359       0.568\nx              3.1152      0.609      5.112      0.000       1.904       4.326\nx^2           -1.1750      1.118     -1.051      0.296      -3.396       1.046\nx^3            0.0469      1.210      0.039      0.969      -2.357       2.451\nx^4            1.5973      1.410      1.133      0.260      -1.204       4.398\nx^5           -0.1238      0.752     -0.165      0.870      -1.619       1.371\nx^6           -0.6898      0.655     -1.053      0.295      -1.991       0.612\nx^7            0.0312      0.178      0.175      0.861      -0.323       0.385\nx^8            0.1156      0.125      0.927      0.356      -0.132       0.363\nx^9           -0.0020      0.014     -0.139      0.890      -0.030       0.026\nx^10          -0.0065      0.008     -0.783      0.435      -0.023       0.010\n==============================================================================\nOmnibus:                        6.530   Durbin-Watson:                   1.946\nProb(Omnibus):                  0.038   Jarque-Bera (JB):                7.138\nSkew:                          -0.382   Prob(JB):                       0.0282\nKurtosis:                       4.062   Cond. No.                     6.89e+04\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 6.89e+04. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\n\nYou test both on new data and Model A performs better.\n\nWhy might Model A outperform B despite having fewer predictors?\nWhat is overfitting?\nHow does cross-validation help avoid it?\n\n\n\nShow solutions\n\n\nModel B may have overfit noise in training data; simpler model generalizes better. Model A also has an innate advantage in that we prefer simpler models. By studyin R-squared and adjusted R-squared in both A and B we can tell that teh additional vairables don’t really help explain the realtionship.\nOverfitting: model captures noise rather than signal, reducing generalizability. The danger here is that if you can perfectly explain a sample, you may have been unlucky, and found effects that aren’t necessarily present in the entire population, no matter how rpresentative the sample.\nCross-validation assesses performance on unseen data, discouraging overfitting.I.e., you validate a model by using it on more than just the data it was built on.\n\n\n\n\n\nA model is developed to predict house prices. Another researcher claims, “This model proves that number of bedrooms causes higher prices.”\n\nIs this a valid conclusion?\nWhat’s the difference between prediction and causal inference?\nUnder what conditions could we argue causation?\n\n\n\nShow solutions\n\n\nNo, correlation is not causation. Though the model sees that more bedrooms tend to be associated with higher prices, the model itself it is made as a forecast, it is not capable of making inferences on a causal relationship.\nPrediction focuses on accurate forecasts; explanation seeks underlying causal mechanisms.\nCausation requires random assignment or strong quasi-experimental design to eliminate confounding.\n\n\n\n\n\nGiven: \\(\\text{score} = 50 + 2 \\cdot \\text{study\\_hours}\\) You calculate:\n\n95% confidence interval for mean score at 6 hours: (60, 64)\n95% prediction interval for individual score: (55, 69)\n\n\nWhat do these intervals represent?\nWhy is the prediction interval wider?\nIf we wanted to reduce the width of both, what could help?\n\n\n\nShow solutions\n\n\nCI gives a range for mean score of all students studying 6 hours; PI gives range for a new individual’s score.\nPI includes both model uncertainty and residual variability.\nIncreasing sample size reduces standard errors, narrowing both intervals.\n\n\n\n\n\nYou compare two models:\n\nModel 1: \\(R^2=0.72\\)\nModel 2: \\(R^2=0.76\\)\n\n\nCan we conclude Model 2 is better?\nWhat’s a limitation of using R² alone?\nSuggest a better metric.\n\n\n\nShow solutions\n\n\nNot necessarily; depends on context and number of predictors. The R-squared is quite similar, so that on it’s own can’t really tel the whole story.\n\\(R^2\\) can never decrease when adding more predictors, and this can be misleading. In theory, one could add a thousand predictors, all of whom barely increase the \\(R^2\\), but will in the end only help to overfit the model.\nThere are different measures, like adjusted R-squared and AIC, both of whom punish adding explanatory variables that dont contribute.\n\n\n\n\n\nYou include dummy variables for 4 regions: North, South, East, West.\n\nWhat problem arises if all 4 are included in the regression model?\nHow should this be handled?\n\n\n\nShow solutions\n\n\nThe model will have what we call perfect colinearity, which leads to the model not being able to estimate parameters due to linear dependence.\nOmit one region as reference category; remaining coefficients are interpreted relative to it. E.g. we take North as the base region, now all other dummies will display coefficients that indicate a score relative to north as a basis. (This is not common practice, but one can also omit the intercept \\(\\beta_0\\), instead of a category)\n\n\n\n\n\nA regression includes both “income” and “wealth”, which are highly correlated.\n\nWhat is multicollinearity and how does it affect regression?\nHow can it be detected?\nWhat might you do to resolve it?\n\n\n\nShow solutions\n\n\nMulticollinearity inflates standard errors, making estimates unstable, and it comes from high correlation between several explanatory variables. Perfect multicolinearity comes into play when we have two or more perfectly correlated explanatory variables (height in feet and height in cm for instance)\nCheck variance inflation factors (VIF) or the correlation matirx.\nDropping one variable or combining both into one are common fixes.\n\n\n\n\n\nA regression of \\(Y\\) on \\(X\\) shows curvature. You try \\(log(X)\\):\n\\[\nY = \\beta_0 + \\beta_1 \\log(X)\n\\]\n\nWhy use this transformation?\nWhat does \\(\\beta_1\\) now represent?\nWhen might a log-log model be preferable?\n\n\n\nShow solutions\n\n\nTo linearise exponential-like relationships. In essence, we can create a line best-fit line that much more clearly fits our data.\nFor our model a roughly 1% positive change in \\(X\\) will be reflected as a \\(\\b_1\\) cahnge in \\(Y\\)\nWhen both \\(X\\) and \\(Y\\) span many orders of magnitude or show multiplicative effects, a log-log model may be appropriate.\n\n\n\n\n\nA model:\n\\[\n\\text{score} = 40 + 5 \\cdot \\text{study} + 10 \\cdot \\text{prep} + 4 \\cdot \\text{study} \\cdot \\text{prep}\n\\]\nWhere prep is 0 or 1.\n\nWhat does the interaction term mean?\nCompute predicted scores for 4 hours of study with and without prep.\nInterpret the effect of study hours with and without prep.\n\n\n\nShow solutions\n\n\nThe Interaction term tells us that the effect study hours will have on the score, will also be affected by whether or not the students chooses to participate in the prep course.\n\n\n\nWith prep: \\(40 + 5*4 + 10*1 + 4*4 = 40 + 20 + 10 + 16 = 86\\)\nWithout prep: \\(40 + 5*4 = 60\\)\n\n\nPrep amplifies study’s effect, each hour contributes 9 points instead of 5, meaning that the prep has a much greater effect on further studying in this model.\n\n\n\n\n\n\nResearchers collected data on the size of their feet for a group of right-handed male and females and counted how many of each sex had a left-foot larger than the right, equal feet or right foot larger than the left. The results are given in the table here:\n\n\n\nSex\nL&gt;R\nL=R\nL&lt;R\nTotal\n\n\n\n\nMen\n2\n10\n28\n40\n\n\nWomen\n55\n18\n14\n87\n\n\n\nDoes the data indicate that gender has a strong effect on the development of foot asymmetry? Specify the null- and alternative hypothesis, compute the \\(\\chi^2\\) test statistic and obtain the p-value.\n\n\nShow solutions\n\n$H_0: $ Gender and foot asymmetry are independent.\n$H_A: $ Gender and foot asymmetry are dependent.\n\n\n\nSex\nL&gt;R\nL=R\nL&lt;R\nTotal\n\n\n\n\nMen\n2\n10\n28\n40\n\n\nWomen\n55\n18\n14\n87\n\n\nTotal\n57\n28\n42\n127\n\n\n\nExpected values under the null hypothesis:\n\\(e_{ij} = \\frac{o_{\\cdot j}o_{i\\cdot}}{n}\\), e.g. \\(e_{22}= \\frac{28\\cdot 87}{127}=19.18\\).\n\n\n\nSex\nL&gt;R\nL=R\nL&lt;R\nTotal\n\n\n\n\nMen\n17.95\n8.82\n13.23\n40\n\n\nWomen\n39.05\n19.18\n28.77\n87\n\n\nTotal\n57\n28\n42\n127\n\n\n\nTest statistic: \\[\\chi^2 = \\sum_{i=1}^2\\sum_{j=1}^3 \\frac{(o_{ij}-e_{ij})^2}{e_{ij}}=\\frac{(2-17.95)^2}{17.95}+\\cdots + \\frac{(14-28.77)^2}{28.77}=45.00\\]\np-value:\n\nfrom scipy import stats\np = 1-stats.chi2.cdf(45.00, df = (3-1)*(2-1))\nprint(f\"P-value: {p:.4f}\")\n\nP-value: 0.0000\n\n\np-value is zero. This indicates strong evidence against the null hypothesis. We would conclude that foot asymmetry depends on gender.\n\nimport numpy as np\nfrom scipy.stats import chi2_contingency\n\n# Construct the contingency table\n# Rows: Men, Women\n# Columns: L &gt; R, L = R, L &lt; R\ntable = np.array([\n    [2, 10, 28],\n    [55, 18, 14]\n])\n\n# Perform the chi-square test of independence\nchi2, p, dof, expected = chi2_contingency(table, correction = False)\n\n# Display results\nprint(f\"Chi-square statistic: {chi2:.4f}\")\n\nChi-square statistic: 45.0029\n\nprint(f\"Degrees of freedom: {dof}\")\n\nDegrees of freedom: 2\n\nprint(f\"P-value: {p:.4f}\")\n\nP-value: 0.0000\n\nprint(\"Expected frequencies:\")\n\nExpected frequencies:\n\nprint(expected)\n\n[[17.95275591  8.81889764 13.22834646]\n [39.04724409 19.18110236 28.77165354]]\n\n\n\n\n\n\nA company wants to assess whether customer response (Yes/No) to a marketing campaign depends on the platform used (Email, SMS, Social Media).\n\n\n\nResponse\nEmail\nSMS\nSocial Media\n\n\n\n\nYes\n120\n45\n80\n\n\nNo\n180\n105\n120\n\n\n\n\nFormulate the null and alternative hypotheses.\nCompute the expected frequencies.\nCalculate the chi-squared test statistic.\nDetermine the degrees of freedom.\nAt a 5% significance level, is there evidence to suggest the response rate depends on the platform?\n\n\n\nShow solutions\n\n\n\n\n\\(H_0\\): Platform used and response are independent.\n\\(H_0\\): Platform used and response are dependent.\n\nCalculating row and columns sums\n\n\n\n\nResponse\nEmail\nSMS\nSocial Media\nTotal\n\n\n\n\nYes\n120\n45\n80\n245\n\n\nNo\n180\n105\n120\n405\n\n\nTotal\n300\n150\n200\n650\n\n\n\nExpected values under the null hypothesis:\n\\(e_{ij} = \\frac{o_{\\cdot j}o_{i\\cdot}}{n}\\), e.g. \\(e_{22}= \\frac{150\\cdot 405}{650}=19.18\\).\n\n\n\nResponse\nEmail\nSMS\nSocial Media\nTotal\n\n\n\n\nYes\n113.08\n56.54\n75.38\n245\n\n\nNo\n186.92\n93.46\n124.62\n405\n\n\nTotal\n300\n150\n200\n650\n\n\n\n\nTest statistic: \\[\\chi^2 = \\sum_{i=1}^2\\sum_{j=1}^3 \\frac{(o_{ij}-e_{ij})^2}{e_{ij}}=\\frac{(120-113.08)^2}{113.08}+\\cdots + \\frac{(120-124.62)^2}{124.62}=4.91\\]\nDegrees of freedom is \\[(\\text{number of rows} -1)\\times(\\text{number of columns} -1) = (2-1)\\cdot (3-1)=2.\\]\nWe find the p-value:\n\n\nfrom scipy.stats import chi2\np = 1-chi2.cdf(4.91, df = 2)\nprint(f\"P-value: {p:.4f}\")\n\nP-value: 0.0859\n\n\nSince p-value &gt; 5%, we do not reject the null hypothesis. There is not sufficient evidence to suggest that response is dependent on platform used.\n\nimport numpy as np\nfrom scipy.stats import chi2_contingency\n\n# Construct the contingency table\n# Rows: Yes, No\n# Columns: Email, SMS, Social Media\ntable = np.array([\n    [120, 45, 80],\n    [180, 105, 120]\n])\n\n# Perform the chi-square test of independence\nchi2, p, dof, expected = chi2_contingency(table, correction = False)\n\n# Display results\nprint(f\"Chi-square statistic: {chi2:.4f}\")\n\nChi-square statistic: 4.9131\n\nprint(f\"Degrees of freedom: {dof}\")\n\nDegrees of freedom: 2\n\nprint(f\"P-value: {p:.4f}\")\n\nP-value: 0.0857\n\nprint(\"Expected frequencies:\")\n\nExpected frequencies:\n\nprint(expected)\n\n[[113.07692308  56.53846154  75.38461538]\n [186.92307692  93.46153846 124.61538462]]\n\n\n\n\n\n\nThe Norwegian Labour and Welfare Administration (NAV) organizes online job application workshops on Teams. The course responsible has noticed that except for himself, the attendees that have their camera on during the course is mostly the older participants. He wonders if there is a relationship between participants having camera on and participant’s age group? He therefore takes a screenshot of the meeting (that he deletes afterwards, of course) and counts the number of faces he sees for the different ages. The results are given in the table below. Perform a chi-squared test to answer this question, including setting up the hypothesis, finding the p-value and making your conclusion. Remember to settle on a significance level first.\n\n\n\nCamera on\n18–35\n36–53\n54-70\nTotal\n\n\n\n\nYes\n4\n3\n20\n27\n\n\nNo\n20\n13\n39\n72\n\n\nTotal\n24\n16\n59\n99\n\n\n\nIs there a Simpson’s paradox here?\n\n\nShow solutions\n\nThe course responsible just wants to test out a curiousity, and can set a relatively high significance level. Let’s say \\(\\alpha = 10\\%\\).\n\\(H_0:\\) Age group and whether a person has their camera on are independent variables.\n\\(H_A:\\) Age group and whether a person has their camera on are dependent variables.\nExpected values under the null hypothesis:\n\\(e_{ij} = \\frac{o_{\\cdot j}o_{i\\cdot}}{n}\\), e.g. \\(e_{11}= \\frac{24\\cdot 27}{99}=6.54\\).\n\n\n\nCamera on\n18–35\n36–53\n54-70\nTotal\n\n\n\n\nYes\n6.54\n4.36\n16.1\n27\n\n\nNo\n17.5\n11.6\n42.9\n72\n\n\nTotal\n24\n16\n59\n99\n\n\n\nCalculate test statistic:\n\\[\\chi^2 = \\sum_{i=1}^2\\sum_{j=1}^3 \\frac{(o_{ij}-e_{ij})^2}{e_{ij}}=\\frac{(4-6.54)^2}{6.54}+\\cdots + \\frac{(39-42.9)^2}{42.9}=3.25\\] P-value: \\(P(\\chi^2 &gt; 3.25)=1-P(\\chi^2 \\le3.25 )= 0.197\\)\n\nfrom scipy.stats import chi2\np = 1-chi2.cdf(3.25, df = 2)\nprint(f\"p-value: {p:.3f}\")\n\np-value: 0.197\n\n\nSince the p-value is above \\(\\alpha = 0.1\\), we fail to reject the null hypothesis. There is not sufficient evidence to suggest that age group and whether they have their camera on is dependent variables.\n\nimport numpy as np\nfrom scipy.stats import chi2_contingency\n\n# Contingency table: Rows = Camera on (Yes/No), Columns = Age groups\ntable = np.array([\n    [4, 3, 20],\n    [20, 13, 39]\n])\n\n# Perform the chi-square test of independence\nchi2, p, dof, expected = chi2_contingency(table, correction = False)\n\n# Display results\nprint(f\"Chi-square statistic: {chi2:.4f}\")\n\nChi-square statistic: 3.2528\n\nprint(f\"Degrees of freedom: {dof}\")\n\nDegrees of freedom: 2\n\nprint(f\"P-value: {p:.4f}\")\n\nP-value: 0.1966\n\nprint(\"Expected frequencies:\")\n\nExpected frequencies:\n\nprint(expected)\n\n[[ 6.54545455  4.36363636 16.09090909]\n [17.45454545 11.63636364 42.90909091]]\n\n\nIs there a Simpson’s paradox here? For that, we calculate the rates of “camera on” for each age group:\n\n\n\nAge group\n18–35\n36–53\n54-70\nTotal\n\n\n\n\nRate of camera on\n16.7%\n18.8%\n33.9%\n27.3%\n\n\nObservations\n24\n16\n59\n99\n\n\n\nThe rate of camera on increases with age group. For the number of observations, we also see that more participants are in the elder group. The proportion of age group 54-70 among camera users is 20/27 = 74.1%, and for the “No” group 39/72=54.2%. Hence, the “Yes” group has disproportionately more older people, who are more likely to use the camera. There is no reversal of trends between the subgroups and the aggregate — rather, the aggregate reflects the within-group pattern (camera usage increases with age). Therefore, there is no Simpson’s paradox here.",
    "crumbs": [
      "Modules",
      "Module 5: Measuring relationships and fitting models",
      "Exercises"
    ]
  },
  {
    "objectID": "5-exercises.html#contingency-tables-problems",
    "href": "5-exercises.html#contingency-tables-problems",
    "title": "Exercises",
    "section": "",
    "text": "Researchers collected data on the size of their feet for a group of right-handed male and females and counted how many of each sex had a left-foot larger than the right, equal feet or right foot larger than the left. The results are given in the table here:\n\n\n\nSex\nL&gt;R\nL=R\nL&lt;R\nTotal\n\n\n\n\nMen\n2\n10\n28\n40\n\n\nWomen\n55\n18\n14\n87\n\n\n\nDoes the data indicate that gender has a strong effect on the development of foot asymmetry? Specify the null- and alternative hypothesis, compute the \\(\\chi^2\\) test statistic and obtain the p-value.\n\n\nShow solutions\n\n$H_0: $ Gender and foot asymmetry are independent.\n$H_A: $ Gender and foot asymmetry are dependent.\n\n\n\nSex\nL&gt;R\nL=R\nL&lt;R\nTotal\n\n\n\n\nMen\n2\n10\n28\n40\n\n\nWomen\n55\n18\n14\n87\n\n\nTotal\n57\n28\n42\n127\n\n\n\nExpected values under the null hypothesis:\n\\(e_{ij} = \\frac{o_{\\cdot j}o_{i\\cdot}}{n}\\), e.g. \\(e_{22}= \\frac{28\\cdot 87}{127}=19.18\\).\n\n\n\nSex\nL&gt;R\nL=R\nL&lt;R\nTotal\n\n\n\n\nMen\n17.95\n8.82\n13.23\n40\n\n\nWomen\n39.05\n19.18\n28.77\n87\n\n\nTotal\n57\n28\n42\n127\n\n\n\nTest statistic: \\[\\chi^2 = \\sum_{i=1}^2\\sum_{j=1}^3 \\frac{(o_{ij}-e_{ij})^2}{e_{ij}}=\\frac{(2-17.95)^2}{17.95}+\\cdots + \\frac{(14-28.77)^2}{28.77}=45.00\\]\np-value:\n\nfrom scipy import stats\np = 1-stats.chi2.cdf(45.00, df = (3-1)*(2-1))\nprint(f\"P-value: {p:.4f}\")\n\nP-value: 0.0000\n\n\np-value is zero. This indicates strong evidence against the null hypothesis. We would conclude that foot asymmetry depends on gender.\n\nimport numpy as np\nfrom scipy.stats import chi2_contingency\n\n# Construct the contingency table\n# Rows: Men, Women\n# Columns: L &gt; R, L = R, L &lt; R\ntable = np.array([\n    [2, 10, 28],\n    [55, 18, 14]\n])\n\n# Perform the chi-square test of independence\nchi2, p, dof, expected = chi2_contingency(table, correction = False)\n\n# Display results\nprint(f\"Chi-square statistic: {chi2:.4f}\")\n\nChi-square statistic: 45.0029\n\nprint(f\"Degrees of freedom: {dof}\")\n\nDegrees of freedom: 2\n\nprint(f\"P-value: {p:.4f}\")\n\nP-value: 0.0000\n\nprint(\"Expected frequencies:\")\n\nExpected frequencies:\n\nprint(expected)\n\n[[17.95275591  8.81889764 13.22834646]\n [39.04724409 19.18110236 28.77165354]]\n\n\n\n\n\n\nA company wants to assess whether customer response (Yes/No) to a marketing campaign depends on the platform used (Email, SMS, Social Media).\n\n\n\nResponse\nEmail\nSMS\nSocial Media\n\n\n\n\nYes\n120\n45\n80\n\n\nNo\n180\n105\n120\n\n\n\n\nFormulate the null and alternative hypotheses.\nCompute the expected frequencies.\nCalculate the chi-squared test statistic.\nDetermine the degrees of freedom.\nAt a 5% significance level, is there evidence to suggest the response rate depends on the platform?\n\n\n\nShow solutions\n\n\n\n\n\\(H_0\\): Platform used and response are independent.\n\\(H_0\\): Platform used and response are dependent.\n\nCalculating row and columns sums\n\n\n\n\nResponse\nEmail\nSMS\nSocial Media\nTotal\n\n\n\n\nYes\n120\n45\n80\n245\n\n\nNo\n180\n105\n120\n405\n\n\nTotal\n300\n150\n200\n650\n\n\n\nExpected values under the null hypothesis:\n\\(e_{ij} = \\frac{o_{\\cdot j}o_{i\\cdot}}{n}\\), e.g. \\(e_{22}= \\frac{150\\cdot 405}{650}=19.18\\).\n\n\n\nResponse\nEmail\nSMS\nSocial Media\nTotal\n\n\n\n\nYes\n113.08\n56.54\n75.38\n245\n\n\nNo\n186.92\n93.46\n124.62\n405\n\n\nTotal\n300\n150\n200\n650\n\n\n\n\nTest statistic: \\[\\chi^2 = \\sum_{i=1}^2\\sum_{j=1}^3 \\frac{(o_{ij}-e_{ij})^2}{e_{ij}}=\\frac{(120-113.08)^2}{113.08}+\\cdots + \\frac{(120-124.62)^2}{124.62}=4.91\\]\nDegrees of freedom is \\[(\\text{number of rows} -1)\\times(\\text{number of columns} -1) = (2-1)\\cdot (3-1)=2.\\]\nWe find the p-value:\n\n\nfrom scipy.stats import chi2\np = 1-chi2.cdf(4.91, df = 2)\nprint(f\"P-value: {p:.4f}\")\n\nP-value: 0.0859\n\n\nSince p-value &gt; 5%, we do not reject the null hypothesis. There is not sufficient evidence to suggest that response is dependent on platform used.\n\nimport numpy as np\nfrom scipy.stats import chi2_contingency\n\n# Construct the contingency table\n# Rows: Yes, No\n# Columns: Email, SMS, Social Media\ntable = np.array([\n    [120, 45, 80],\n    [180, 105, 120]\n])\n\n# Perform the chi-square test of independence\nchi2, p, dof, expected = chi2_contingency(table, correction = False)\n\n# Display results\nprint(f\"Chi-square statistic: {chi2:.4f}\")\n\nChi-square statistic: 4.9131\n\nprint(f\"Degrees of freedom: {dof}\")\n\nDegrees of freedom: 2\n\nprint(f\"P-value: {p:.4f}\")\n\nP-value: 0.0857\n\nprint(\"Expected frequencies:\")\n\nExpected frequencies:\n\nprint(expected)\n\n[[113.07692308  56.53846154  75.38461538]\n [186.92307692  93.46153846 124.61538462]]\n\n\n\n\n\n\nThe Norwegian Labour and Welfare Administration (NAV) organizes online job application workshops on Teams. The course responsible has noticed that except for himself, the attendees that have their camera on during the course is mostly the older participants. He wonders if there is a relationship between participants having camera on and participant’s age group? He therefore takes a screenshot of the meeting (that he deletes afterwards, of course) and counts the number of faces he sees for the different ages. The results are given in the table below. Perform a chi-squared test to answer this question, including setting up the hypothesis, finding the p-value and making your conclusion. Remember to settle on a significance level first.\n\n\n\nCamera on\n18–35\n36–53\n54-70\nTotal\n\n\n\n\nYes\n4\n3\n20\n27\n\n\nNo\n20\n13\n39\n72\n\n\nTotal\n24\n16\n59\n99\n\n\n\nIs there a Simpson’s paradox here?\n\n\nShow solutions\n\nThe course responsible just wants to test out a curiousity, and can set a relatively high significance level. Let’s say \\(\\alpha = 10\\%\\).\n\\(H_0:\\) Age group and whether a person has their camera on are independent variables.\n\\(H_A:\\) Age group and whether a person has their camera on are dependent variables.\nExpected values under the null hypothesis:\n\\(e_{ij} = \\frac{o_{\\cdot j}o_{i\\cdot}}{n}\\), e.g. \\(e_{11}= \\frac{24\\cdot 27}{99}=6.54\\).\n\n\n\nCamera on\n18–35\n36–53\n54-70\nTotal\n\n\n\n\nYes\n6.54\n4.36\n16.1\n27\n\n\nNo\n17.5\n11.6\n42.9\n72\n\n\nTotal\n24\n16\n59\n99\n\n\n\nCalculate test statistic:\n\\[\\chi^2 = \\sum_{i=1}^2\\sum_{j=1}^3 \\frac{(o_{ij}-e_{ij})^2}{e_{ij}}=\\frac{(4-6.54)^2}{6.54}+\\cdots + \\frac{(39-42.9)^2}{42.9}=3.25\\] P-value: \\(P(\\chi^2 &gt; 3.25)=1-P(\\chi^2 \\le3.25 )= 0.197\\)\n\nfrom scipy.stats import chi2\np = 1-chi2.cdf(3.25, df = 2)\nprint(f\"p-value: {p:.3f}\")\n\np-value: 0.197\n\n\nSince the p-value is above \\(\\alpha = 0.1\\), we fail to reject the null hypothesis. There is not sufficient evidence to suggest that age group and whether they have their camera on is dependent variables.\n\nimport numpy as np\nfrom scipy.stats import chi2_contingency\n\n# Contingency table: Rows = Camera on (Yes/No), Columns = Age groups\ntable = np.array([\n    [4, 3, 20],\n    [20, 13, 39]\n])\n\n# Perform the chi-square test of independence\nchi2, p, dof, expected = chi2_contingency(table, correction = False)\n\n# Display results\nprint(f\"Chi-square statistic: {chi2:.4f}\")\n\nChi-square statistic: 3.2528\n\nprint(f\"Degrees of freedom: {dof}\")\n\nDegrees of freedom: 2\n\nprint(f\"P-value: {p:.4f}\")\n\nP-value: 0.1966\n\nprint(\"Expected frequencies:\")\n\nExpected frequencies:\n\nprint(expected)\n\n[[ 6.54545455  4.36363636 16.09090909]\n [17.45454545 11.63636364 42.90909091]]\n\n\nIs there a Simpson’s paradox here? For that, we calculate the rates of “camera on” for each age group:\n\n\n\nAge group\n18–35\n36–53\n54-70\nTotal\n\n\n\n\nRate of camera on\n16.7%\n18.8%\n33.9%\n27.3%\n\n\nObservations\n24\n16\n59\n99\n\n\n\nThe rate of camera on increases with age group. For the number of observations, we also see that more participants are in the elder group. The proportion of age group 54-70 among camera users is 20/27 = 74.1%, and for the “No” group 39/72=54.2%. Hence, the “Yes” group has disproportionately more older people, who are more likely to use the camera. There is no reversal of trends between the subgroups and the aggregate — rather, the aggregate reflects the within-group pattern (camera usage increases with age). Therefore, there is no Simpson’s paradox here.",
    "crumbs": [
      "Modules",
      "Module 5: Measuring relationships and fitting models",
      "Exercises"
    ]
  },
  {
    "objectID": "5-least-squares-estimation.html",
    "href": "5-least-squares-estimation.html",
    "title": "Least Squares Estimation",
    "section": "",
    "text": "Slides for “Least Squares Estimation”"
  },
  {
    "objectID": "5-logistic-regression.html",
    "href": "5-logistic-regression.html",
    "title": "Logistic regression",
    "section": "",
    "text": "“Logistic regression”\n\n\n\n\nWhen do we need logistic regression?\nWhat are we modelling when doing logistic regression?\nHow do you interpret \\(\\beta_x\\) in a logistic regression?",
    "crumbs": [
      "Modules",
      "Module 5: Measuring relationships and fitting models",
      "Logistic regression"
    ]
  },
  {
    "objectID": "5-logistic-regression.html#slides",
    "href": "5-logistic-regression.html#slides",
    "title": "Logistic regression",
    "section": "",
    "text": "“Logistic regression”",
    "crumbs": [
      "Modules",
      "Module 5: Measuring relationships and fitting models",
      "Logistic regression"
    ]
  },
  {
    "objectID": "5-logistic-regression.html#control-questions",
    "href": "5-logistic-regression.html#control-questions",
    "title": "Logistic regression",
    "section": "",
    "text": "When do we need logistic regression?\nWhat are we modelling when doing logistic regression?\nHow do you interpret \\(\\beta_x\\) in a logistic regression?",
    "crumbs": [
      "Modules",
      "Module 5: Measuring relationships and fitting models",
      "Logistic regression"
    ]
  },
  {
    "objectID": "5-python.html",
    "href": "5-python.html",
    "title": "Python",
    "section": "",
    "text": "Python\nHere we will present some useful python commands relevant for what we want to do in Module 5.",
    "crumbs": [
      "Modules",
      "Module 5: Measuring relationships and fitting models",
      "Python"
    ]
  },
  {
    "objectID": "5-textbook.html",
    "href": "5-textbook.html",
    "title": "TECH3 Applied statistics",
    "section": "",
    "text": "The following is a redistribution of chapters 12, 13, 14, and 17 of Statistical Thinking for the 21st Century by Russell A. Poldrack, 2019, under the CC BY-NC 4.0 licence.",
    "crumbs": [
      "Modules",
      "Module 5: Measuring relationships and fitting models",
      "Textbook"
    ]
  },
  {
    "objectID": "5-textbook.html#modeling-categorical-relationships",
    "href": "5-textbook.html#modeling-categorical-relationships",
    "title": "TECH3 Applied statistics",
    "section": "Modeling categorical relationships",
    "text": "Modeling categorical relationships",
    "crumbs": [
      "Modules",
      "Module 5: Measuring relationships and fitting models",
      "Textbook"
    ]
  },
  {
    "objectID": "5-textbook.html#modeling-continuous-relationships",
    "href": "5-textbook.html#modeling-continuous-relationships",
    "title": "TECH3 Applied statistics",
    "section": "Modeling continuous relationships",
    "text": "Modeling continuous relationships",
    "crumbs": [
      "Modules",
      "Module 5: Measuring relationships and fitting models",
      "Textbook"
    ]
  },
  {
    "objectID": "5-textbook.html#the-general-linear-model",
    "href": "5-textbook.html#the-general-linear-model",
    "title": "TECH3 Applied statistics",
    "section": "The General Linear Model",
    "text": "The General Linear Model",
    "crumbs": [
      "Modules",
      "Module 5: Measuring relationships and fitting models",
      "Textbook"
    ]
  },
  {
    "objectID": "5-textbook.html#practical-statistical-modeling",
    "href": "5-textbook.html#practical-statistical-modeling",
    "title": "TECH3 Applied statistics",
    "section": "Practical statistical modeling",
    "text": "Practical statistical modeling",
    "crumbs": [
      "Modules",
      "Module 5: Measuring relationships and fitting models",
      "Textbook"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Sondre Hølleland,  Assistant professor\n\n\n\n\n\nGeir Drage Berentsen,  Associate professor\n\n\n\nThis course and website have been developed by Sondre Hølleland, assistant professor, and Geir Drage Berentsen, professor, both at Norwegian School of Economics, Department of Business and Management Science. More about us will be added."
  },
  {
    "objectID": "case-1.html",
    "href": "case-1.html",
    "title": "Case 1",
    "section": "",
    "text": "Case 1\n\n\n\nPoster by Hege Landsvik ©\n\n\nSource: Landsvik, H., Martuza, J., Skard, S., Pedersen, L. J. T., & Jørgensen, S. Group Identity and Pro-Environmental Behavior in Public Settings: Using Identity-based Nudges to Enhance Recycling in the Field. Preprint.\nIn this case session, you will be making a descriptive analysis of a dataset from a field experiment on Brann Stadium, the home of Bergen’s local soccer team.\nThe researchers gathered data on the number of cups sold on the away stand at Brann Stadium. After each match, they went through the garbage, at counted how many cups was correctly recycled, that is, put in the bin for soda- and coffee cups. The experiment was constructed using two treatments. Firstly, they used a nudge, by putting a sign over the bins asking “Who is the most environmental away supporters in Eliteserien (Norway’s highest soccer league)?” Sort you garbage here.” The sign would also have the logo of the away team. This is called an ingroup appeal. The other treatment is gamification. Here they would set up a table showing the sorting rate of the other away teams. The intent of the researchers is to make it a sorting competition against the other teams.\nThe dataset has the following variables:\n\nMatch_Number: Match ID. Starting from the second home match of the 2023 season.\nOpponent: Name of the away team - adjusted for æøå.\nAway_Conditon: Baseline, Ingroup or Outgroup\nGamification_Days: Baseline, No, Yes.\nWinner: Who won the match? Home, tie or Away.\nSortedCup: Was the cup sorted? 0: No, 1: Yes.\n\nEach observation is a cup sold. If the cup was put in the correct bin, sortedCup column is 1, otherwise it is zero.\n\nCreate a frequency table for each match counting how many cups was sold and how many was correctly sorted. Which away team supporters are the largest consumers of coffee?\nAdd a relative frequency (sorting rate) column to your table. What team is the best recyclers?\nSummarize the sorting rates across teams and treatments, using mean, median, standard deviation, 1st and 3rd quartile.\nCreate a good visualization of the sorting rate for the various teams.\nNow, the different teams were exposed to different treatments, both for the in- or out-group message on the banner and whether they were exposed to gamification or not. If we disregard the individual teams and only consider these treatments marginally (marginally means one treatment at a time).\n\n\nUse suitable summary statistics to summarize the effect of different treatments.\nMake suitable figures to visualize the effects. One suggestion: Use boxplots.\n\nDiscuss your summary statistics and visuals within the group."
  },
  {
    "objectID": "data/countries/country_data_wrangling.html",
    "href": "data/countries/country_data_wrangling.html",
    "title": "Country data - wrangling",
    "section": "",
    "text": "Wrangling of countries data into a single dataset\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.6\n✔ forcats   1.0.1     ✔ stringr   1.6.0\n✔ ggplot2   4.0.1     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.2\n✔ purrr     1.2.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\ncountry codes with continent from https://datahub.io/core/country-codes\n\n# north america is coded as NA so need to exclude it from NA list\ncountry_codes &lt;- read_delim('country_codes.csv', na=c('')) %&gt;%\n    rename(CountryCode = `ISO3166-1-Alpha-3`, \n           Name = `UNTERM English Formal`) %&gt;%\n    select(Continent, CountryCode)\n\nRows: 250 Columns: 56\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (48): FIFA, Dial, ISO3166-1-Alpha-3, MARC, is_independent, ISO3166-1-num...\ndbl  (5): Intermediate Region Code, M49, Sub-region Code, Region Code, Geona...\nnum  (2): GAUL, ISO4217-currency_minor_unit\nlgl  (1): Global Code\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nfrom google, this contains latitude/longitude\n\ncountries &lt;- read_delim('countries/countries.csv') %&gt;%\n  rename(CountryName = name)\n\nRows: 245 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): country, name\ndbl (2): latitude, longitude\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nfrom worldbank, this contains population over time\n\n\nyears are in columns\n\npopulation &lt;- read_csv('worldbank/API_SP.POP.TOTL_DS2_en_csv_v2_3469297.csv', show_col_types = FALSE, skip=4) %&gt;% \n  select(-`...66`) %&gt;%\n  rename(CountryName = `Country Name`,\n         CountryCode = `Country Code`,\n         Population2020 = `2020`) %&gt;%\n  select(CountryName, CountryCode, Population2020)\n\nNew names:\n• `` -&gt; `...66`\n\n\n\ncountry_metadata &lt;- read_csv('worldbank/Metadata_Country_API_SP.POP.TOTL_DS2_en_csv_v2_3469297.csv', show_col_types = FALSE) %&gt;% \n  select(-`...6`) %&gt;%\n  rename(CountryCode = `Country Code`) %&gt;%\n  select(-SpecialNotes, -TableName)\n\nNew names:\n• `` -&gt; `...6`\n\n\n\npopdata &lt;- merge(population, country_metadata, by='CountryCode') %&gt;%\n  drop_na(Region)  # remove regional summaries\n\npopdata &lt;- merge(popdata, country_codes, by='CountryCode') %&gt;%\n  drop_na(Continent)\n\n\npopdata_with_geodata &lt;- merge(popdata, countries, by=\"CountryName\")\n\n\ngdp &lt;- read_csv('worldbank_gdp/API_NY.GDP.MKTP.CD_DS2_en_csv_v2_3469429.csv', show_col_types = FALSE, skip=4) %&gt;% \n  select(-`...66`) %&gt;%\n  rename(CountryName = `Country Name`,\n         CountryCode = `Country Code`,\n         GDP2020 = `2020`)  %&gt;%\n  select(CountryCode, GDP2020) %&gt;%\n  drop_na(GDP2020)\n\nNew names:\n• `` -&gt; `...66`\n\npopdata_with_geodata_and_gdp &lt;- merge(popdata_with_geodata, gdp)\n\n\nwrite_csv(popdata_with_geodata_and_gdp, 'country_data.csv')"
  },
  {
    "objectID": "datalab2.html",
    "href": "datalab2.html",
    "title": "Datalab 2",
    "section": "",
    "text": "Datalab 2\nThe second datalab is about getting familiar with python function relevant for common distributions. The content of the datalab can be cloned using the following line in git bash:\ngit clone https://github.com/holleland/TECH3_datalab2\nThis will make a local copy of the files on the github repository at the current location of the git bash terminal.\nIf you dont want to use git bash, you can also open the github link in your browser and download the files “manually”.\nThe repository consists of three files:\n\ndatalab2.ipynb: The file for you to be working with.\ndatalab2_solutions.ipynb: Suggestive solutions.\nenvironment.yml: Conda environment file.\n\nTo create the conda environment TECH3_student from the environment file, you can open an anaconda prompt and write:\nconda env create --file environment.yml\nAfter the environment has been set up, you can activate the conda environment and open Jupyter Notebook using\nconda activate TECH3_student\njupyter notebook\nFor further details on how to set up the conda environment, see the guides on the repository for TECH2.\nOpen the file datalab2.ipynb, and go through the document. Write your own code and try to solve the problems yourself before consulting the solutions notebook.",
    "crumbs": [
      "Datalabs",
      "Datalab 2"
    ]
  },
  {
    "objectID": "datalab4.html",
    "href": "datalab4.html",
    "title": "Datalab 4",
    "section": "",
    "text": "Datalab 4",
    "crumbs": [
      "Datalabs",
      "Datalab 4"
    ]
  },
  {
    "objectID": "datalabs.html",
    "href": "datalabs.html",
    "title": "Practicalities",
    "section": "",
    "text": "Practicalities\nHere you will find practical information about the datalabs. The data labs are meant to help you learn to do relevant tasks in Python by doing them by yourself. They will help you get started by providing some example code and then task you to solve some tasks yourself. The solutions will also be available, but please only use this if you get stuck.\nSince you have learned to use git and github in TECH2, we want to facilitate you to continue using these tools. The datalabs are therefore published on github. You can choose to clone or fork the repositories using git bash and commands. Using git is not a learning goal of TECH3, so you may also just download the notebooks from github using your web browser.\nIn TECH2 you used anaconda and environments. We have therefore made a environment.yml file that you can use to create a conda environment with the necessary python packages for TECH3 (see Datalab 1). This can be used for all python activities in the course and during the exam.",
    "crumbs": [
      "Datalabs",
      "Practicalities"
    ]
  },
  {
    "objectID": "oldexams.html",
    "href": "oldexams.html",
    "title": "Old exams",
    "section": "",
    "text": "Old exams\nIn the table below, you find links to PDFs of old exams. The first semester TECH3 was introduced was Spring 2025. The assessment form was then a 4-hour school exam, and we made three exams: One trial exam that students got to train for the exam (getting familiarized with the format), one ordinary school exam and one retake exam for those who did not pass.\nFrom the Spring 2026, we have moved over to a 6-hour digital school exam after feedback from the students.\n\n\n\n\n\nCourse\nSemester\nType\nProblems\nSolutions\n\n\n\n\nTECH3\nSpring 2025\nTrial Exam\nPDF\nPDF\n\n\nTECH3\nSpring 2025\nOrdinary\nPDF\nPDF\n\n\nTECH3\nSpring 2025\nRetake\nPDF\nPDF",
    "crumbs": [
      "Old exams"
    ]
  },
  {
    "objectID": "seminar2.html",
    "href": "seminar2.html",
    "title": "Seminar 2",
    "section": "",
    "text": "Seminar 2\n\nExercise 1\nLet \\(X\\) follow a power distribution, with density \\[f(x) = \\alpha x^{\\alpha-1}, \\quad 0&lt;x&lt;1,\\quad \\alpha&gt;0.\\]\n\nWhat is the expectation of \\(X\\)?\nWhat is the variance of \\(X\\)?\nDerive the maximum likelihood estimator of \\(\\alpha\\) for an independent and identically distributed sample of \\(X:\\) \\[X_1,\\ldots, X_n.\\]\nFind the point estimate of \\(\\alpha\\), given the following sample:\n\n\nimport numpy as np\nx = np.array((0.794, 0.884, 0.774, 0.559, 0.240, \n              0.754, 0.705, 0.639, 0.614, 0.897))\nprint(x)\n\n[0.794 0.884 0.774 0.559 0.24  0.754 0.705 0.639 0.614 0.897]\n\n\n\nUse a bootstrap routine to estimate the 5th and 95th percentile of the estimator using\n\nThe sample only.\nThe parametric distribution of \\(X\\) stated at the top.\nNormal approximation.\n\n\nHint: The stated distribution is a special case of a beta distribution with \\(b=1\\) and \\(a=\\alpha\\). We can simulate from it using the following code, provided a value for alpha.\n\nfrom scipy.stats import beta\nx = beta.rvs(a=alpha, b=1, size=10)\nprint(x)\n\n\nElaborate on the differences between the approaches.\nDo a Monte Carlo simulation experiment to check if the MLE fulfills the consistency requirements.\n\n\n\nExercise 2 (Adapted from Exam 2025, Problem 3 c-d)\nA startup offering hands-on craft beer brewing workshops is piloting a weekend course format. Each session runs on Saturdays only and is limited to 4 participants due to space and brewing setup constraints. Based on early demand, the number of participants X per Saturday follows this discrete distribution:\n\n\n\nParticipants (X)\n0\n1\n2\n3\n4\n\n\n\n\nProbability P(X=x)\n0.10\n0.25\n0.30\n0.20\n0.15\n\n\n\nEach participant pays NOK 2,200, and costs the company NOK 250 in materials. There is also a fixed cost of NOK 4,200 per workshop. The company starts with NOK 50,000 in capital and plans to run the workshop every Saturday for one year (52 weeks).\nThe expected number of participants per week is \\(2.05\\) and that the standard deviation is \\(1.20\\). See Seminar 1, Exercise 5.\n\nAssuming the number of participants each week is independent, use the Central Limit Theorem to estimate the probability that after 52 weeks, the company’s ending capital is negative(i.e. their initial capital of NOK 50,000 has been spent).\n\n\n\n\n\n\n\nTipThe Central Limit Theorem for a sum\n\n\n\nLet \\(X_1, X_2,...,X_n\\) be a sequence of independent and identically distributed random variables having a distribution with expectation \\(E(X)=\\mu_X\\) and finite variance \\(Var(X)=\\sigma^2\\). Then the sum \\(\\sum_{i=1}^n X_i\\) is approximately normally distributed with expectation \\(n\\cdot\\mu\\) and variance \\(n\\cdot\\sigma^2\\), i.e. \\(N(n\\cdot\\mu, n\\cdot\\sigma^2)\\).\n\n\n\n\n\n\n\n\n\n\n\n\nWrite a Monte Carlo simulation routine in Python to estimate the probability in part (c).\n\n\n\nExercise 3 (Inspired by trial exam, Problem 2)\nRoulette is a game of chance where a roulette wheel has 37 numbered pockets where a ball can land. 18 of these pockets are red, 18 are black, and 1 is green (at least in Europe).\nIf, for example, you bet 1, on red, and red occurs, then you win your bet back plus 1, (i.e., a gain of 1). But if black or green occurs, you lose the bet (a gain of -1).\nYou bet 1, on red. In Seminar 1 you found the following:\ni) What is the expected gain? -1/37\nii) What is the variance of this gain? 0.99\niii) What is the probability of a positive gain? 18/37\n\nYou continue this strategy for \\(n=50\\) times. Use the Central Limit Theorem to answer the questions in part (a) for the average gain.\nSolve (a) using simulation techniques in Python. Explain what method you use and why. Compare the value to the central limit theorem approximation.\nA common strategy is to double the bet if you loose (to win back the loss). Say you start out with 100 euros. Simulate one such game and plot the status of your wallet after for each spin of the roulette.\nWhat is the expected number of games before you are bankrupt?\nSay you have a stopping rule: If your balance reaches 200 euros, you will stop playing. What is the probably that you will leave richer? What is the probability that you go bankrupt with this stopping rule?\nWhat is the problem with this strategy in the long run? How much money will you need to cover your bets if the roulette outcomes are 10 consecutive blacks?",
    "crumbs": [
      "Seminars",
      "Seminar 2"
    ]
  }
]