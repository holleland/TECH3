# Exercises

### Problem 1

You have an event your interested in studying, $A$. What are the lower and upper bounds for the probability that $A$ occurs? I.e.what are the lower and upper bounds of $P(A)$?

<details>

<summary>Show solutions</summary>

By the definition of the probability of an event we know that it's bounded both from above and below. The upper bound being that $A$ is guaranteed, and the lower bound being that it is impossible. We represent this on the interval $[0,1]$. I.e. we know that the probability of even t $A$ occurring is between $0$ and $1$. More compact $P(A)\in[0,1]$

</details>

### Problem 2

One day you walk outside and overhear a person talking about the probability of an event. You hear little of what they have to say, but you *do* hear this person say "...this won't have to be enough, we will have to redo this experiment many more times to be certain I am approaching the right probability." Amazingly, some time later you hear another person mumbling to them selves about probability. "Wow, now this is a surprise. This result did not coincide with my previous beliefs at all. I will have to rethink the probability of such an event occurring", they say.

You get the feeling that these two people have very different way of interpreting probability? Identify who the frequentist and who the bayesian statistician is.

<details>

<summary>Show solutions</summary>

Here, person 1 is using frequentistic methods. Person 1's focus is entirely on getting in as many independent trials as possible to get a clear image of the probability of an event. Person 2 on the other hand has a completely different view. Person 2 obviously had an *initial belief* about the probability of an event, but after seeing a surprising result they were determined to *update* their beliefs about the probability of this event.

</details>

### Problem 3

Vegard is very interested in the quality of watermelons. He has decided that he wants to find out the probability of watermelons being overripe at REMA 1000. He feels he has two feasible tests he can do to find this probability, one is Bayesian in nature and one is frequentist. Vegard can either go in with an initial belief, buy a single watermelon, then update his beliefs. He can keep doing this until he is quite certain of the probability of the population. On the other hand, Vegard can buy 40 watermelons at once, check them all, and then argue for that sample being representative of the population.

Which method is Bayesian and which is frequentist?

<details>

<summary>Show solutions</summary>

The first option here is Bayesian. Vegard has an initial belief on the probability of the watermelons being overripe, and then he iterates and tries to make that more exact by checking one watermelon at the time.

The second option is a very typical frequentist test. From the whole population, a decent chunk is tested at once, in hope that it's a representative sample. In the representative sample overripe watermelons should have the same relative frequency as in the entire population.

</details>

### Problem 4

You have a sample space, $S$, and in that sample space you have two identifiable events, $A$ and $B$. Can you determine a case where the probability of the intersection of $A$ and $B$ ($P(A\cap B)$) occurring, equals the probability of $A$ ($P(A)$) occurring? Draw a Venn diagram!

<details>

<summary>Show solutions</summary>

Recall that the intersection of two events is when **they both** occur. I.e. for us to be in the intersection in a Venn diagram, both shapes have to cover the same area. There are infinitely variations here, but the important part is that $A$ must be **contained** within $B$. When this is the case, all of A will be intersected by $B$ (we say that $A$ is a *subset* of $B$, and the notation is $A\subseteq B$). In this case we get: $$P(A\cap B)=P(A)$$

</details>

### Problem 5

You have a sample space, $S$, and in that sample space you have two identifiable events, $A$ and $B$. Can you determine a case where the probability of the union of $A$ and $B$ ($P(A\cup B)$) occurring, equals the probability of $B$ ($P(B)$) occurring? Draw a Venn diagram!

<details>

<summary>Show solutions</summary>

Recall that the union of two events is when **either one or both** events occur. This means that all area covered by either $A$ or $B$ will be part of the union. We now want the probability of the union occurring to equal the probability of $B$ occurring. Since all area covered by either $A$ or $B$ is part of the union, we can have no excess are beyond $B$, i.e. $A$ should once again be a subset of $B$ ($A\subseteq B$). If this is the case we get: $$P(A\cup B)=P(B).$$

</details>

### Problem 6

Consider a situation where you have $0<P(A)<P(B)<1$.

a)  What are the lower and upper bounds of $P(A\cap B)$?

b)  What are the lower and upper bounds of $P(A\cup B)$?

<details>

<summary>Show solutions</summary>

a)  The intersection tells us that both $A$ and $B$ have to occur, and as such, we the *lower* one of the two gives us an upper bound, as the one event cant intersect with more than the cases where it itself occurs (draw a Venn diagram to confirm). We know nothing as to whether or not $A$ and $B$ are disjoint, and as this is still a possibility we may not have an intersection, this makes $0$ our lower bound. I.e. $$P(A\cap B)\in[0, P(A)]$$

b)  The union tells us that either $A$ or $B$ or both occurs. As all events where either one or both occurs, the unions lower bound is given by the *higher* of the two probabilities. We could also be in a situation where $A\cup B$ cover the whole sample space $S$, and in such a case we would have an upper bound of 1. I.e: $$P(A\cup B)\in[P(B),1]$$

</details>

### Problem 7

What is $A\cap B$ when events $A$ and $B$ are disjoint.

<details>

<summary>Show solutions</summary>

When the two events $A$ and $B$ are disjoint, there are no possible situation that would fit into the intersection $A \cap B$. In set notation we would write: $$A\cap B=\emptyset$$ where $\emptyset$ is called the empty set and denotes a set with no elements.

</details>

### Problem 8

You throw a fair six-sided dice, compute the probabilities of the following events.

a)  You roll 1

b)  You roll 3 or 4

c)  You roll an odd number

d)  You roll a number that's greater than 5

e)  You roll a number that's less than or equal to 10

<details>

<summary>Show solutions</summary>

A fair 6 sided die gives us the sample space $S=\{1,2,3,4,5,6\}$ with each respective outcome having the same probability to be tossed. i.e. if A is an event that denotes any one number being tossed, then: $$
P(A)=\frac{1}{6}$$

a)  Rolling a 1 is a single element in the sample space, i.e. $P(1)=\frac{1}{6}$

b)  3 and 4 constitute 2 possible disjoint outcomes, i.e. $$P(3\cup 4)=P(3)+P(4)=P\frac{1}{6}+\frac{1}{6}=\frac{2}{6}=\frac{1}{3}$$

c)  There are 3 different odd numbered outcomes

$$P(odd)=3\cdot\frac{1}{6}=\frac{1}{2}$$

d)  There is only one possible outcome greater than 5, namely 6. I.e.

$$ P(X>5)=P(6)=\frac{1}{6}$$

e)  Our sample space tells us we can only roll integers from 1 to 6, all of which are less than 10.

$$ P(X\leq10)=P(1\cup2\cup3\cup4\cup5\cup6)=\sum_{i=1}^6\frac{1}{6}=1$$

</details>

### Problem 9

You flip a coin 5 times as an experiment.

a)  Let event $A$ be getting *all* heads. What is $P(A)$?

b)  Let event $B$ be the complement of $A$, i.e $B=A^c$. What is the probability of getting $B$?

c)  What is $P(A\cap B)$?

d)  Compute $P(A \cup B)$, how do you interpret this result?

<details>

<summary>Show solutions</summary>

a)  We treat each coin flip as independent of all the others, we then also let the probability of getting heads be $P(H)=\frac{1}{2}$ for each coin toss. Since each toss is independent we can simply multiply them all together to find our final probability:

$$ P(A)=P(HHHHH)=\prod_{i=1}^5\frac{1}{2}=\left(\frac{1}{2}\right)^5=\frac{1}{2^5}=\frac{1}{32} $$

b)  $B$ being the complement of $A$ means that it will occur whenever $A$ *doesn't*. It's also simple to compute as we can use a nifty formula.

$$\begin{align}P(A^c)=1-P(A) \\
\Rightarrow P(B)=P(A^c)=1-P(A)=1-\frac{1}{32}=\frac{31}{32}
\end{align}$$

c)  Since $A$ and $B$ are *complements*, they are by definition *disjoint*, as at no point will both occur (no intersection!!!). As such, we get a clear implication that $A\cap B=\emptyset$ Following from this $P(A\cap B)=0$

d)  Finally we can compute the union. Recall that we have a formula for this.

$$P(A \cup B)=P(A)+P(B)-P(A\cap B) $$

We know all of the relevant probabilities needed for the computation.

$$ P(A \cup B)=\frac{1}{32}+\frac{31}{32}-0=1 $$

The probability of the union being 1 means that either $A$, $B$ or $A\cap B$ **will** occur. We already know, however that the events are disjoint, and as such there is no intersection. I.e. it's either $A$ or $B$ that \*\*must occur*. Interpretaing this we end up at the quite banal fact that when we do this experiment, we will* either\* end up with 5 heads, or we will not (any other permutation). In fact this situation illustrates a powerful concept concerning complements of events, being that if an event does *not* occur, then the complement *will* occur.

</details>

### Problem 10

Let $S$ be a sample space. What is the probability of the complement of $S$ ($P(S^c)$)? What does this imply?

<details>

<summary>Show solutions</summary>

We know from the axioms that $P(S)=1$, i.e. we can easily compute the complement.

$$ P(S^c)=1-P(S)=1-1=0 $$

This implies that the event of being within the sample space covers all other events. I.e. when we there is no chance of any event occurring that is not in the sample space.

</details>

### Probelm 11

You roll two 6 sided dice, one blue and one red. Consider now, what would be your sample space if:

a)  you considered the value on the red die and blue die separately?

b)  you consider the sum of the two dice?

<details>

<summary>Show solutions</summary>

a)  For every toss, we here have to consider two separate values, and we denote them as $(x,y)$. Each of the two dice can roll from 1 to 6. Adding it all together we can construct the sample space

$$ \begin{align*}S=\left\{(1,1), (1,2), (1,3), (1,4), (1,5), (1,6), \\
(2,1), (2,2), (2,3), (2,4), (2,5), (2,6), \\
(3,1), (3,2), (3,3), (3,4), (3,5), (3,6), \\
(4,1), (4,2), (4,3), (4,4), (4,5), (4,6), \\
(5,1), (5,2), (5,3), (5,4), (5,5), (5,6), \\
(6,1), (6,2), (6,3), (6,4), (6,5), (6,6) \right\}
\end{align*}$$

b)  We now consider the sum of the two dice. The lowest value is when both roll 1's, and the highest possible one is when both roll 6's. Every integer value between these two sums are achievable. We now end up with:

$$ S_{sum}=\{2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12\} $$

### Problem 12

You throw two dice.

a)  In how many outcomes is their sum less equals 6?

b)  What is the probability of getting a sum total of 2?

<details>

<summary>Show solutions</summary>

a)  If dice 1 rolls a 6, then the other dice will make the sum at least 7, i.e. no candidates. Dice 1 rolls 5, then we get 6 if dice 2 rolls 1, i.e. 1 candidate. Following similar logic if dice 1 rolls 1-4, then each produce 1 more candidate.

In total we end up with 5 outcomes that give us a sum of 6.

b)  For us to have a sum of 2, we need *both* dice to roll 1. i.e. there is only one outcome that nets this event There are 36 possible outcomes. i.e. $P(sum \ 2)=\frac{1}{36}$

</details>

### Problem 13

Let $A=\{1,2,3,4,6\}$, $B=\{2,4,6,8\}$, and $C=\{2,3,4,5\}$.

a)  Find $A\cap B\cap C$

b)  Find $A\cup B\cup C$

<details>

<sumamry>Show solutions

</summary>

a)  Recall first that

$$ A\cap (B\cap C)=A\cap B\cap C $$

So we can find the intersection simply by looking for the elements that appear in all of the sets. We end up with.

$$ A\cap B\cap C = \{2,4\} $$

b)  Recall first that

$$ A\cup (B\cup C)=A\cup B\cup C $$ The union will now consist of any element that falls within any of the sets, and as such we get

$$A\cup B\cup C =\{1,2,3,4,5,6,8\} $$

### Problem 14

You know that the probabilities of some intersections are given by $P(A\cap B)=\frac{1}{2}$ and $P(A\cap B^c)=\frac{1}{3}$. Compute $P(A)$.

<details>

<summary>Show solutions</summary>

Two things are very clear when working with complements, and that's that they are disjoint events and that the probability of one or the other occurring sum to 1 exactly. Directly following from this is that we can *always* make use of the law of total probability when we know the probabilities of the intersections between an event, $A$, and both complements. In this case we have exactly the right information to make use of the law of total probability. Recall the formula. $$ P(A)=\sum_{i=1}^kP(A\cap B_i)=P(A\cap B_1)+\cdots+P(A\cap B_2) $$ Let's use this to compute the probability

$$ P(A)=P(A\cap B)+P(A\cap B^c)=\frac{1}{2}+\frac{1}{3}=\frac{3+2}{6}=\frac{5}{6} $$

### Problem 15

We have an event, $A$, and we are interested in figuring out the probability of it occurring. We know the probabilities of $B_1,...,B_n$ and we know that $\sum_{i=1}^nP(B_i)=1$. Why can't we use the law of total probability here? What could go wrong? (It might help to draw a Venn diagram if you're struggling.)

<details>

<summary>Show solutions</summary>

The problem here is that we don't know if $B_1,...,B_n$ are disjoint events, and since, there may be areas where they intersect with each other, we run the risk of counting some space several times if we were to sum all intersections between $A$ and all $B_i$. We also can't be certain taht the entire sample space is covered by $B_i$ if tehy are not all disjoint (see problem 16 for implications). We can illustrate algebraically (to simplify we reduce some generality, but this argument does extend). Let $$ n=2, \ P(B_1\cap B_2)>0 $$ We can now let the $A\cap (B_1\cap B_2)\neq\emptyset$, i.e. $A$ also occurs in the intersection. We can now try to compute $P(A)$

$$\begin{align*}
P(A)=P(A\cap B_1)+P(A \cap B_2)=P(A\cap((B_1\cap B_2)\cup(B_1\cap B_2^c)))+P(A\cap((B_1\cap B_2)\cup(B_1^c\cap B_2))) \\
=P(A\cap(B_1\cap B_2)) + P(A\cap(B_1\cap B_2^c))+ P(A\cap(B_1\cap B_2)) + P(A\cap(B_1^c\cap B_2)) \\
=2P(A\cap(B_1\cap B_2))+ P(A\cap(B_1\cap B_2^c))+ P(A\cap(B_1^c\cap B_2)) 
\end{align*}$$

As we can see here, we have to account for an area where $A$ intersects twice, and as such, this effect will have us overestimate the probability of $A$ occurring. As such the equation above is necessarily *incorrect*.

</details>

### Problem 16

We have an event, $A$, and we are interested in figuring out the probability of it occurring. We know the probabilities of $B_1,...,B_n$ and we know that all $B_i$ are pairwise disjoint($B_i\cap B_j=\emptyset, \ i\neq j, \ \forall i,j$). Why can't we use the law of total probability here? What could go wrong? (It might help to draw a Venn diagram if you're struggling.)

<details>

<summary>Show solutions</summary>

The problem here is that the sum of all probabilities $P(B_i)$ may not equal 1. If the sum of probabilities $P(B_i)$ is less than 1, then there are cases where no events $B_i$ occur. If $A$ intersects here, the law of total probability will lead us to underestimating $P(A)$, as there is no $B_i$ that intersects this part of $A$. Let's try to show this algebraically (we reduce generality here too, but argument holds). $$ P((B_1\cup...\cup B_n)^c)>0, \ (B_1\cup...\cup B_n)^c\subset A, \ n=2 $$ If we try to compute using the law of total probability we then get

$$ P(A)=P(A\cap B_1)+P(A\cap B_2) \ (*)$$

Note that $(E_1\cup E_2)^c=E_1^c\cap E_2^c$

From this we know that $B_1^c \cap B_2^c\subset A$.

Because they are disjoint we also know that $B_1\subset B_2^c, \ B_2\subset B_1^c$, but we obviously have $B_1, B_2\not\subset B_1^c \cap B_2^c$, as any event will be disjoint with its complement.

This tells us that we don't account for $B_1^c \cap B_2^c\subset A$ in $(*)$, and thus we *must* be underestimating the probability, and the equation *cannot* hold.

</details>

### Problem 17

We are considering the probability of an event, $A$. We know that $P(A\cap B)=0.2$ and that $P(A\cap B^c)=0.05$. Compute $P(A)$.

<details>

<summary>Show solutions</summary>

By definition, the events $B$ and $B^c$ are disjoint and $P(B)+P(B^c)=1$, and as such, we can make use of the law of total probability. $$ P(A)=\sum_{i=1}^nP(A\cap B_i)=P(A\cap B)+P(A\cap B^c)=0.2+0.05=0.25=\frac{1}{4} $$

</details>

### Problem 18

We get that $P(A)=0.7$, $P(B_1)=0.3$, $P(B_2)=0.2$, $P(B_3)=0.5$. All $B_i$ are disjoint events. We also get that $P(A | B_1)=0.3$, $P(A | B_2)=0.9$. Find $P(A | B_3)$ using the law of total probability.

<details>

<summary>Show solutions</summary>

We need to rewrite the law of total probability to compute the conditional probability we want. Let's use the definition conditional probability for this. $$
P(A|B)=\frac{P(A\cap B)}{P(B)} \Rightarrow P(A\cap B)=P(A | B)P(B) \ (*)
$$

Recall the law of total probability, now we can rewrite it by substituting in $(*)$. We can use this altered equation to solve for $P(A|B_3)$

$$ \begin{align*}
P(A)=\sum_{i=1}^nP(A\cap B_i)=\sum^n_{i=1}P(A|B_i)P(B_i) \\
\Rightarrow P(A)=P(A|B_1)P(B_1)+P(A|B_2)P(B_2)+P(A|B_3)P(B_3) \\
\Rightarrow P(A|B_3)=\frac{P(A)-P(A|B_1)P(B_1)-P(A|B_2)P(B_2)}{P(B_3)}
\end{align*}$$

Now we can easily compute $P(A|B_3)$

$$
P(A|B_3)=\frac{0.7-0.3*0.3-0.2*0.9}{0.5}=\frac{0.7-0.09-0.18}{0.5}=0.86
$$

</details>

### Problem 19

Consider two events, $A$and $B$. You get that $P(A)=0.1$, $P(B)=0.2$ and $P(A\cap B)=0.05$. Compute \$P(A \cup B) \$ Argue for why $P(A\cup B)=P(A)+P(B)-P(A\cap B)$ makes sense.

<details>

<summary>Show solution</summary>

We already know the formula used for the computation.

$$ P(A\cup B)=0.1+0.2-0.05=0.25 $$

To argue for why the formula makes sense, let's first divide up the union between A and B into disjoint parts. We know that the union includes any element within either A, B or both. We will have one part which is A and *not* B, and these will can be represented as intersections in and of themselves, i.e. $A\cap B^c$ and $A^c \cap B$. The final part of the union is when *both* A and B occurr, meaning our intersection $A\cap B$. With this we can find a formula for the union $$ P(A\cup B)=P(A\cap B^c)+P(A^c\cap B)+P(A\cap B) \ (*)$$ Now, let's consider $P(A)$ and $P(B)$. Any set $A$ can necessarily be constructed by the union of it's intersection with another set $B$ and the complement of that set $B^c$ on the sample space $S$. This is very technical, but the idea is that every element (possible outcome) within $A$ will either also be in $B$, or not, hence ion $B^c$. This idea let's us rewrite $P(A)+P(B)$, and we will represent them as sums similar to what we have in $(*)$.

$$\begin{align*}P(A)+P(B)=P(A\cap B)+P(A\cap B^c)+P(A\cap B)+P(A^c\cap B)=2P(A\cap B)+P(A\cap B^c)+P(A\cap B) \ (**)\end{align*}$$

It's plain to see that that $(**)-P(A\cap B)=(*)$

$$\therefore P(A\cup B)=P(A)+P(B)-P(A\cap B) $$

</details>

### Problem 20

You get that $P(B)=0.5$, $P(A\cap B)=0.4$. Compute $P(A|B)$.

<details>

<summary>Show solutions</summary>

We compute the conditional probability using the definition.

$$ P(A|B)=\frac{P(A\cap B)}{P(B)}=\frac{0.4}{0.5}=0.8 $$

</details>

### Problem 21

You get that $P(B)=0.2$, $P(A | B)=0.9$. Compute $P(A\cap B)$.

<details>

<summary>Show solution</summary>

We make use of the definition here to compute the probability.

$$
\begin{align*} 
P(A|B)=\frac{P(A\cap B)}{P(B)} \Rightarrow P(A\cap B)=P(A|B)P(B) \\
P(A|B)=0.9*0.2=0.18
\end{align*}
$$

</details>

### Problem 22

You find that $P(A)=0.6, \ P(B)=0.3$ and $P(A|B)+P(B|A)=0.9$. Find $P(A\cap B)$.

<details>

<summary>Show solutions</summary>

Let's try to rewrite $P(A|B)+P(B|A)=0.9$ and solve for $P(A\cap B)$.

$$\begin{align*}
P(A|B)+P(B|A)=0.9 \Leftrightarrow \frac{P(A\cap B)}{P(B)}+\frac{P(A\cap B)}{P(A)}=\frac{9}{10} \\
\Leftrightarrow \frac{10}{3}P(A\cap B)+\frac{10}{6}P(A\cap B)=\frac{9}{10} \\
\Leftrightarrow \frac{30}{6}P(A\cap B)=\frac{9}{10} \Leftrightarrow 5P(A\cap B)=\frac{9}{10} \\
\therefore P(A\cap B)=\frac{9}{50}=0.18
\end{align*}$$

</details>

### Problem 23

You get that $P((A\cup B)^c)=\frac{1}{2}$ and that $P(A\cap B)=\frac{3}{20}$ $C$ is the event that either $A$ or $B$ occurs, but *not* both. What is $P(C)$?

<details>

<summary>Show solution</summary>

First we should find a way to express $P(C)$. The probability of $E$ occurring is obviously related to the probabilities of $A$ and $B$ occrring. but we have to *remove* any cases where both of them occur, i.e. the intersection. Recall from problem 19 as we can construct an event $A$ as all the outcomes *shared with* $B$ ($P(A\cap B)$) and all the outcomes which aren't shared, or shared with *not* $B$ (the complement $P(A\cap B^c)$), in the same sample space. I.e.

$$ P(A)=P(A\cap B)+P(A\cap B^c) $$

The same can be done for $B$. Now to construct $P(C)$ we can remove the respective $P(A\cap B)$ andb sum the remaining probabilities.

$$P(C)=P(A\cap B^c)+P(A^c\cap B)=P(A\cup B)-P(A\cap B)$$ Now we can finally start relating this to the probabilities given in the problem. We need to recall the complement rule, before we can compute however.

$$ P(A)=1-P(A^c) \Rightarrow P(A\cup B)=1-P((A\cup B)^c)$$

We can substitute this in to the expression for $P(C)$

$$ P(C)=1-P((A\cup B)^c)-P(A\cap B)=1-\frac{1}{2}-\frac{3}{20}=\frac{7}{20}=0.35 $$

</details>

### Problem 24

Prove that for any event $A$ such that $A$ is independent of another event $B$, show that

$$P(A|B)=P(A)$$

<details>

<summary>Show solution</summary>

Consider first the definition of independence.

$$ P(A\cap B)=P(A)P(B) \ (*)$$

Now let's also consider the definition of conditional probability.

$$ P(A|B)=\frac{P(A\cap B)}{P(B)} $$

Finally let's substitute in the definition given in $(*)$ and reduce the fraction.

$$ P(A|B)=\frac{P(A)P(B)}{P(B)}=P(A) \  \  \  \ \  \ q.e.d.$$

</details>

### Problem 25

You find that $P(A)=\frac{3}{10}$, $P(B)=\frac{1}{2}$ and $P(A\cap B)=\frac{3}{10}$

Are events $A$ and $B$ independent?

<details>

<summary>Show solution</summary>

We check if the probabilities fulfill the conditions given by the definition.

$$ P(A)P(B)=\frac{3}{10}\frac{1}{2}=\frac{3}{20}\neq\frac{3}{10}=P(A\cap B) $$ I.e. A and B are *not* independent.

</details>

### Problem 26

You find that $P(A)=\frac{5}{8}$, $P(B)=\frac{1}{5}$ and $P(A\cap B)=\frac{1}{8}$

Are events $A$ and $B$ independent?

<details>

<summary>Show solution</summary>

We check if the probabilities fulfill the conditions given by the definition.

$$ P(A)P(B)=\frac{5}{8}\frac{1}{5}=\frac{1}{8}=P(A\cap B) $$ I.e. A and B are independent.

</details>

### Problem 26

You find that $P(A)=\frac{5}{8}$, $P(B)=\frac{1}{5}$ and $P(A\cap B)=\frac{1}{8}$

Are events $A$ and $B$ independent?

<details>

<summary>Show solution</summary>

We check if the probabilities fulfill the conditions given by the definition.

$$ P(A)P(B)=\frac{5}{8}\frac{1}{5}=\frac{1}{8}=P(A\cap B) $$ I.e. A and B are independent.

</details>

### Problem 27

You find that $P(A)=1$, $P(B)=0.2$ and $P(A\cap B)=0.125$

Are events $A$ and $B$ independent?

<details>

<summary>Show solution</summary>

We check if the probabilities fulfill the conditions given by the definition.

$$ P(A)P(B)=1*0.2=0.2\neq 0.125=P(A\cap B) $$ I.e. A and B are *not* independent.

</details>

### Problem 28

You find that $P(A)=\frac{3}{4}$ and $P(A\cap B)=\frac{1}{10}$

What must $P(B)$ be for the events $A$ and $B$ to be independent.

<details>

<summary>Show solution</summary>

Our criterion follows from the definition.

$$P(A\cap B)=P(A)P(B) \Rightarrow P(B)=\frac{P(A\cap B)}{P(A)} $$ Now we can compute the probability.

$$ P(B)=\frac{\frac{1}{10}}{\frac{3}{4}}=\frac{1}{10}\frac{4}{3}=\frac{4}{30}=\frac{2}{15} $$

So $P(B)$ should be $\frac{2}{15}\approx0.133$

</details>

### Proplem 29

You have $P(A|B)=0.3$, $P(A)=0.4$, $P(B)=0.1$. Compute the probability $P(B|A)$.

<details>

<summary>Show solution</summary>

Here we can simply compute using Bayes rule!

$$ P(B|A)=\frac{P(A|B)P(B)}{P(A)} $$ Let's compute:

$$ P(B|A)=\frac{0.3*0.1}{0.4}=\frac{0.03}{0.4}=2.5*0.03=0.075$$

So we conclude the probability of B given A is 7.5%

</summary>

### Problem 30

A certain disease affects 1% of a population. A test for the disease is 90% accurate for those who have the disease (i.e., it correctly identifies 90% of sick people) but also has a 5% false positive rate (i.e., it incorrectly identifies 5% of healthy people as having the disease).

If a randomly chosen person tests positive, what is the probability that they actually have the disease?

Following, what would be the probability that someone testing positive is *not* sick?

Would you consider this to be a good test?

<details>

<summary>Show solution</summary>

Let S be the event that a random person is suffering from the unnamed disease. Let T be the event that a random person gets a positive test. With this in mind, let's try to identify what is given in the text.

$$ \begin{align*}
P(S)=0.01 \Rightarrow P(S^c)=0.99 \\
P(T|S)=0.90 \\
P(T|S^c)=0.05 
\end{align*}$$

We are now looking for then probability that a person who tests positive is sick. i.e. $P(S|T)$.

$$\begin{align*}
P(S|T)=\frac{P(S\cap T)}{P(S)}=\frac{P(T|S)P(S)}{P(T)}
\end{align*}$$

We already know the values of $P(T|S)$ and $P(S)$. Remains to find $P(T)$.

We can attempt to use the law of total probability to find $P(T)$.

$$ P(T)=P(T|S)P(S)+P(T|S^c)P(S^c)=0.90*0.01+0.05*0.99=0.009+0.0495=0.0585 $$

Finally; we can compute $P(S|T)$

$$P(S|T)=\frac{0.90*0.01}{0.0585}\approx0.154$$

Let's now find the complement, i.e. $P(S^c|t)=1-P(S|T)$.

$$P(S^c|T)\approx1-0.154=0.846$$

This should in no way be considered. When only 15% of cases report a true positive, that's a bad sign. The false positive that constitutes about 85% of the positive tests is what we would call *Type I errors*. There is no *one* answer to what's good level of Type I errors to have is when doing a test, but the most common benchmark is 5% (commonly denoted as $\alpha=0.05$, and is usually called the significance level of a test).

</details>

### Problem 31

Let $A$ and $B$ be disjoint events. What is $P(A\cap B)$? Feel free to draw a venn diagram to visualise this concept.

<details>

<summary>Show solution</summary>

When $A$ and $B$ disjoint, we get that

$$A\cap B=\emptyset \Rightarrow P(A\cap B)=0$$

Let's plug this into the definition of a conditional probability.

$$ P(A|B)=\frac{P(A\cap B)}{P(B)}=\frac{0}{P(B)}=0 $$
