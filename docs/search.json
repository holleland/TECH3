[
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "pythonbasics.html",
    "href": "pythonbasics.html",
    "title": "Python",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\n\n\nFigure 1: A line plot on a polar axis"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction",
    "section": "",
    "text": "Welcome to the website for TECH3 Applied Statistics. We will use this website as a supplement to lectures. The website is ongoing development, so not all subjects will have content yet. Below you will find a detailed (preliminary) lecture plan, link to the textbook and curriculum. The course description can be found here.\n\n\n\n\n\n\n\nWeek\nModule\nMonday 14:15 - 16:00\nThursday 14:15 - 16:00\n\n\n\n\n2\nModule 1\n06.01: Introduction to TECH3/Overview lecture in Aud D\n09.01: Collaborative learning session in Aud J\n\n\n3\nModule 1\n13.01: Practical session in Aud C.\n16.01: Case session in Aud J\n\n\n4\nModule 2\n20.01: Overview lecture in Aud C\n23.01: Collaborative learning session in Aud J\n\n\n5\nModule 2\n27.01: Overview lecture in Aud C\n30.01: Exercise session in Aud J\n\n\n6\nModule 2\n03.02: No lecture\n06.02: Case session in Aud J\n\n\n7\nModule 3\n10.02: Overview lecture in Aud C\n13.02: Collaborative learning session in Aud J\n\n\n8\nModule 3\n17.02: Overview lecture in Aud C\n20.02: No lecture.\n\n\n9\nModule 3\n24.02: Practical session in Aud C.\n27.02: Case session in Aud J\n\n\n10\nModule 4\n03:03: Overview lecture in Aud C\n06.03: Oracle session in Aud J\n\n\n11\nModule 4\n10:03: Overview lecture in Aud C\n13.03: Collaborative learning session in Aud J.\n\n\n12\nModule 4\n17.03: Practical session i Aud C\n20.03: No lecture (Symposium)\n\n\n13\nModule 5\n24.03: Overview lecture in Aud C\n26.03: Case session in Aud J\n\n\n14\nModule 5\n31.03: Overview lecture in Aud C\n03.04: Collaborative learning session in Aud J\n\n\n15\nModule 5\n07.04: Practical session in Aud C\n10.04: Case session in Aud J\n\n\n16\nExam preparations\n14.04: No lecture (Easter)\n17.04: No lecture (Easter)\n\n\n17\nExam preparations\n21.04: No lecture (Easter)\n24.04: Exam prepartion session? in Aud J\n\n\n\n\n\n\n\n\n\n\n\nStatistical Thinking in the 21st Century\nPython Companion to Statistical Thinking in the 21st Century\n\n\n\n\nAll the material on this website, including chapters 1-10, 12-14, and 17 of Statistical Thinking in the 21st Century and Python Companion to Statistical Thinking in the 21st Century.\n\n\n\nUpon completing the course, the students can:\n\n\n\nUnderstand basic statistical theory and corresponding methods, and how to apply this knowledge in practical situations.\n\n\n\n\n\nExplore data using software that can summarize and visualize data.\nMaster basic probability theory.\nMake inferences about an entire population based on a sample of individuals from that population using both classical statistical methods and modern resampling techniques.\nDesign basic experiments, perform hypothesis testing, and quantify effects.\nMeasure relationships between both categorical and continuous variables.\nFit and evaluate regression models for both inference and prediction.\n\n\n\n\n\nIdentify and solve statistical problems.\nPerform basic data analysis using modern computer tools.\nPerform data-driven decision-making for a sustainable future."
  },
  {
    "objectID": "index.html#lecture-plan",
    "href": "index.html#lecture-plan",
    "title": "Introduction",
    "section": "",
    "text": "Week\nModule\nMonday 14:15 - 16:00\nThursday 14:15 - 16:00\n\n\n\n\n2\nModule 1\n06.01: Introduction to TECH3/Overview lecture in Aud D\n09.01: Collaborative learning session in Aud J\n\n\n3\nModule 1\n13.01: Practical session in Aud C.\n16.01: Case session in Aud J\n\n\n4\nModule 2\n20.01: Overview lecture in Aud C\n23.01: Collaborative learning session in Aud J\n\n\n5\nModule 2\n27.01: Overview lecture in Aud C\n30.01: Exercise session in Aud J\n\n\n6\nModule 2\n03.02: No lecture\n06.02: Case session in Aud J\n\n\n7\nModule 3\n10.02: Overview lecture in Aud C\n13.02: Collaborative learning session in Aud J\n\n\n8\nModule 3\n17.02: Overview lecture in Aud C\n20.02: No lecture.\n\n\n9\nModule 3\n24.02: Practical session in Aud C.\n27.02: Case session in Aud J\n\n\n10\nModule 4\n03:03: Overview lecture in Aud C\n06.03: Oracle session in Aud J\n\n\n11\nModule 4\n10:03: Overview lecture in Aud C\n13.03: Collaborative learning session in Aud J.\n\n\n12\nModule 4\n17.03: Practical session i Aud C\n20.03: No lecture (Symposium)\n\n\n13\nModule 5\n24.03: Overview lecture in Aud C\n26.03: Case session in Aud J\n\n\n14\nModule 5\n31.03: Overview lecture in Aud C\n03.04: Collaborative learning session in Aud J\n\n\n15\nModule 5\n07.04: Practical session in Aud C\n10.04: Case session in Aud J\n\n\n16\nExam preparations\n14.04: No lecture (Easter)\n17.04: No lecture (Easter)\n\n\n17\nExam preparations\n21.04: No lecture (Easter)\n24.04: Exam prepartion session? in Aud J"
  },
  {
    "objectID": "index.html#literature",
    "href": "index.html#literature",
    "title": "Introduction",
    "section": "",
    "text": "Statistical Thinking in the 21st Century\nPython Companion to Statistical Thinking in the 21st Century"
  },
  {
    "objectID": "index.html#curriculum",
    "href": "index.html#curriculum",
    "title": "Introduction",
    "section": "",
    "text": "All the material on this website, including chapters 1-10, 12-14, and 17 of Statistical Thinking in the 21st Century and Python Companion to Statistical Thinking in the 21st Century."
  },
  {
    "objectID": "index.html#learning-outcomes",
    "href": "index.html#learning-outcomes",
    "title": "Introduction",
    "section": "",
    "text": "Upon completing the course, the students can:\n\n\n\nUnderstand basic statistical theory and corresponding methods, and how to apply this knowledge in practical situations.\n\n\n\n\n\nExplore data using software that can summarize and visualize data.\nMaster basic probability theory.\nMake inferences about an entire population based on a sample of individuals from that population using both classical statistical methods and modern resampling techniques.\nDesign basic experiments, perform hypothesis testing, and quantify effects.\nMeasure relationships between both categorical and continuous variables.\nFit and evaluate regression models for both inference and prediction.\n\n\n\n\n\nIdentify and solve statistical problems.\nPerform basic data analysis using modern computer tools.\nPerform data-driven decision-making for a sustainable future."
  },
  {
    "objectID": "case-2.html",
    "href": "case-2.html",
    "title": "Case 2",
    "section": "",
    "text": "Case 2"
  },
  {
    "objectID": "calender.html",
    "href": "calender.html",
    "title": "Calender",
    "section": "",
    "text": "Calender\n\n\n\n\n\nWeek\nModule\nMonday 14:15 - 16:00\nThursday 14:15 - 16:00\n\n\n\n\n2\nModule 1\n06.01: Introduction to TECH3/Overview lecture in Aud D\n09.01: Collaborative learning session in Aud J\n\n\n3\nModule 1\n13.01: Practical session in Aud C.\n16.01: Case session in Aud J\n\n\n4\nModule 2\n20.01: Overview lecture in Aud C\n23.01: Collaborative learning session in Aud J\n\n\n5\nModule 2\n27.01: Overview lecture in Aud C\n30.01: Exercise session in Aud J\n\n\n6\nModule 2\n03.02: No lecture\n06.02: Case session in Aud J\n\n\n7\nModule 3\n10.02: Overview lecture in Aud C\n13.02: Collaborative learning session in Aud J\n\n\n8\nModule 3\n17.02: Overview lecture in Aud C\n20.02: No lecture.\n\n\n9\nModule 3\n24.02: Practical session in Aud C.\n27.02: Case session in Aud J\n\n\n10\nModule 4\n03:03: Overview lecture in Aud C\n06.03: Oracle session in Aud J\n\n\n11\nModule 4\n10:03: Overview lecture in Aud C\n13.03: Collaborative learning session in Aud J.\n\n\n12\nModule 4\n17.03: Practical session i Aud C\n20.03: No lecture (Symposium)\n\n\n13\nModule 5\n24.03: Overview lecture in Aud C\n26.03: Case session in Aud J\n\n\n14\nModule 5\n31.03: Overview lecture in Aud C\n03.04: Collaborative learning session in Aud J\n\n\n15\nModule 5\n07.04: Practical session in Aud C\n10.04: Case session in Aud J\n\n\n16\nExam preparations\n14.04: No lecture (Easter)\n17.04: No lecture (Easter)\n\n\n17\nExam preparations\n21.04: No lecture (Easter)\n24.04: Exam prepartion session? in Aud J"
  },
  {
    "objectID": "5-videos.html",
    "href": "5-videos.html",
    "title": "Videos",
    "section": "",
    "text": "Videos"
  },
  {
    "objectID": "5-simpsons-paradox.html",
    "href": "5-simpsons-paradox.html",
    "title": "Simpson’s paradox",
    "section": "",
    "text": "Slides for “Simpson’s paradox”\n\n\n\nWhat is Simpson’s paradox?\nHow does it occur?",
    "crumbs": [
      "Modules",
      "Module 5: Measuring relationships and fitting models",
      "Simpson's paradox"
    ]
  },
  {
    "objectID": "5-simpsons-paradox.html#control-questions",
    "href": "5-simpsons-paradox.html#control-questions",
    "title": "Simpson’s paradox",
    "section": "",
    "text": "What is Simpson’s paradox?\nHow does it occur?",
    "crumbs": [
      "Modules",
      "Module 5: Measuring relationships and fitting models",
      "Simpson's paradox"
    ]
  },
  {
    "objectID": "5-logistic-regression.html",
    "href": "5-logistic-regression.html",
    "title": "Logistic regression",
    "section": "",
    "text": "“Logistic regression”\n\n\n\n\nWhen do we need logistic regression?\nWhat are we modelling when doing logistic regression?\nHow do you interpret \\(\\beta_x\\) in a logistic regression?",
    "crumbs": [
      "Modules",
      "Module 5: Measuring relationships and fitting models",
      "Logistic regression"
    ]
  },
  {
    "objectID": "5-logistic-regression.html#slides",
    "href": "5-logistic-regression.html#slides",
    "title": "Logistic regression",
    "section": "",
    "text": "“Logistic regression”",
    "crumbs": [
      "Modules",
      "Module 5: Measuring relationships and fitting models",
      "Logistic regression"
    ]
  },
  {
    "objectID": "5-logistic-regression.html#control-questions",
    "href": "5-logistic-regression.html#control-questions",
    "title": "Logistic regression",
    "section": "",
    "text": "When do we need logistic regression?\nWhat are we modelling when doing logistic regression?\nHow do you interpret \\(\\beta_x\\) in a logistic regression?",
    "crumbs": [
      "Modules",
      "Module 5: Measuring relationships and fitting models",
      "Logistic regression"
    ]
  },
  {
    "objectID": "5-least-squares-estimation.html",
    "href": "5-least-squares-estimation.html",
    "title": "Least Squares Estimation",
    "section": "",
    "text": "Slides for “Least Squares Estimation”"
  },
  {
    "objectID": "5-exercises.html",
    "href": "5-exercises.html",
    "title": "Exercises",
    "section": "",
    "text": "Researchers collected data on the size of their feet for a group of right-handed male and females and counted how many of each sex had a left-foot larger than the right, equal feet or right foot larger than the left. The results are given in the table here:\n\n\n\nSex\nL&gt;R\nL=R\nL&lt;R\nTotal\n\n\n\n\nMen\n2\n10\n28\n40\n\n\nWomen\n55\n18\n14\n87\n\n\n\nDoes the data indicate that gender has a strong effect on the development of foot asymmetry? Specify the null- and alternative hypothesis, compute the \\(\\chi^2\\) test statistic and obtain the p-value.\n\n\n\nA company wants to assess whether customer response (Yes/No) to a marketing campaign depends on the platform used (Email, SMS, Social Media).\n\n\n\nResponse\nEmail\nSMS\nSocial Media\n\n\n\n\nYes\n120\n45\n80\n\n\nNo\n180\n105\n120\n\n\n\n\nFormulate the null and alternative hypotheses.\nCompute the expected frequencies.\nCalculate the chi-squared test statistic.\nDetermine the degrees of freedom.\nAt a 5% significance level, is there evidence to suggest the response rate depends on the platform?\n\n\n\n\nThe Norwegian Labour and Welfare Administration (NAV) organizes online job application workshops on Teams. The course responsible has noticed that except for himself, the attendees that have their camera on during the course is mostly the older participants. He wonders if there is a relationship between participants having camera on and participant’s age group? He therefore takes a screenshot of the meeting (that he deletes afterwards, of course) and counts the number of faces he sees for the different ages. The results are given in the table below. Perform a chi-squared test to answer this question, including setting up the hypothesis, finding the p-value and making your conclusion. Remember to settle on a significance level first.\n|Camera on| 18–35| 36–53| 54-70| Total| |Yes| 4| 3| 20| 27| No |20| 13| 39| 72| Total |24| 16| 59 |99|\nIs there a Simpson’s paradox here?",
    "crumbs": [
      "Modules",
      "Module 5: Measuring relationships and fitting models",
      "Exercises"
    ]
  },
  {
    "objectID": "5-exercises.html#contingency-tables-problems",
    "href": "5-exercises.html#contingency-tables-problems",
    "title": "Exercises",
    "section": "",
    "text": "Researchers collected data on the size of their feet for a group of right-handed male and females and counted how many of each sex had a left-foot larger than the right, equal feet or right foot larger than the left. The results are given in the table here:\n\n\n\nSex\nL&gt;R\nL=R\nL&lt;R\nTotal\n\n\n\n\nMen\n2\n10\n28\n40\n\n\nWomen\n55\n18\n14\n87\n\n\n\nDoes the data indicate that gender has a strong effect on the development of foot asymmetry? Specify the null- and alternative hypothesis, compute the \\(\\chi^2\\) test statistic and obtain the p-value.\n\n\n\nA company wants to assess whether customer response (Yes/No) to a marketing campaign depends on the platform used (Email, SMS, Social Media).\n\n\n\nResponse\nEmail\nSMS\nSocial Media\n\n\n\n\nYes\n120\n45\n80\n\n\nNo\n180\n105\n120\n\n\n\n\nFormulate the null and alternative hypotheses.\nCompute the expected frequencies.\nCalculate the chi-squared test statistic.\nDetermine the degrees of freedom.\nAt a 5% significance level, is there evidence to suggest the response rate depends on the platform?\n\n\n\n\nThe Norwegian Labour and Welfare Administration (NAV) organizes online job application workshops on Teams. The course responsible has noticed that except for himself, the attendees that have their camera on during the course is mostly the older participants. He wonders if there is a relationship between participants having camera on and participant’s age group? He therefore takes a screenshot of the meeting (that he deletes afterwards, of course) and counts the number of faces he sees for the different ages. The results are given in the table below. Perform a chi-squared test to answer this question, including setting up the hypothesis, finding the p-value and making your conclusion. Remember to settle on a significance level first.\n|Camera on| 18–35| 36–53| 54-70| Total| |Yes| 4| 3| 20| 27| No |20| 13| 39| 72| Total |24| 16| 59 |99|\nIs there a Simpson’s paradox here?",
    "crumbs": [
      "Modules",
      "Module 5: Measuring relationships and fitting models",
      "Exercises"
    ]
  },
  {
    "objectID": "5-correlation-and-causation.html",
    "href": "5-correlation-and-causation.html",
    "title": "Correlation and causation",
    "section": "",
    "text": "Slides for “Correlation and causation”\n\n\n\nIs heavier rain correlated with more umbrellas in the street?\nWhat is the causal relationship between heavier rain and proportion of umbrellas in the street?\nWhy do we use controlled experiments?\nWhy is randomization important in this context?\nWhat is the difference between an observational study and an experimental one?\nWhat is a confounder?",
    "crumbs": [
      "Modules",
      "Module 5: Measuring relationships and fitting models",
      "Correlation and causation"
    ]
  },
  {
    "objectID": "5-correlation-and-causation.html#control-questions",
    "href": "5-correlation-and-causation.html#control-questions",
    "title": "Correlation and causation",
    "section": "",
    "text": "Is heavier rain correlated with more umbrellas in the street?\nWhat is the causal relationship between heavier rain and proportion of umbrellas in the street?\nWhy do we use controlled experiments?\nWhy is randomization important in this context?\nWhat is the difference between an observational study and an experimental one?\nWhat is a confounder?",
    "crumbs": [
      "Modules",
      "Module 5: Measuring relationships and fitting models",
      "Correlation and causation"
    ]
  },
  {
    "objectID": "5-contingency-tables.html",
    "href": "5-contingency-tables.html",
    "title": "Contingency tables",
    "section": "",
    "text": "Slides for “Contingency tables”\n\n\n\nWhat is a contingency table?\nWhat do we test when doing a chi-squared test on contingency tables?\nWhat do we \\(P(S\\cup B)=P(S)\\cdot P(B)\\) under the null hypothesis?",
    "crumbs": [
      "Modules",
      "Module 5: Measuring relationships and fitting models",
      "Contingency tables"
    ]
  },
  {
    "objectID": "5-contingency-tables.html#control-questions",
    "href": "5-contingency-tables.html#control-questions",
    "title": "Contingency tables",
    "section": "",
    "text": "What is a contingency table?\nWhat do we test when doing a chi-squared test on contingency tables?\nWhat do we \\(P(S\\cup B)=P(S)\\cdot P(B)\\) under the null hypothesis?",
    "crumbs": [
      "Modules",
      "Module 5: Measuring relationships and fitting models",
      "Contingency tables"
    ]
  },
  {
    "objectID": "5-assessing-the-model.html",
    "href": "5-assessing-the-model.html",
    "title": "Assessing the model",
    "section": "",
    "text": "“Goodness of fit”\n“Criticizing the model and checking assumptions”\n\n\n\n\n\nWhat is the interpretation of \\(R^2=0.80\\)?\nIf the correlation between \\(X\\) and \\(Y\\) is \\(0.5\\), what is \\(R^2\\) in the regression \\(Y=\\beta_0+\\beta_1 X+\\epsilon\\)?\nWhat are the assumptions of a linear regression?\nHow do we assess whether the linearity assumption is fulfilled?\nHow do we assess whether the residuals are homoskedastic (have constant variance)?\nWhat is QQ-plot and what do we use it for?\nDo we need normality of the errors when estimating coefficients?\nDo we need normality when making prediction intervals?",
    "crumbs": [
      "Modules",
      "Module 5: Measuring relationships and fitting models",
      "Assessing the model"
    ]
  },
  {
    "objectID": "5-assessing-the-model.html#slides",
    "href": "5-assessing-the-model.html#slides",
    "title": "Assessing the model",
    "section": "",
    "text": "“Goodness of fit”\n“Criticizing the model and checking assumptions”",
    "crumbs": [
      "Modules",
      "Module 5: Measuring relationships and fitting models",
      "Assessing the model"
    ]
  },
  {
    "objectID": "5-assessing-the-model.html#control-questions",
    "href": "5-assessing-the-model.html#control-questions",
    "title": "Assessing the model",
    "section": "",
    "text": "What is the interpretation of \\(R^2=0.80\\)?\nIf the correlation between \\(X\\) and \\(Y\\) is \\(0.5\\), what is \\(R^2\\) in the regression \\(Y=\\beta_0+\\beta_1 X+\\epsilon\\)?\nWhat are the assumptions of a linear regression?\nHow do we assess whether the linearity assumption is fulfilled?\nHow do we assess whether the residuals are homoskedastic (have constant variance)?\nWhat is QQ-plot and what do we use it for?\nDo we need normality of the errors when estimating coefficients?\nDo we need normality when making prediction intervals?",
    "crumbs": [
      "Modules",
      "Module 5: Measuring relationships and fitting models",
      "Assessing the model"
    ]
  },
  {
    "objectID": "4-textbook.html",
    "href": "4-textbook.html",
    "title": "TECH3 Applied statistics",
    "section": "",
    "text": "The following is a redistribution of chapters 9 and 10  of Statistical Thinking for the 21st Century by Russell A. Poldrack, 2019, under the CC BY-NC 4.0 licence.",
    "crumbs": [
      "Modules",
      "Module 4: Designing studies, hypothesis testing, and quantifying effects",
      "Textbook"
    ]
  },
  {
    "objectID": "4-textbook.html#hypothesis-testing",
    "href": "4-textbook.html#hypothesis-testing",
    "title": "TECH3 Applied statistics",
    "section": "Hypothesis testing",
    "text": "Hypothesis testing",
    "crumbs": [
      "Modules",
      "Module 4: Designing studies, hypothesis testing, and quantifying effects",
      "Textbook"
    ]
  },
  {
    "objectID": "4-textbook.html#quantifying-effects-and-designing-studies",
    "href": "4-textbook.html#quantifying-effects-and-designing-studies",
    "title": "TECH3 Applied statistics",
    "section": "Quantifying effects and designing studies",
    "text": "Quantifying effects and designing studies",
    "crumbs": [
      "Modules",
      "Module 4: Designing studies, hypothesis testing, and quantifying effects",
      "Textbook"
    ]
  },
  {
    "objectID": "4-textbook.html#comparing-means",
    "href": "4-textbook.html#comparing-means",
    "title": "TECH3 Applied statistics",
    "section": "Comparing means",
    "text": "Comparing means",
    "crumbs": [
      "Modules",
      "Module 4: Designing studies, hypothesis testing, and quantifying effects",
      "Textbook"
    ]
  },
  {
    "objectID": "4-hypothesis-testing.html",
    "href": "4-hypothesis-testing.html",
    "title": "Hypothesis testing",
    "section": "",
    "text": "Hypothesis testing\n… no scientific worker has a fixed level of significance at which from year to year, and in all circumstances, he rejects hypotheses; he rather gives his mind to each particular case in the light of his evidence and his ideas.\nSir Ronald A. Fisher (1956)\n\nRecommended reading\n\nMindless statistics by Gerd Gigerenzer, Journal of Socio-Economics",
    "crumbs": [
      "Modules",
      "Module 4: Designing studies, hypothesis testing, and quantifying effects",
      "Hypothesis testing"
    ]
  },
  {
    "objectID": "3-what-is-an-estimator.html",
    "href": "3-what-is-an-estimator.html",
    "title": "What is an estimator?",
    "section": "",
    "text": "Slides for “What is an estimator?”\n\n\n\nWhat is an estimator?\nWhat is an estimate?\nWhat does it mean that an estimator is unbiased?\nWhat about a consistent estimator?\nIs the sample mean \\(\\bar{X}_n\\) unbiased and/or consistent?\nIs the sample variance \\(S_n^2\\) unbiased and/or consistent?",
    "crumbs": [
      "Modules",
      "Module 3: Estimation, sampling distributions and resampling",
      "What is an estimator?"
    ]
  },
  {
    "objectID": "3-what-is-an-estimator.html#control-questions",
    "href": "3-what-is-an-estimator.html#control-questions",
    "title": "What is an estimator?",
    "section": "",
    "text": "What is an estimator?\nWhat is an estimate?\nWhat does it mean that an estimator is unbiased?\nWhat about a consistent estimator?\nIs the sample mean \\(\\bar{X}_n\\) unbiased and/or consistent?\nIs the sample variance \\(S_n^2\\) unbiased and/or consistent?",
    "crumbs": [
      "Modules",
      "Module 3: Estimation, sampling distributions and resampling",
      "What is an estimator?"
    ]
  },
  {
    "objectID": "3-textbook.html",
    "href": "3-textbook.html",
    "title": "TECH3 Applied statistics",
    "section": "",
    "text": "The following is a redistribution of chapters 5, 7 and 8 of Statistical Thinking for the 21st Century by Russell A. Poldrack, 2019, under the CC BY-NC 4.0 licence.",
    "crumbs": [
      "Modules",
      "Module 3: Estimation, sampling distributions and resampling",
      "Textbook"
    ]
  },
  {
    "objectID": "3-textbook.html#fitting-models-to-data",
    "href": "3-textbook.html#fitting-models-to-data",
    "title": "TECH3 Applied statistics",
    "section": "Fitting models to data",
    "text": "Fitting models to data",
    "crumbs": [
      "Modules",
      "Module 3: Estimation, sampling distributions and resampling",
      "Textbook"
    ]
  },
  {
    "objectID": "3-textbook.html#sampling",
    "href": "3-textbook.html#sampling",
    "title": "TECH3 Applied statistics",
    "section": "Sampling",
    "text": "Sampling",
    "crumbs": [
      "Modules",
      "Module 3: Estimation, sampling distributions and resampling",
      "Textbook"
    ]
  },
  {
    "objectID": "3-textbook.html#resampling-and-simulation",
    "href": "3-textbook.html#resampling-and-simulation",
    "title": "TECH3 Applied statistics",
    "section": "Resampling and Simulation",
    "text": "Resampling and Simulation",
    "crumbs": [
      "Modules",
      "Module 3: Estimation, sampling distributions and resampling",
      "Textbook"
    ]
  },
  {
    "objectID": "3-random-number-generation.html",
    "href": "3-random-number-generation.html",
    "title": "Random number generation",
    "section": "",
    "text": "Slides for “Random number generation”\n\n\n\nWhat is random?\nWhat do we call random numbers generated from an algorithm?\nAre these numbers truly random?\nWhat do mean when we say that numbers generated by the algorithms are deterministic?\nWhat does a setting a seed do?\nCan you explain the inverse transform sampling procedure?",
    "crumbs": [
      "Modules",
      "Module 3: Estimation, sampling distributions and resampling",
      "Random number generation"
    ]
  },
  {
    "objectID": "3-random-number-generation.html#control-questions",
    "href": "3-random-number-generation.html#control-questions",
    "title": "Random number generation",
    "section": "",
    "text": "What is random?\nWhat do we call random numbers generated from an algorithm?\nAre these numbers truly random?\nWhat do mean when we say that numbers generated by the algorithms are deterministic?\nWhat does a setting a seed do?\nCan you explain the inverse transform sampling procedure?",
    "crumbs": [
      "Modules",
      "Module 3: Estimation, sampling distributions and resampling",
      "Random number generation"
    ]
  },
  {
    "objectID": "3-monte-carlo-simulation.html",
    "href": "3-monte-carlo-simulation.html",
    "title": "Monte Carlo simulation",
    "section": "",
    "text": "Slides for “Monte Carlo simulation”\n\n\n\nWhat is the purpose of using Monte Carlo simulations?\nWhat does the law of large numbers have to do with Monte Carlo simulations?\nWhat is the expected total score after the 6th round in Yatzi?\nWhat is the probabilty of achieving the bonus in Yatzi?",
    "crumbs": [
      "Modules",
      "Module 3: Estimation, sampling distributions and resampling",
      "Monte Carlo simulation"
    ]
  },
  {
    "objectID": "3-monte-carlo-simulation.html#control-questions",
    "href": "3-monte-carlo-simulation.html#control-questions",
    "title": "Monte Carlo simulation",
    "section": "",
    "text": "What is the purpose of using Monte Carlo simulations?\nWhat does the law of large numbers have to do with Monte Carlo simulations?\nWhat is the expected total score after the 6th round in Yatzi?\nWhat is the probabilty of achieving the bonus in Yatzi?",
    "crumbs": [
      "Modules",
      "Module 3: Estimation, sampling distributions and resampling",
      "Monte Carlo simulation"
    ]
  },
  {
    "objectID": "3-intro.html",
    "href": "3-intro.html",
    "title": "TECH3 Applied statistics",
    "section": "",
    "text": "Estimation, sampling distributions and resampling\n\nFocus: Modern inference using Monte Carlo methods.\nKey topics: Central limit theorem, sampling error of various statistics, Monte Carlo simulation.\nSpecial Emphasis: Distinguish between a population and a sample and between population parameters and sample statistics.",
    "crumbs": [
      "Modules",
      "Module 3: Estimation, sampling distributions and resampling",
      "Introduction"
    ]
  },
  {
    "objectID": "3-central-limit-theorem.html",
    "href": "3-central-limit-theorem.html",
    "title": "Central Limit Theorem",
    "section": "",
    "text": "Slides for “Central limit theorem”\n\n\n\nWhat does the central limit theorem tell us?\nWhat are the assumptions for the central limit theorem?\nWhat is the asymptotic variance of the mean?\nWhat is becoming large in the central limit theorem?\nWill a sum also be normally distributed when the sample size is large?\nDoes the sampled data have to be generated from a normal distribution for the central limit theorem to hold?",
    "crumbs": [
      "Modules",
      "Module 3: Estimation, sampling distributions and resampling",
      "Central Limit Theorem"
    ]
  },
  {
    "objectID": "3-central-limit-theorem.html#control-questions",
    "href": "3-central-limit-theorem.html#control-questions",
    "title": "Central Limit Theorem",
    "section": "",
    "text": "What does the central limit theorem tell us?\nWhat are the assumptions for the central limit theorem?\nWhat is the asymptotic variance of the mean?\nWhat is becoming large in the central limit theorem?\nWill a sum also be normally distributed when the sample size is large?\nDoes the sampled data have to be generated from a normal distribution for the central limit theorem to hold?",
    "crumbs": [
      "Modules",
      "Module 3: Estimation, sampling distributions and resampling",
      "Central Limit Theorem"
    ]
  },
  {
    "objectID": "2-what-is-probability.html",
    "href": "2-what-is-probability.html",
    "title": "What is probability?",
    "section": "",
    "text": "Slides for “What is probability?”\n\n\n\nHow would you interpret an event with probability 0?\nHow would you interpret an event with probability 1?\nWhat is the key assumption behind the assigning probability using the classical method?\nHow would a frequentist interpret a probability?\nWhat is a subjective probability?\nWhat is a Bayesian probability?",
    "crumbs": [
      "Modules",
      "Module 2: Probability, random variables, probability distributions and simulations.",
      "What is probability?"
    ]
  },
  {
    "objectID": "2-what-is-probability.html#controll-questions",
    "href": "2-what-is-probability.html#controll-questions",
    "title": "What is probability?",
    "section": "",
    "text": "How would you interpret an event with probability 0?\nHow would you interpret an event with probability 1?\nWhat is the key assumption behind the assigning probability using the classical method?\nHow would a frequentist interpret a probability?\nWhat is a subjective probability?\nWhat is a Bayesian probability?",
    "crumbs": [
      "Modules",
      "Module 2: Probability, random variables, probability distributions and simulations.",
      "What is probability?"
    ]
  },
  {
    "objectID": "2-independent-events.html",
    "href": "2-independent-events.html",
    "title": "Independent events",
    "section": "",
    "text": "Slides for “Independent events”\n\n\n\nWhat is the definition of two independent events?\nExplain how conditional probabilities can be used to check for independence between two events.\nExplain independence with words.",
    "crumbs": [
      "Modules",
      "Module 2: Probability, random variables, probability distributions and simulations.",
      "Independent events"
    ]
  },
  {
    "objectID": "2-independent-events.html#controll-questions",
    "href": "2-independent-events.html#controll-questions",
    "title": "Independent events",
    "section": "",
    "text": "What is the definition of two independent events?\nExplain how conditional probabilities can be used to check for independence between two events.\nExplain independence with words.",
    "crumbs": [
      "Modules",
      "Module 2: Probability, random variables, probability distributions and simulations.",
      "Independent events"
    ]
  },
  {
    "objectID": "2-exercises.html",
    "href": "2-exercises.html",
    "title": "Exercises",
    "section": "",
    "text": "Exercises\n\nProblem 1\nYou have an event your interested in studying, \\(A\\). What are the lower and upper bounds for the probability that \\(A\\) occurs? I.e.what are the lower and upper bounds of \\(P(A)\\)?\n\n\nShow solutions\n\nBy the definition of the probability of an event we know that it’s bounded both from above and below. The upper bound being that \\(A\\) is guaranteed, and the lower bound being that it is impossible. We represent this on the interval \\([0,1]\\). I.e. we know that the probability of even t \\(A\\) occurring is between \\(0\\) and \\(1\\). More compact \\(P(A)\\in[0,1]\\)\n\n\n\nProblem 2\nOne day you walk outside and overhear a person talking about the probability of an event. You hear little of what they have to say, but you do hear this person say “…this won’t have to be enough, we will have to redo this experiment many more times to be certain I am approaching the right probability.” Amazingly, some time later you hear another person mumbling to them selves about probability. “Wow, now this is a surprise. This result did not coincide with my previous beliefs at all. I will have to rethink the probability of such an event occurring”, they say.\nYou get the feeling that these two people have very different way of interpreting probability? Identify who the frequentist and who the bayesian statistician is.\n\n\nShow solutions\n\nHere, person 1 is using frequentistic methods. Person 1’s focus is entirely on getting in as many independent trials as possible to get a clear image of the probability of an event. Person 2 on the other hand has a completely different view. Person 2 obviously had an initial belief about the probability of an event, but after seeing a surprising result they were determined to update their beliefs about the probability of this event.\n\n\n\nProblem 3\nVegard is very interested in the quality of watermelons. He has decided that he wants to find out the probability of watermelons being overripe at REMA 1000. He feels he has two feasible tests he can do to find this probability, one is Bayesian in nature and one is frequentist. Vegard can either go in with an initial belief, buy a single watermelon, then update his beliefs. He can keep doing this until he is quite certain of the probability of the population. On the other hand, Vegard can buy 40 watermelons at once, check them all, and then argue for that sample being representative of the population.\nWhich method is Bayesian and which is frequentist?\n\n\nShow solutions\n\nThe first option here is Bayesian. Vegard has an initial belief on the probability of the watermelons being overripe, and then he iterates and tries to make that more exact by checking one watermelon at the time.\nThe second option is a very typical frequentist test. From the whole population, a decent chunk is tested at once, in hope that it’s a representative sample. In the representative sample overripe watermelons should have the same relative frequency as in the entire population.\n\n\n\nProblem 4\nYou have a sample space, \\(S\\), and in that sample space you have two identifiable events, \\(A\\) and \\(B\\). Can you determine a case where the probability of the intersection of \\(A\\) and \\(B\\) (\\(P(A\\cap B)\\)) occurring, equals the probability of \\(A\\) (\\(P(A)\\)) occurring? Draw a Venn diagram!\n\n\nShow solutions\n\nRecall that the intersection of two events is when they both occur. I.e. for us to be in the intersection in a Venn diagram, both shapes have to cover the same area. There are infinitely variations here, but the important part is that \\(A\\) must be contained within \\(B\\). When this is the case, all of A will be intersected by \\(B\\) (we say that \\(A\\) is a subset of \\(B\\), and the notation is \\(A\\subseteq B\\)). In this case we get: \\[P(A\\cap B)=P(A)\\]\n\n\n\nProblem 5\nYou have a sample space, \\(S\\), and in that sample space you have two identifiable events, \\(A\\) and \\(B\\). Can you determine a case where the probability of the union of \\(A\\) and \\(B\\) (\\(P(A\\cup B)\\)) occurring, equals the probability of \\(B\\) (\\(P(B)\\)) occurring? Draw a Venn diagram!\n\n\nShow solutions\n\nRecall that the union of two events is when either one or both events occur. This means that all area covered by either \\(A\\) or \\(B\\) will be part of the union. We now want the probability of the union occurring to equal the probability of \\(B\\) occurring. Since all area covered by either \\(A\\) or \\(B\\) is part of the union, we can have no excess are beyond \\(B\\), i.e. \\(A\\) should once again be a subset of \\(B\\) (\\(A\\subseteq B\\)). If this is the case we get: \\[P(A\\cup B)=P(B).\\]\n\n\n\nProblem 6\nConsider a situation where you have \\(0&lt;P(A)&lt;P(B)&lt;1\\).\n\nWhat are the lower and upper bounds of \\(P(A\\cap B)\\)?\nWhat are the lower and upper bounds of \\(P(A\\cup B)\\)?\n\n\n\nShow solutions\n\n\nThe intersection tells us that both \\(A\\) and \\(B\\) have to occur, and as such, we the lower one of the two gives us an upper bound, as the one event cant intersect with more than the cases where it itself occurs (draw a Venn diagram to confirm). We know nothing as to whether or not \\(A\\) and \\(B\\) are disjoint, and as this is still a possibility we may not have an intersection, this makes \\(0\\) our lower bound. I.e. \\[P(A\\cap B)\\in[0, P(A)]\\]\nThe union tells us that either \\(A\\) or \\(B\\) or both occurs. As all events where either one or both occurs, the unions lower bound is given by the higher of the two probabilities. We could also be in a situation where \\(A\\cup B\\) cover the whole sample space \\(S\\), and in such a case we would have an upper bound of 1. I.e: \\[P(A\\cup B)\\in[P(B),1]\\]\n\n\n\n\nProblem 7\nWhat is \\(A\\cap B\\) when events \\(A\\) and \\(B\\) are disjoint.\n\n\nShow solutions\n\nWhen the two events \\(A\\) and \\(B\\) are disjoint, there are no possible situation that would fit into the intersection \\(A \\cap B\\). In set notation we would write: \\[A\\cap B=\\emptyset\\] where \\(\\emptyset\\) is called the empty set and denotes a set with no elements.\n\n\n\nProblem 8\nYou throw a fair six-sided dice, compute the probabilities of the following events.\n\nYou roll 1\nYou roll 3 or 4\nYou roll an odd number\nYou roll a number that’s greater than 5\nYou roll a number that’s less than or equal to 10\n\n\n\nShow solutions\n\nA fair 6 sided die gives us the sample space \\(S=\\{1,2,3,4,5,6\\}\\) with each respective outcome having the same probability to be tossed. i.e. if A is an event that denotes any one number being tossed, then: \\[\nP(A)=\\frac{1}{6}\\]\n\nRolling a 1 is a single element in the sample space, i.e. \\(P(1)=\\frac{1}{6}\\)\n3 and 4 constitute 2 possible disjoint outcomes, i.e. \\[P(3\\cup 4)=P(3)+P(4)=P\\frac{1}{6}+\\frac{1}{6}=\\frac{2}{6}=\\frac{1}{3}\\]\nThere are 3 different odd numbered outcomes\n\n\\[P(odd)=3\\cdot\\frac{1}{6}=\\frac{1}{2}\\]\n\nThere is only one possible outcome greater than 5, namely 6. I.e.\n\n\\[ P(X&gt;5)=P(6)=\\frac{1}{6}\\]\n\nOur sample space tells us we can only roll integers from 1 to 6, all of which are less than 10.\n\n\\[ P(X\\leq10)=P(1\\cup2\\cup3\\cup4\\cup5\\cup6)=\\sum_{i=1}^6\\frac{1}{6}=1\\]\n\n\n\nProblem 9\nYou flip a coin 5 times as an experiment.\n\nLet event \\(A\\) be getting all heads. What is \\(P(A)\\)?\nLet event \\(B\\) be the complement of \\(A\\), i.e \\(B=A^c\\). What is the probability of getting \\(B\\)?\nWhat is \\(P(A\\cap B)\\)?\nCompute \\(P(A \\cup B)\\), how do you interpret this result?\n\n\n\nShow solutions\n\n\nWe treat each coin flip as independent of all the others, we then also let the probability of getting heads be \\(P(H)=\\frac{1}{2}\\) for each coin toss. Since each toss is independent we can simply multiply them all together to find our final probability:\n\n\\[ P(A)=P(HHHHH)=\\prod_{i=1}^5\\frac{1}{2}=\\left(\\frac{1}{2}\\right)^5=\\frac{1}{2^5}=\\frac{1}{32} \\]\n\n\\(B\\) being the complement of \\(A\\) means that it will occur whenever \\(A\\) doesn’t. It’s also simple to compute as we can use a nifty formula.\n\n\\[\\begin{align}P(A^c)=1-P(A) \\\\\n\\Rightarrow P(B)=P(A^c)=1-P(A)=1-\\frac{1}{32}=\\frac{31}{32}\n\\end{align}\\]\n\nSince \\(A\\) and \\(B\\) are complements, they are by definition disjoint, as at no point will both occur (no intersection!!!). As such, we get a clear implication that \\(A\\cap B=\\emptyset\\) Following from this \\(P(A\\cap B)=0\\)\nFinally we can compute the union. Recall that we have a formula for this.\n\n\\[P(A \\cup B)=P(A)+P(B)-P(A\\cap B) \\]\nWe know all of the relevant probabilities needed for the computation.\n\\[ P(A \\cup B)=\\frac{1}{32}+\\frac{31}{32}-0=1 \\]\nThe probability of the union being 1 means that either \\(A\\), \\(B\\) or \\(A\\cap B\\) will occur. We already know, however that the events are disjoint, and as such there is no intersection. I.e. it’s either \\(A\\) or \\(B\\) that **must occur. Interpretaing this we end up at the quite banal fact that when we do this experiment, we will either* end up with 5 heads, or we will not (any other permutation). In fact this situation illustrates a powerful concept concerning complements of events, being that if an event does not occur, then the complement will occur.\n\n\n\nProblem 10\nLet \\(S\\) be a sample space. What is the probability of the complement of \\(S\\) (\\(P(S^c)\\))? What does this imply?\n\n\nShow solutions\n\nWe know from the axioms that \\(P(S)=1\\), i.e. we can easily compute the complement.\n\\[ P(S^c)=1-P(S)=1-1=0 \\]\nThis implies that the event of being within the sample space covers all other events. I.e. when we there is no chance of any event occurring that is not in the sample space.\n\n\n\nProbelm 11\nYou roll two 6 sided dice, one blue and one red. Consider now, what would be your sample space if:\n\nyou considered the value on the red die and blue die separately?\nyou consider the sum of the two dice?\n\n\n\nShow solutions\n\n\nFor every toss, we here have to consider two separate values, and we denote them as \\((x,y)\\). Each of the two dice can roll from 1 to 6. Adding it all together we can construct the sample space\n\n\\[ \\begin{align*}S=\\left\\{(1,1), (1,2), (1,3), (1,4), (1,5), (1,6), \\\\\n(2,1), (2,2), (2,3), (2,4), (2,5), (2,6), \\\\\n(3,1), (3,2), (3,3), (3,4), (3,5), (3,6), \\\\\n(4,1), (4,2), (4,3), (4,4), (4,5), (4,6), \\\\\n(5,1), (5,2), (5,3), (5,4), (5,5), (5,6), \\\\\n(6,1), (6,2), (6,3), (6,4), (6,5), (6,6) \\right\\}\n\\end{align*}\\]\n\nWe now consider the sum of the two dice. The lowest value is when both roll 1’s, and the highest possible one is when both roll 6’s. Every integer value between these two sums are achievable. We now end up with:\n\n\\[ S_{sum}=\\{2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12\\} \\]\n\n\nProblem 12\nYou throw two dice.\n\nIn how many outcomes is their sum less equals 6?\nWhat is the probability of getting a sum total of 2?\n\n\n\nShow solutions\n\n\nIf dice 1 rolls a 6, then the other dice will make the sum at least 7, i.e. no candidates. Dice 1 rolls 5, then we get 6 if dice 2 rolls 1, i.e. 1 candidate. Following similar logic if dice 1 rolls 1-4, then each produce 1 more candidate.\n\nIn total we end up with 5 outcomes that give us a sum of 6.\n\nFor us to have a sum of 2, we need both dice to roll 1. i.e. there is only one outcome that nets this event There are 36 possible outcomes. i.e. \\(P(sum \\ 2)=\\frac{1}{36}\\)\n\n\n\n\nProblem 13\nLet \\(A=\\{1,2,3,4,6\\}\\), \\(B=\\{2,4,6,8\\}\\), and \\(C=\\{2,3,4,5\\}\\).\n\nFind \\(A\\cap B\\cap C\\)\nFind \\(A\\cup B\\cup C\\)\n\n\nShow solutions\n\n\nRecall first that\n\n\\[ A\\cap (B\\cap C)=A\\cap B\\cap C \\]\nSo we can find the intersection simply by looking for the elements that appear in all of the sets. We end up with.\n\\[ A\\cap B\\cap C = \\{2,4\\} \\]\n\nRecall first that\n\n\\[ A\\cup (B\\cup C)=A\\cup B\\cup C \\] The union will now consist of any element that falls within any of the sets, and as such we get\n\\[A\\cup B\\cup C =\\{1,2,3,4,5,6,8\\} \\]\n\n\nProblem 14\nYou know that the probabilities of some intersections are given by \\(P(A\\cap B)=\\frac{1}{2}\\) and \\(P(A\\cap B^c)=\\frac{1}{3}\\). Compute \\(P(A)\\).\n\n\nShow solutions\n\nTwo things are very clear when working with complements, and that’s that they are disjoint events and that the probability of one or the other occurring sum to 1 exactly. Directly following from this is that we can always make use of the law of total probability when we know the probabilities of the intersections between an event, \\(A\\), and both complements. In this case we have exactly the right information to make use of the law of total probability. Recall the formula. \\[ P(A)=\\sum_{i=1}^kP(A\\cap B_i)=P(A\\cap B_1)+\\cdots+P(A\\cap B_2) \\] Let’s use this to compute the probability\n\\[ P(A)=P(A\\cap B)+P(A\\cap B^c)=\\frac{1}{2}+\\frac{1}{3}=\\frac{3+2}{6}=\\frac{5}{6} \\]\n\n\nProblem 15\nWe have an event, \\(A\\), and we are interested in figuring out the probability of it occurring. We know the probabilities of \\(B_1,...,B_n\\) and we know that \\(\\sum_{i=1}^nP(B_i)=1\\). Why can’t we use the law of total probability here? What could go wrong? (It might help to draw a Venn diagram if you’re struggling.)\n\n\nShow solutions\n\nThe problem here is that we don’t know if \\(B_1,...,B_n\\) are disjoint events, and since, there may be areas where they intersect with each other, we run the risk of counting some space several times if we were to sum all intersections between \\(A\\) and all \\(B_i\\). We also can’t be certain taht the entire sample space is covered by \\(B_i\\) if tehy are not all disjoint (see problem 16 for implications). We can illustrate algebraically (to simplify we reduce some generality, but this argument does extend). Let \\[ n=2, \\ P(B_1\\cap B_2)&gt;0 \\] We can now let the \\(A\\cap (B_1\\cap B_2)\\neq\\emptyset\\), i.e. \\(A\\) also occurs in the intersection. We can now try to compute \\(P(A)\\)\n\\[\\begin{align*}\nP(A)=P(A\\cap B_1)+P(A \\cap B_2)=P(A\\cap((B_1\\cap B_2)\\cup(B_1\\cap B_2^c)))+P(A\\cap((B_1\\cap B_2)\\cup(B_1^c\\cap B_2))) \\\\\n=P(A\\cap(B_1\\cap B_2)) + P(A\\cap(B_1\\cap B_2^c))+ P(A\\cap(B_1\\cap B_2)) + P(A\\cap(B_1^c\\cap B_2)) \\\\\n=2P(A\\cap(B_1\\cap B_2))+ P(A\\cap(B_1\\cap B_2^c))+ P(A\\cap(B_1^c\\cap B_2))\n\\end{align*}\\]\nAs we can see here, we have to account for an area where \\(A\\) intersects twice, and as such, this effect will have us overestimate the probability of \\(A\\) occurring. As such the equation above is necessarily incorrect.\n\n\n\nProblem 16\nWe have an event, \\(A\\), and we are interested in figuring out the probability of it occurring. We know the probabilities of \\(B_1,...,B_n\\) and we know that all \\(B_i\\) are pairwise disjoint(\\(B_i\\cap B_j=\\emptyset, \\ i\\neq j, \\ \\forall i,j\\)). Why can’t we use the law of total probability here? What could go wrong? (It might help to draw a Venn diagram if you’re struggling.)\n\n\nShow solutions\n\nThe problem here is that the sum of all probabilities \\(P(B_i)\\) may not equal 1. If the sum of probabilities \\(P(B_i)\\) is less than 1, then there are cases where no events \\(B_i\\) occur. If \\(A\\) intersects here, the law of total probability will lead us to underestimating \\(P(A)\\), as there is no \\(B_i\\) that intersects this part of \\(A\\). Let’s try to show this algebraically (we reduce generality here too, but argument holds). \\[ P((B_1\\cup...\\cup B_n)^c)&gt;0, \\ (B_1\\cup...\\cup B_n)^c\\subset A, \\ n=2 \\] If we try to compute using the law of total probability we then get\n\\[ P(A)=P(A\\cap B_1)+P(A\\cap B_2) \\ (*)\\]\nNote that \\((E_1\\cup E_2)^c=E_1^c\\cap E_2^c\\)\nFrom this we know that \\(B_1^c \\cap B_2^c\\subset A\\).\nBecause they are disjoint we also know that \\(B_1\\subset B_2^c, \\ B_2\\subset B_1^c\\), but we obviously have \\(B_1, B_2\\not\\subset B_1^c \\cap B_2^c\\), as any event will be disjoint with its complement.\nThis tells us that we don’t account for \\(B_1^c \\cap B_2^c\\subset A\\) in \\((*)\\), and thus we must be underestimating the probability, and the equation cannot hold.\n\n\n\nProblem 17\nWe are considering the probability of an event, \\(A\\). We know that \\(P(A\\cap B)=0.2\\) and that \\(P(A\\cap B^c)=0.05\\). Compute \\(P(A)\\).\n\n\nShow solutions\n\nBy definition, the events \\(B\\) and \\(B^c\\) are disjoint and \\(P(B)+P(B^c)=1\\), and as such, we can make use of the law of total probability. \\[ P(A)=\\sum_{i=1}^nP(A\\cap B_i)=P(A\\cap B)+P(A\\cap B^c)=0.2+0.05=0.25=\\frac{1}{4} \\]\n\n\n\nProblem 18\nWe get that \\(P(A)=0.7\\), \\(P(B_1)=0.3\\), \\(P(B_2)=0.2\\), \\(P(B_3)=0.5\\). All \\(B_i\\) are disjoint events. We also get that \\(P(A | B_1)=0.3\\), \\(P(A | B_2)=0.9\\). Find \\(P(A | B_3)\\) using the law of total probability.\n\n\nShow solutions\n\nWe need to rewrite the law of total probability to compute the conditional probability we want. Let’s use the definition conditional probability for this. \\[\nP(A|B)=\\frac{P(A\\cap B)}{P(B)} \\Rightarrow P(A\\cap B)=P(A | B)P(B) \\ (*)\n\\]\nRecall the law of total probability, now we can rewrite it by substituting in \\((*)\\). We can use this altered equation to solve for \\(P(A|B_3)\\)\n\\[ \\begin{align*}\nP(A)=\\sum_{i=1}^nP(A\\cap B_i)=\\sum^n_{i=1}P(A|B_i)P(B_i) \\\\\n\\Rightarrow P(A)=P(A|B_1)P(B_1)+P(A|B_2)P(B_2)+P(A|B_3)P(B_3) \\\\\n\\Rightarrow P(A|B_3)=\\frac{P(A)-P(A|B_1)P(B_1)-P(A|B_2)P(B_2)}{P(B_3)}\n\\end{align*}\\]\nNow we can easily compute \\(P(A|B_3)\\)\n\\[\nP(A|B_3)=\\frac{0.7-0.3*0.3-0.2*0.9}{0.5}=\\frac{0.7-0.09-0.18}{0.5}=0.86\n\\]\n\n\n\nProblem 19\nConsider two events, \\(A\\)and \\(B\\). You get that \\(P(A)=0.1\\), \\(P(B)=0.2\\) and \\(P(A\\cap B)=0.05\\). Compute $P(A B) $ Argue for why \\(P(A\\cup B)=P(A)+P(B)-P(A\\cap B)\\) makes sense.\n\n\nShow solution\n\nWe already know the formula used for the computation.\n\\[ P(A\\cup B)=0.1+0.2-0.05=0.25 \\]\nTo argue for why the formula makes sense, let’s first divide up the union between A and B into disjoint parts. We know that the union includes any element within either A, B or both. We will have one part which is A and not B, and these will can be represented as intersections in and of themselves, i.e. \\(A\\cap B^c\\) and \\(A^c \\cap B\\). The final part of the union is when both A and B occurr, meaning our intersection \\(A\\cap B\\). With this we can find a formula for the union \\[ P(A\\cup B)=P(A\\cap B^c)+P(A^c\\cap B)+P(A\\cap B) \\ (*)\\] Now, let’s consider \\(P(A)\\) and \\(P(B)\\). Any set \\(A\\) can necessarily be constructed by the union of it’s intersection with another set \\(B\\) and the complement of that set \\(B^c\\) on the sample space \\(S\\). This is very technical, but the idea is that every element (possible outcome) within \\(A\\) will either also be in \\(B\\), or not, hence ion \\(B^c\\). This idea let’s us rewrite \\(P(A)+P(B)\\), and we will represent them as sums similar to what we have in \\((*)\\).\n\\[\\begin{align*}P(A)+P(B)=P(A\\cap B)+P(A\\cap B^c)+P(A\\cap B)+P(A^c\\cap B)=2P(A\\cap B)+P(A\\cap B^c)+P(A\\cap B) \\ (**)\\end{align*}\\]\nIt’s plain to see that that \\((**)-P(A\\cap B)=(*)\\)\n\\[\\therefore P(A\\cup B)=P(A)+P(B)-P(A\\cap B) \\]\n\n\n\nProblem 20\nYou get that \\(P(B)=0.5\\), \\(P(A\\cap B)=0.4\\). Compute \\(P(A|B)\\).\n\n\nShow solutions\n\nWe compute the conditional probability using the definition.\n\\[ P(A|B)=\\frac{P(A\\cap B)}{P(B)}=\\frac{0.4}{0.5}=0.8 \\]\n\n\n\nProblem 21\nYou get that \\(P(B)=0.2\\), \\(P(A | B)=0.9\\). Compute \\(P(A\\cap B)\\).\n\n\nShow solution\n\nWe make use of the definition here to compute the probability.\n\\[\n\\begin{align*}\nP(A|B)=\\frac{P(A\\cap B)}{P(B)} \\Rightarrow P(A\\cap B)=P(A|B)P(B) \\\\\nP(A|B)=0.9*0.2=0.18\n\\end{align*}\n\\]\n\n\n\nProblem 22\nYou find that \\(P(A)=0.6, \\ P(B)=0.3\\) and \\(P(A|B)+P(B|A)=0.9\\). Find \\(P(A\\cap B)\\).\n\n\nShow solutions\n\nLet’s try to rewrite \\(P(A|B)+P(B|A)=0.9\\) and solve for \\(P(A\\cap B)\\).\n\\[\\begin{align*}\nP(A|B)+P(B|A)=0.9 \\Leftrightarrow \\frac{P(A\\cap B)}{P(B)}+\\frac{P(A\\cap B)}{P(A)}=\\frac{9}{10} \\\\\n\\Leftrightarrow \\frac{10}{3}P(A\\cap B)+\\frac{10}{6}P(A\\cap B)=\\frac{9}{10} \\\\\n\\Leftrightarrow \\frac{30}{6}P(A\\cap B)=\\frac{9}{10} \\Leftrightarrow 5P(A\\cap B)=\\frac{9}{10} \\\\\n\\therefore P(A\\cap B)=\\frac{9}{50}=0.18\n\\end{align*}\\]\n\n\n\nProblem 23\nYou get that \\(P((A\\cup B)^c)=\\frac{1}{2}\\) and that \\(P(A\\cap B)=\\frac{3}{20}\\) \\(C\\) is the event that either \\(A\\) or \\(B\\) occurs, but not both. What is \\(P(C)\\)?\n\n\nShow solution\n\nFirst we should find a way to express \\(P(C)\\). The probability of \\(E\\) occurring is obviously related to the probabilities of \\(A\\) and \\(B\\) occrring. but we have to remove any cases where both of them occur, i.e. the intersection. Recall from problem 19 as we can construct an event \\(A\\) as all the outcomes shared with \\(B\\) (\\(P(A\\cap B)\\)) and all the outcomes which aren’t shared, or shared with not \\(B\\) (the complement \\(P(A\\cap B^c)\\)), in the same sample space. I.e.\n\\[ P(A)=P(A\\cap B)+P(A\\cap B^c) \\]\nThe same can be done for \\(B\\). Now to construct \\(P(C)\\) we can remove the respective \\(P(A\\cap B)\\) andb sum the remaining probabilities.\n\\[P(C)=P(A\\cap B^c)+P(A^c\\cap B)=P(A\\cup B)-P(A\\cap B)\\] Now we can finally start relating this to the probabilities given in the problem. We need to recall the complement rule, before we can compute however.\n\\[ P(A)=1-P(A^c) \\Rightarrow P(A\\cup B)=1-P((A\\cup B)^c)\\]\nWe can substitute this in to the expression for \\(P(C)\\)\n\\[ P(C)=1-P((A\\cup B)^c)-P(A\\cap B)=1-\\frac{1}{2}-\\frac{3}{20}=\\frac{7}{20}=0.35 \\]\n\n\n\nProblem 24\nProve that for any event \\(A\\) such that \\(A\\) is independent of another event \\(B\\), show that\n\\[P(A|B)=P(A)\\]\n\n\nShow solution\n\nConsider first the definition of independence.\n\\[ P(A\\cap B)=P(A)P(B) \\ (*)\\]\nNow let’s also consider the definition of conditional probability.\n\\[ P(A|B)=\\frac{P(A\\cap B)}{P(B)} \\]\nFinally let’s substitute in the definition given in \\((*)\\) and reduce the fraction.\n\\[ P(A|B)=\\frac{P(A)P(B)}{P(B)}=P(A) \\  \\  \\  \\ \\  \\ q.e.d.\\]\n\n\n\nProblem 25\nYou find that \\(P(A)=\\frac{3}{10}\\), \\(P(B)=\\frac{1}{2}\\) and \\(P(A\\cap B)=\\frac{3}{10}\\)\nAre events \\(A\\) and \\(B\\) independent?\n\n\nShow solution\n\nWe check if the probabilities fulfill the conditions given by the definition.\n\\[ P(A)P(B)=\\frac{3}{10}\\frac{1}{2}=\\frac{3}{20}\\neq\\frac{3}{10}=P(A\\cap B) \\] I.e. A and B are not independent.\n\n\n\nProblem 26\nYou find that \\(P(A)=\\frac{5}{8}\\), \\(P(B)=\\frac{1}{5}\\) and \\(P(A\\cap B)=\\frac{1}{8}\\)\nAre events \\(A\\) and \\(B\\) independent?\n\n\nShow solution\n\nWe check if the probabilities fulfill the conditions given by the definition.\n\\[ P(A)P(B)=\\frac{5}{8}\\frac{1}{5}=\\frac{1}{8}=P(A\\cap B) \\] I.e. A and B are independent.\n\n\n\nProblem 26\nYou find that \\(P(A)=\\frac{5}{8}\\), \\(P(B)=\\frac{1}{5}\\) and \\(P(A\\cap B)=\\frac{1}{8}\\)\nAre events \\(A\\) and \\(B\\) independent?\n\n\nShow solution\n\nWe check if the probabilities fulfill the conditions given by the definition.\n\\[ P(A)P(B)=\\frac{5}{8}\\frac{1}{5}=\\frac{1}{8}=P(A\\cap B) \\] I.e. A and B are independent.\n\n\n\nProblem 27\nYou find that \\(P(A)=1\\), \\(P(B)=0.2\\) and \\(P(A\\cap B)=0.125\\)\nAre events \\(A\\) and \\(B\\) independent?\n\n\nShow solution\n\nWe check if the probabilities fulfill the conditions given by the definition.\n\\[ P(A)P(B)=1*0.2=0.2\\neq 0.125=P(A\\cap B) \\] I.e. A and B are not independent.\n\n\n\nProblem 28\nYou find that \\(P(A)=\\frac{3}{4}\\) and \\(P(A\\cap B)=\\frac{1}{10}\\)\nWhat must \\(P(B)\\) be for the events \\(A\\) and \\(B\\) to be independent.\n\n\nShow solution\n\nOur criterion follows from the definition.\n\\[P(A\\cap B)=P(A)P(B) \\Rightarrow P(B)=\\frac{P(A\\cap B)}{P(A)} \\] Now we can compute the probability.\n\\[ P(B)=\\frac{\\frac{1}{10}}{\\frac{3}{4}}=\\frac{1}{10}\\frac{4}{3}=\\frac{4}{30}=\\frac{2}{15} \\]\nSo \\(P(B)\\) should be \\(\\frac{2}{15}\\approx0.133\\)\n\n\n\nProplem 29\nYou have \\(P(A|B)=0.3\\), \\(P(A)=0.4\\), \\(P(B)=0.1\\). Compute the probability \\(P(B|A)\\).\n\n\nShow solution\n\nHere we can simply compute using Bayes rule!\n\\[ P(B|A)=\\frac{P(A|B)P(B)}{P(A)} \\] Let’s compute:\n\\[ P(B|A)=\\frac{0.3*0.1}{0.4}=\\frac{0.03}{0.4}=2.5*0.03=0.075\\]\nSo we conclude the probability of B given A is 7.5%\n\n\n\nProblem 30\nA certain disease affects 1% of a population. A test for the disease is 90% accurate for those who have the disease (i.e., it correctly identifies 90% of sick people) but also has a 5% false positive rate (i.e., it incorrectly identifies 5% of healthy people as having the disease).\nIf a randomly chosen person tests positive, what is the probability that they actually have the disease?\nFollowing, what would be the probability that someone testing positive is not sick?\nWould you consider this to be a good test?\n\n\nShow solution\n\nLet S be the event that a random person is suffering from the unnamed disease. Let T be the event that a random person gets a positive test. With this in mind, let’s try to identify what is given in the text.\n\\[ \\begin{align*}\nP(S)=0.01 \\Rightarrow P(S^c)=0.99 \\\\\nP(T|S)=0.90 \\\\\nP(T|S^c)=0.05\n\\end{align*}\\]\nWe are now looking for then probability that a person who tests positive is sick. i.e. \\(P(S|T)\\).\n\\[\\begin{align*}\nP(S|T)=\\frac{P(S\\cap T)}{P(S)}=\\frac{P(T|S)P(S)}{P(T)}\n\\end{align*}\\]\nWe already know the values of \\(P(T|S)\\) and \\(P(S)\\). Remains to find \\(P(T)\\).\nWe can attempt to use the law of total probability to find \\(P(T)\\).\n\\[ P(T)=P(T|S)P(S)+P(T|S^c)P(S^c)=0.90*0.01+0.05*0.99=0.009+0.0495=0.0585 \\]\nFinally; we can compute \\(P(S|T)\\)\n\\[P(S|T)=\\frac{0.90*0.01}{0.0585}\\approx0.154\\]\nLet’s now find the complement, i.e. \\(P(S^c|t)=1-P(S|T)\\).\n\\[P(S^c|T)\\approx1-0.154=0.846\\]\nThis should in no way be considered. When only 15% of cases report a true positive, that’s a bad sign. The false positive that constitutes about 85% of the positive tests is what we would call Type I errors. There is no one answer to what’s good level of Type I errors to have is when doing a test, but the most common benchmark is 5% (commonly denoted as \\(\\alpha=0.05\\), and is usually called the significance level of a test).\n\n\n\nProblem 31\nLet \\(A\\) and \\(B\\) be disjoint events. What is \\(P(A\\cap B)\\)? Feel free to draw a venn diagram to visualise this concept.\n\n\nShow solution\n\nWhen \\(A\\) and \\(B\\) disjoint, we get that\n\\[A\\cap B=\\emptyset \\Rightarrow P(A\\cap B)=0\\]\nLet’s plug this into the definition of a conditional probability.\n\\[ P(A|B)=\\frac{P(A\\cap B)}{P(B)}=\\frac{0}{P(B)}=0 \\]",
    "crumbs": [
      "Modules",
      "Module 2: Probability, random variables, probability distributions and simulations.",
      "Exercises"
    ]
  },
  {
    "objectID": "2-conditional-probability.html",
    "href": "2-conditional-probability.html",
    "title": "Conditional probability",
    "section": "",
    "text": "Slides for “Conditional probability”\n\n\n\nHow is conditional probability \\(P(A|B)\\) defined, and what assumption must hold for it to be valid?\nWhen we condition on an event \\(B\\), how does the sample space change? Use the dice example to explain.\nExplain why the formula for conditional probability divides the probability of \\(A\\cap B\\) by the probability of \\(B\\).\nIf a die roll is known to be odd, what is the probability of rolling a 5?\nHow does the concept of conditional probability apply to real-world scenarios, such as predicting outcomes based on partial information? Provide an example.",
    "crumbs": [
      "Modules",
      "Module 2: Probability, random variables, probability distributions and simulations.",
      "Conditional probability"
    ]
  },
  {
    "objectID": "2-conditional-probability.html#controll-questions",
    "href": "2-conditional-probability.html#controll-questions",
    "title": "Conditional probability",
    "section": "",
    "text": "How is conditional probability \\(P(A|B)\\) defined, and what assumption must hold for it to be valid?\nWhen we condition on an event \\(B\\), how does the sample space change? Use the dice example to explain.\nExplain why the formula for conditional probability divides the probability of \\(A\\cap B\\) by the probability of \\(B\\).\nIf a die roll is known to be odd, what is the probability of rolling a 5?\nHow does the concept of conditional probability apply to real-world scenarios, such as predicting outcomes based on partial information? Provide an example.",
    "crumbs": [
      "Modules",
      "Module 2: Probability, random variables, probability distributions and simulations.",
      "Conditional probability"
    ]
  },
  {
    "objectID": "2-basic-probability-rules.html",
    "href": "2-basic-probability-rules.html",
    "title": "Basic probability rules",
    "section": "",
    "text": "Slides for “Basic probability rules”\n\n\n\nWhat are the three axioms of probability?\nWhat does it mean for two events to be disjoint, and how does the third axiom apply to such events?\nExplain the complement rule in your own words.\nWhen calculating the probability of the union of two events, why do we need to subtract the probability of their intersection?\nDescribe the Law of Total Probability.",
    "crumbs": [
      "Modules",
      "Module 2: Probability, random variables, probability distributions and simulations.",
      "Basic probability rules"
    ]
  },
  {
    "objectID": "2-basic-probability-rules.html#controll-questions",
    "href": "2-basic-probability-rules.html#controll-questions",
    "title": "Basic probability rules",
    "section": "",
    "text": "What are the three axioms of probability?\nWhat does it mean for two events to be disjoint, and how does the third axiom apply to such events?\nExplain the complement rule in your own words.\nWhen calculating the probability of the union of two events, why do we need to subtract the probability of their intersection?\nDescribe the Law of Total Probability.",
    "crumbs": [
      "Modules",
      "Module 2: Probability, random variables, probability distributions and simulations.",
      "Basic probability rules"
    ]
  },
  {
    "objectID": "1-working-with-data.html",
    "href": "1-working-with-data.html",
    "title": "Working with data",
    "section": "",
    "text": "Slides for “Working with data”\n\n\n\nWhat is a categorical variable?\nWhat is a numerical variable?\nIs precipitation in millimeters a real number or an integer?\nIs a binary variable numeric or categorical?\nWhat is a good measurement?\nCan measurement error arise due to typos?",
    "crumbs": [
      "Modules",
      "Module 1: Summarizing and visualizing data",
      "Working with data"
    ]
  },
  {
    "objectID": "1-working-with-data.html#controll-questions",
    "href": "1-working-with-data.html#controll-questions",
    "title": "Working with data",
    "section": "",
    "text": "What is a categorical variable?\nWhat is a numerical variable?\nIs precipitation in millimeters a real number or an integer?\nIs a binary variable numeric or categorical?\nWhat is a good measurement?\nCan measurement error arise due to typos?",
    "crumbs": [
      "Modules",
      "Module 1: Summarizing and visualizing data",
      "Working with data"
    ]
  },
  {
    "objectID": "1-textbook.html",
    "href": "1-textbook.html",
    "title": "TECH3 Applied statistics",
    "section": "",
    "text": "The following is a redistribution of chapters 1-4 of Statistical Thinking for the 21st Century by Russell A. Poldrack, 2019, under the CC BY-NC 4.0 licence. Chapters 2-4 are of main interest for this module, but chapter 1 is also part of the curriculum and gives an introduction to statistical thinking.",
    "crumbs": [
      "Modules",
      "Module 1: Summarizing and visualizing data",
      "Textbook"
    ]
  },
  {
    "objectID": "1-textbook.html#introduction",
    "href": "1-textbook.html#introduction",
    "title": "TECH3 Applied statistics",
    "section": "Introduction",
    "text": "Introduction",
    "crumbs": [
      "Modules",
      "Module 1: Summarizing and visualizing data",
      "Textbook"
    ]
  },
  {
    "objectID": "1-textbook.html#working-with-data",
    "href": "1-textbook.html#working-with-data",
    "title": "TECH3 Applied statistics",
    "section": "Working with data",
    "text": "Working with data",
    "crumbs": [
      "Modules",
      "Module 1: Summarizing and visualizing data",
      "Textbook"
    ]
  },
  {
    "objectID": "1-textbook.html#summarizing-data",
    "href": "1-textbook.html#summarizing-data",
    "title": "TECH3 Applied statistics",
    "section": "Summarizing data",
    "text": "Summarizing data",
    "crumbs": [
      "Modules",
      "Module 1: Summarizing and visualizing data",
      "Textbook"
    ]
  },
  {
    "objectID": "1-textbook.html#data-visualization",
    "href": "1-textbook.html#data-visualization",
    "title": "TECH3 Applied statistics",
    "section": "Data visualization",
    "text": "Data visualization",
    "crumbs": [
      "Modules",
      "Module 1: Summarizing and visualizing data",
      "Textbook"
    ]
  },
  {
    "objectID": "1-summarizing-data.html",
    "href": "1-summarizing-data.html",
    "title": "Summarizing data",
    "section": "",
    "text": "Slides for “Summarizing data”\nSlides for “Summarizing data using tables”\n\n\n\nWhy do we need to summarize data?\nWhat is the difference between a frequency and a relative frequency?\nWhat is a cumulative frequency?",
    "crumbs": [
      "Modules",
      "Module 1: Summarizing and visualizing data",
      "Summarizing data"
    ]
  },
  {
    "objectID": "1-summarizing-data.html#controll-questions",
    "href": "1-summarizing-data.html#controll-questions",
    "title": "Summarizing data",
    "section": "",
    "text": "Why do we need to summarize data?\nWhat is the difference between a frequency and a relative frequency?\nWhat is a cumulative frequency?",
    "crumbs": [
      "Modules",
      "Module 1: Summarizing and visualizing data",
      "Summarizing data"
    ]
  },
  {
    "objectID": "1-plotting-tools.html",
    "href": "1-plotting-tools.html",
    "title": "Plotting tools",
    "section": "",
    "text": "Slides for “Plotting tools”\n\n\n\nWhat is the difference between a bar plot and a histogram?\nWhat is the horizontal line inside the box of a boxplot?\nWhat do we call the height of the box in a boxplot?\nWhat plot would you use for illustrating dependence between two continuous (numeric) variables?\nWhat is a time plot?",
    "crumbs": [
      "Modules",
      "Module 1: Summarizing and visualizing data",
      "Plotting tools"
    ]
  },
  {
    "objectID": "1-plotting-tools.html#controll-questions",
    "href": "1-plotting-tools.html#controll-questions",
    "title": "Plotting tools",
    "section": "",
    "text": "What is the difference between a bar plot and a histogram?\nWhat is the horizontal line inside the box of a boxplot?\nWhat do we call the height of the box in a boxplot?\nWhat plot would you use for illustrating dependence between two continuous (numeric) variables?\nWhat is a time plot?",
    "crumbs": [
      "Modules",
      "Module 1: Summarizing and visualizing data",
      "Plotting tools"
    ]
  },
  {
    "objectID": "1-idealised_representations.html",
    "href": "1-idealised_representations.html",
    "title": "Idealised representations",
    "section": "",
    "text": "Slides for “Idealised representations”\n\n\n\nHow many parameters in a Gaussian distribution?\nWhat are the characteristics of a Gaussian distribution?\nWhat does it mean that a distribution is right-skewed?\nIn the context of distributions, what is the tail?\nWhat does it mean that a distribution has a long-tail?",
    "crumbs": [
      "Modules",
      "Module 1: Summarizing and visualizing data",
      "Idealised representations"
    ]
  },
  {
    "objectID": "1-idealised_representations.html#controll-questions",
    "href": "1-idealised_representations.html#controll-questions",
    "title": "Idealised representations",
    "section": "",
    "text": "How many parameters in a Gaussian distribution?\nWhat are the characteristics of a Gaussian distribution?\nWhat does it mean that a distribution is right-skewed?\nIn the context of distributions, what is the tail?\nWhat does it mean that a distribution has a long-tail?",
    "crumbs": [
      "Modules",
      "Module 1: Summarizing and visualizing data",
      "Idealised representations"
    ]
  },
  {
    "objectID": "1-exercises.html",
    "href": "1-exercises.html",
    "title": "Exercises",
    "section": "",
    "text": "Exercises\n\nProblem 1\nYou are given the following data describing final grades in a tricky statistics course. There were 20 A’s, 97 B’s, 163 C’s, 76 D’s, 31 E’s and 13 F’s.\n\nWhat are the most and least common grades?\nOrganize this data in a frequency table\nFind the relative frequency of each grade\nWhat proportion of students failed the course?\nWhat proportion of students got a B or above?\nWhat proportion of students got between a C and an E?\n\n\n\nShow solutions\n\n\nThe most common grade is C with 163 instances, and the least common grade is F with only 13 instances.\nThe table can be organised like this for instance:\n\n\n\n\nGrade\nNumber\n\n\n\n\nA\n20\n\n\nB\n97\n\n\nC\n163\n\n\nD\n76\n\n\nE\n31\n\n\nF\n13\n\n\n\n\nBy summing the amount of respective registered grades we find that there are 400 total. We can now divide the number of instances for each grade by the total of 400 to find the relative frequency. For example, we have 20 instances of A’s out of 400 total grades.\n\n\\[\\frac{20}{400}=0.05\\]\nWe can repeat this for all grades and add to the existing table:\n\n\n\nGrade\nNumber\nFrequency\n\n\n\n\nA\n20\n0.05\n\n\nB\n97\n0.2425\n\n\nC\n163\n0.4075\n\n\nD\n76\n0.19\n\n\nE\n31\n0.0775\n\n\nF\n13\n0.0325\n\n\n\n\nThe only failing grade is F. We can read from the new table that this we have 0.0325 or 3.25% of students that failed this course.\nTo find out what proportion of students got a B or above we need the cumulative frequency. We find this cumulative frequency by summing thge relative frequency of all the relevant cases. In this case “B or above” is the same as achieving a B or an A. The relative frequency of B’s is 0.2425 and for A’s it’s 0.05.\n\n\\[ 0.2425+0.05=0.2925=29.25\\%\\]\n\nTo find out what proportion of students got grades between C and E we also need to find the cumulative frequency. Like in e this requires summing the appropriate relative frequencies. From the table in b we can see that the respective relative frequencies are 0.4075 for C, 0.19 for D and 0.0775 for E.\n\n\\[ 0.4075+0.19+0.0775=0.675=67.5\\%\\]\nAlternatively we can take note of the fact that we already know the proportion of students that were below this grade range (the proportion of F’s), as well as the proportion of students that scored above this range (the proportion that scored B or above). By subtracting the proportion of students that scored outside this range from 1 (100%), we should arrive at the same answer.\n\\[1-0.0325-0.2925=0.675=67.5\\%\\]\n\n\n\nProblem 2\nYou hear that the average starting salary after finishing a master’s degree at NHH is NOK 550 000, the median salary is NOK 525 000, and the mode is NOK 500 000. What can you infer from this?\n\n\nShow solutions\n\nThough they might seem similar on a surface level, the mode, median and mean give slightly different information.\nThe mode tells us what the most common observation in the data is. In our case this means that the most common starting salary after graduating with a master’s degree from NHH is NOK 500 000.\nThe median is the “middle value” in the dataset, meaning that 50 percent of the observations should be below this value, whilst the other 50 percent should be above this value. In our case it means that half of graduates will have a starting salary that’s NOK 525 000 or below whilst the other half get a starting salary of NOK 525 000 or above.\nFinally, the mean reflects the average of the dataset. We can tell here that the average is NOK 550 000. Additionally, by looking at everything in context we can also make some minor inferences. We see that the mean is higher than the median, indicating that some data points are skewing the average upwards, i.e. some graduates are earning really well compared to their peers from the get go. Additionally we see that the mode is 500 000, compared to the mean of 550 000. As the mode is the most common observation and is below the median, this seems to suggest that those who end up with a higher starting salary start with a substantially higher one compared to their peers, for the average to skew above the median.\n\n\n\nProblem 3\nAt a retail chain Floormart the Median pay by hour is $25, but the average is $33 per hour. What does this suggest?\n\n\nShow solutions\n\nSimilar to in Problem 2, the indication here seems to be that some employees may be earning significantly more than others. Exactly how many employees are earning more and by how much is impossible to tell given our available information, but it seems likely that the executives or managers and above may have significantly higher pay than most employees in Floormart. These few employees having a much higher pay than for instance new hires or other low level workers could explain this disparity.\n\n\n\nProblem 4\nIf possible, find the mode, median and mean of the following set of dice rolls\n\\[4,6,5,5,6,2,1,6,3,6,2,1,3\\]\n\n\nShow solutions\n\nThe mode is the most common observation. As 6 appears 4 times here, more than any other roll, 6 is the mode.\nTo clearly find the median we can sort the data from lowest to highest observation. We then end up with {1,1,2,2,3,3,4,5,5,6,6,6,6}. We have 13 observations here, so the median will be given by the 7th observation from the bottom. In our case we find that 4 is the median.\nFinally we can find the mean by taking the average of the observed rolls.\nRecall the formula for averages is\n\\[\\overline{X}=\\frac{1}{n}\\sum_{i=1}^{n}X_i\\]\nIn our case we can just sum each observation and divide by the number of observations, which is 13.\n\\[\\begin{align*}\n\\overline{X}&=\\frac{1}{13}(1+1+2+2+3+3+4+5+5+6+6+6+6)\\\\\n&=\\frac{50}{13}\\approx3.846\n\\end{align*}\\]\n\n\n\nProblem 5\nIf possible, find the mode, median and mean of the following set of favorite colors\nred,blue,blue,green,yellow,green,green,blue,red,red,pink,purple,blue\n\n\nShow solutions\n\nThe mode is found by finding the most common observation. Here “blue” is the most common favorite color, with 4 instances, and thus “blue” is our mode.\nIt is not possible to find a median within this data, as we need to be able to logically sort the data to be able to have a meaningful median. I.e. as the data is not ordinal, we can’t find a median.\nIt is not possible to find a mean from this data. We need numerical data to be able to give a meaningful mean.\n\n\n\nProblem 6\nWhat level of data is required to find modes, means, and medians? (Categorical, ordinal, etc.)\n\n\nShow solutions\n\nTo find a mode we need at least categorical data.\nTo find a median we need data that is ordinal.\nTo find a mean we need data that is numerical.\n\n\n\nProblem 7\nYou get the dataset \\[1,1,1,1,1,3,3,3,3,7,7,7,7,9,9,9.\\]\n\nFind the mode, median and mean of the above set.\nFind the the first and third quartile of the set.\nWhat are the 0th and 100th percentiles of the set?\nWould the median change if you were to add a single datapoint of 11 to the set? What would the median of the new set be? What if we were to add two datapoints of 11?\n\n\n\nShow solutions\n\n\nThe most common observation is 1, thus the mode is 1. We have an even number of observations in this set, 16.\n\nBy taking the average of the two middlemost values we will find the median. Both the 8th and 9th value in the set are 3, and thus the median of the set is 3.\nWe can find the mean by taking the average of the data.\n\\[\\begin{align*}\n\\overline{X}&=\\frac{1}{16}(1+1+1+1+1+3+3+3+3+7+7+7+7+9+9+9)\\\\\n&=\\frac{72}{16}=4.5\n\\end{align*}\\]\n\nThe first quartile indicates that 25% of observations should be below this threshold. In our case we see that the first quartile would fall between the 4th and 5th observation, which are both 1, telling us the 1st qaurtile is 1.\n\nThe 3rd quartile indicates that 75% of observations should be below it. The 12th and 13th observations are both 7, letting us know that the third quartile is 7.\n\nThe 0th percentile indicates the minimum value of the set while the 100th percentile indicates the maximum value. We can then say that the 0th percentile is 1 and the 100th percentile is 9.\nIf we add a datapoint of 11 to the set, we would now have 17 observations, meaning that the 9th observation from the bottom would indicate the median. The 9th observation from the bottom is 3, which is the same median as in the previous case, so the median would not change. If we were to add two datapoints of 11 to the set, we would have 18 observations. With 18 observations we have to take the average of the two middlemost observations after sorting. The two middlemost observations are the 9th and 10th ones from the bottom, meaning 3 and 7. The average would then be:\n\n\\[\\frac{1}{2}(3+7)=\\frac{10}{2}=5\\]\nThe median in this case would be 5, so it would indeed change from the previous case.\n\n\n\nProblem 8\nYou get this dataset of a group of 11 people and their respective heights in centimetres.\n\\[184,157,168,172,198,154,192,161,186,177,165.\\]\n\nFind the mean/average height of this group.\nFind the variance in height in this group and the standard deviation.\nDoes the variance and standard deviation seem big, small or somewhere in between in this context?\nWhat would be the effect on the variance of adding another person to the sample that is of the average height we found in a. Would the variance increase, decrease or stay the same?\nWhat would be the effect of adding another person to the group that’s 200cm tall. Would the variance increase, decrease or stay the same? Why is the change intuitive?\nWhat would be the effect of adding another person to the group that’s 140cm tall. Would the variance increase, decrease or stay the same? Why is the change intuitive? Why is the change in variance here greater than the one in e.?\n\n\n\nShow solutions\n\na)We sum our observations, and divide by 11, which is the number of observations.\n\n\\[\\begin{align*}\n\\overline{X}=\\frac{1}{11}(&184+157+168+172+198+154+192+161 \\\\ &+186+177+165)=\\frac{1914}{11}=174\n\\end{align*}\\]\n\nHere we need to first recall the formula for variance.\n\n\\[S_X^2=\\frac{1}{n-1}\\sum_{i=1}^{n}(X_i-\\overline{X})^2\\]\nWe recall that the mean we found in a is 174 and that we have 11 total observations. Now we have everything we need to plug in our data and find the variance.\n\\[\\begin{align*}\nS_X^2&=\\frac{1}{11-1}\\bigg((184-174)^2+(157-174)^2+(168-174)^2 \\\\ &+(172-174)^2+(198-174)^2+(154-174)^2+(192-174)^2+(161-174)^2+ \\\\&(186-174)^2+(177-174)^2+(165-174)^2\\bigg)=\\frac{2132}{10}=213.2\n\\end{align*}\\]\nNow we only need to find standard deviation.Recall that the standard deviation is the square root of the variance.\n\\[S_X=\\sqrt{S_X^2}=\\sqrt{\\frac{1}{n-1}\\sum_{i=1}^{n}(X_i-\\overline{X})^2}\\]\nIn our case we need only plug in the variance we have already found.\n\\[S_X=\\sqrt{213.2}\\approx14.6\\]\n\nVariance is hard to interpret directly, but the standard deviation is very simple. The standard deviation tells us that the average absolute difference from the mean in our data. Here we see that the standard deviation is 14.6cm, so on average the people in this group are 14.6 away from the the mean in terms of height. This means the we would expect a random person from this group to be either about 159cm tall or 188cm tall. It’s not all that simple to say if this is a lot or not, that depends on the data collected. If this is a sample from a group of Norwegian men age 20, this could seem like a very big standard deviation and variance. However, if this is a group of men and women with an age range from 14 to 35, this might not seem like such a big variance.\nIf we were to only add more people of the average height we found (174cm) the variance would decrease. There are two effects in play here. The when we add more observations our n gets bigger, and thus we divide by a larger number, but we also have to add another squared difference from the mean. In this case we add another observation that’s exactly equal to the mean. Firstly what effect will this have on the mean itself? Intuitively, the mean will not change in this case. Notice that the squared difference between the mean and itself is 0. \\[(\\overline{X}-\\overline{X})^2=(174-174)^2=0^2=0.\\]\n\nLet’s now add this to our equation for variance. Since we have one more observation we will be dividing by (n-1)+1=n, instead of n-1.\n\\[\\begin{align*}\nS_{X1}^2&=\\frac{1}{n} \\left(\\sum_{i=1}^n(X_i-\\overline{X})^2+(\\overline{X}-\\overline{X})^2\\right)\n    =\\frac{1}{n} \\left(\\sum_{i=1}^n(X_i-\\overline{X})^2+0\\right)\n    \\\\&=\\frac{1}{n} \\left(\\sum_{i=1}^n(X_i-\\overline{X})^2\\right)\n    &lt;\\frac{1}{n-1} \\left(\\sum_{i=1}^n(X_i-\\overline{X})^2\\right)=S_X^2\n\\end{align*}\\]\nFor those that don’t find it intuitive that the mean stays the same when you add more observations the equal the mean:\nUnderneath I have shown algebraically what happens to the mean when you add one more observation in the sample that equals the mean. This can easily be extended to adding any number, m, more observations (try for yourself!).\n\\[\\begin{align*}\n\\overline{X}_1&=\\frac{1}{n+1} \\left( \\sum_{i=1}^{n}X_i+\\overline{X} \\right)=\\frac{1}{n+1}\\left( \\frac{n}{n}\\sum_{i=1}^nX_i+\\overline{X} \\right) \\\\ &=\\frac{1}{n+1}\\left(n\\overline{X}+\\overline{X}\\right) = \\frac{n+1}{n+1}\\overline{X}=\\overline{X}\n\\end{align*}\\]\nNow, let’s see what happens to the variance, when we add another observation that equals the mean.\n\\[S_{X1}^2= \\frac{1}{n-1+1}\\left(\\sum_{i=1}^n(X_i-\\overline{X})^2+(\\overline{X}-\\overline{X})^2\\right)\\]\n\nWe can amend our data by adding the new observation of 200cm, and with this we can compute the new mean and variance. We will notice that since we add an observation that is higher than the average, the mean will increase, and we will also find that the variance (and consequently the standard deviation) will both increase. The new values will be:\n\n\\[ \\overline{X}\\approx176.17 \\\\ S_X^2\\approx250.15\\]\nThe intuition is simply that since our new observation is very different from our mean, the variance will increase. Adding the new squared difference has a greater effect than dividing by a higher number. Recall that our original mean was 174 and our original standard deviation was approximately 14.6. This means that the average absolute difference between an observation and the mean is about 14.6.\n\\[|200-174|=26&gt;14.6\\]\nThis means that by adding this new observation, since it’s difference from the mean is so clearly much greater than the standard deviation, the standard deviation should increase by adding this observation. When the standard deviation increases, so will the variance. (Note that we can only intuit in this way when the absolute difference between the new observation and the mean is quite a bit greater than the standard deviation. The intuition you should build from this is that new observations that are very different from the mean will increase the variance, and consequently, observations that are close to the mean will decrease the variance.)\n\nThis case is very similar to the one in e. The only notable difference is that we add an observation that is much lower than the mean, rather than one which is much higher. Computing the the new mean and variance we get:\n\n\\[\\overline{X}\\approx171.17 \\\\ S_X^2\\approx290.15 \\]\nThe intuition here is exactly the same as in e. Since the new observation is very different from the mean, the variance should increase. Let’s compare the absolute difference between the mean and the new observation to the standard deviation.\n\\[|140-174|=|-34|=34&gt;14.6\\]\nFrom this it should also be clear why the change in variance here is greater than the one in e. Since 140 is an observation that is farther away from the mean (174), than 200, the increase in variance will be greater.\n(Extra challenge for those that can’t get enough: Can you intuit what would happen in an opposite case? Would the variance change more if we added a new observation of someone that was 177cm tall or with someone that was 172cm tall?)\n\n\n\nProblem 9.\nRecall the variance and standard deviation found in Problem 8b. Imagine that instead of the height of people, the variance and standard deviation was instead found from data on height of trees (Note: the collected data is not necessarily the same even though the variance is the same). Does the variance and standard deviation feel big, small or somewhere in between in this context?\n\n\nShow solutions\n\nThis problem is mainly about interpretation, but let’s consider the standard deviation. We have a standard deviation of 14.6cm. A height difference of 14.6cm between two people is quite large, and very noticeable. When it comes to trees, however, that is not necessarily the case. Short trees can be as short as just a few centimeters, where as the tallest trees can reach upwards of 100 meters(!). Even without considering the shortest and tallest trees, we find that there can easily be several meters of difference between completely ordinary trees. Since we only know that we are working with trees in this case, it’s hard to assume anything about the data, but looking at the standard deviation we can tell that these trees must be very similar in stature as far as trees are considered. If anything, this variance, that seems quite reasonable (or even very large), when we consider the heights of people, seems completely minuscule when we consider trees instead. It’s impossible to say whether variance and standard deviation is large or small with no context, as is illustrated with this comparison.\n\n\n\nProblem 10\nIn what scenario would we get a Variance of 0?\n\n\nShow solutions\n\nFor the variance to be 0, our observations simply can’t vary. This means that all observations have to have the same value.\nThis can be shown algebraically. Let all observations \\(X_i=X\\) be equal. Then \\[\\overline{X}=\\frac{1}{n}\\sum_{i=1}^nX_i=\\frac{1}{n}\\sum_{i=1}^nX=\\frac{n}{n}X=X \\]\nIt is clear that all observations here are equal to the mean, which gives us.\n\\[\\begin{align*}\nS_X^2&=\\frac{1}{n-1}\\sum_{i=1}^n(X_i-\\overline{X})^2=\\frac{1}{n-1}\\sum_{i=1}^n(\\overline{X}-\\overline{X})^2 \\\\ &=\\frac{1}{n-1}\\sum_{i=1}^n0=\\frac{0}{n-1}=0\n\\end{align*}\\]\nBy looking at the equation for the variance, we can tell that if any one or more observations were to deviate from the mean, there would be variance in our data.\n\n\n\nProblem 11\nYou have a dataset with six observations and a mean of 50. Show that when you have any observations outside of the interval [45;55] the variance will always be greater than 5.\n\n\nShow solutions\n\nWe have received this information:\n\\[n=6, \\ \\overline{X}=50\\]\nTo show that that our variance will always be greater than 5 when we have an observation outside of the given interval, let’s consider the situation that would create the least variance. Consider a situation where only one observation is 55, and the rest of the observations have to make up for this by all of them being a bit less than 50. Since we use squared sums when we compute the variance, we would get the least variance from the 5 last observations being equally close to to the mean. We can use an equation to find out what each of these 5 observations have to equal. For this we use the formula to calculate an average.\n\\[\\overline{X}=\\frac{1}{n}\\sum_{i=1}^nX_i\\]\nSince we know that the average we’re looking for is 50, one observation is 55 and the last 5 observations are equal, we get an equation with one variable.\n\\[\\begin{align*}\n50&=\\frac{1}{6}(5X+55) \\\\\n\\Rightarrow 6\\cdot 50&=5X+55 \\\\\n\\Rightarrow 300&=5X+55 \\\\\n\\Rightarrow 300-55&=5X \\\\\n\\Rightarrow 245&=5X \\\\\n\\Rightarrow X&=49\n\\end{align*}\\]\nNow we have a clear view of the data that would give the minimum variance. {49,49,49,49,49,55} Finally, we can compute the variance.\n\\[\\begin{align*}\nS_X^2&=\\frac{1}{6-1}\\left(5(49-50)^2+(55-50)^2 \\right) \\\\\n&=\\frac{1}{5}(5\\cdot(-1)^2+5^2)=\\frac{1}{5}(5+25) \\\\\n&=\\frac{30}{5}=6&gt;5\n\\end{align*}\\]\n\n\n\nProblem 12\nConsider the two following data sets, A and B. \\[A=\\{52, 49, 53,57,48,48,57,53\\}\\] and \\[B=\\{29,56,31,21, 79, 45, 51, 21\\}\\]\n\nFind the interquartile ranges of both A and B. Based on the ranges, which one seems to indicate a higher standard deviation?\nFind the variance and standard deviation of both data sets. Do the results fit with your guess from a).\nWhat intuition can interquartile ranges give when comparing variances? Can you always trust this intuition fully?\n\n\n\nShow solutions\n\n\nTo find the interquartile ranges we have to first find the quartiles in each data set, and to find the quartiles we need to order the sets as well. We now end up with\n\n\\[\\begin{align*}\nA=\\{48, 48, 49, 52, 53, 53, 57, 57\\} \\\\\nB=\\{21, 21, 29, 31, 45, 51, 56, 79\\}\n\\end{align*}\\]\nNow that both sets are ordered we can find the frst and third quartile in each of them by considering the 25th and 75th percentiles. In both sets the 25th percentile will fall between the second and third observations, whereas the 75th will fall between the 6th and 7th. Thus we take the average between these observations to fin the respective quatiles.\n\\[\\begin{align*}\nq_{1A}=\\frac{1}{2}(48+49)=48.5, \\ q_{3A}=\\frac{1}{2}(53+57)=55 \\\\\nq_{1B}=\\frac{1}{2}(21+29)=25, \\ q_{3B}=\\frac{1}{2}(51+56)=53.5\n\\end{align*}\\]\nFinally, we can find the interquartile ranges and compare them. Recall the formula for an interquartile range is given by the difference in quartiles.\n\\(IQR=q_3-q_1\\) Now, let’s compute the IQR for both A and B. \\[\\begin{align*}\nIQR_A=55-48.5=6.5 \\\\\nIQR_B=53.5-25=28.5\n\\end{align*}\\] Here we can see that the interquartile range for B is much larger than for A, which indicates a larger spread of data in B, which should give us a higher standard deviation.\n\nLet us compute the variances and standard deviations. By using the formulas familiar to us we will end up with.\n\n\\[\\begin{align*}\n\\overline{X}_A=52.125, \\ S^2_A\\approx13.27, \\ S_A\\approx3.64 \\\\\n\\overline{X}_B=41.625, \\ S^2_B\\approx403.70, \\ S_B\\approx20.09\n\\end{align*}\\] We can easily see that the indication from the IQR told us which data set had more variance.\n\nThe reason interquartile ranges can help give an indication of what data has more variance, is cause the interquartile range also gives an indication of how spread out the data is. The interquartile range tells us how far the 75th percentile is from the 25th percentile, i.e. a range for the middle 50% of the data. Keep in mind, however that a higher IQR does not necessarily mean a data set has a higher variance, even though it might seem that way at a first glance. When computing variance and standard deviation extreme values are very highly weighted even though they may not have any effect on the IQR.\n\nLet’s finish up by giving a quick example. We can amend A such that we change one of the data points that reads 57 to 1000 instead. We will call the amended set A’. We then get: \\[\\begin{align*}\nA=\\{48, 48, 49, 52, 53, 53, 57, 57\\}\\rightarrow A'=\\{48, 48, 49, 52, 53, 53, 57, 1000\\}\n\\end{align*}\\] Note here that the first and third quartile have not changed, and thus the IQR stays the same. However, when computing mean, variance and standard deviation we are going to see huge shifts.\n\\[\\begin{align*}\n\\overline{X}_{A'}=170, \\ S^2_{A'}=112482.86, \\ S_{A'}\\approx335.38\n\\end{align*}\\]\nThe 1000 observation is clearly an outlier here, but the point is that we always have to be careful when saying the IQR implies the variance, or anything similar. In many cases it may be true, but we should always consider the data we’re working with.\n\n\n\nProblem 13\nConsider the Boxplot below.\n\nWhich of the two groups has the higher median?\nWhich of the two groups seems to have the higher variance?\nWhich of the two groups has the higher IQR? Does this coincide with the one that has the higher variance?\nWould you think there are any outliers in the data? Why? Why not?\n\n\n\n\n\n\n\n\n\n\n\n\nShow solutions\n\n\nThe bold, black, horizontal line in each box represents the median for the data. We note that for group 1 the median is a little less than 15, and for group 2 it seems to be about 17. As such, group 2 has the higher median.\nWe note that the whiskers for group 2’s plot stretch out far wider than the ones for group 1. There are no points beyond the whiskers for either group, and as such we can conclude that group 2 probably has the higher variance. This coincides with the code used to generate the data.\nWe note that the box for group 2 is far taller than the one for group 1, which clearly shows that group 2 has a higher IQR. The lower line of the box indicates the the 1st quartile and the upper line gives the 3rd quartile. As such, the height of a box represents the IQR for the data used to generate the boxplot. This indeed coincides with the data we believe to have the higher variance.\nWe see no points beyond the whiskers of the boxes, and as such we treat none of the data as outliers. The whiskers here represent 1.5 times the IQR of the data, beyond either the 1st or 3rd quartiles. in many cases this covers all the whole range of data in a sample, however that is not always the case. In the below boxplot I have used the same data, however I added as single, clear outlier to group 1. This outlier is now the new maximum value and is indicated as a red dot. In general, when we consider outliers using boxplots, it’s most common to start by considering everything beyond the whiskers of the plot.\n\n\n\n\n\n\n\n\n\n\n\n\n\nProblem 14\nConsider the two viloinplots below.\n\nIn what ranges in each of the groups is data most dense?\nWhich group has the highest maximum, which one has the lowest minimum?\nIs it clear which group would have a higher median? Do you think this group also has a higher mean? Argue why or why not?\nWhat are some key differences to consider between a violin plot and a boxplot?\n\n\n\n\n\n\n\n\n\n\n\n\nShow solutions\n\n\nFrom a visually inspections it’s not 100% clear what the exact ranges are, but it does seem like for group 1 data is most dense in the intervals (12, 13), whereas for group 2 it’s around (13, 15). We see this as this is where the “violins” are the widest.\nA key aspect of violin plots is that they chow the distribution of the data in its entirety. This means that the top and bottom ends of the violins indicate the range of the data. Here it??s clear rthat group n2 has the higher maximum and group 1 has the lower minimum.\nThe median is the middle observation. Though we can’t see the exact point on a violin plot, we can see that the group 1 is comparatively “bottom-heavy” next to group 2, which indicates that the median should be higher in 2. We can also infer that the higher density of high numbers in group 2 indicate a higher mean as well. We can’t be 100% certain though, as we have no direct way of telling. (In an obvious case such as this though we can be at least 99% certain).\nThe key distinctions between violin and box plots are what attributes of the data they show. Whereas box plots show summary statistics such as the median, 1st and 3rd quartile and IQR, the violin plot shows how the data is distributed (i.e. the density of the data). As you should have noticed from the exdercises above, we can’t make many direct statements based on the violin plot, other than to draw semi-certain conclusions based on where we see the data is more and less dense. The boc plot also isolates ourtliers in a more clear way compared to the violin. Take a look at the below figure to see boc plots in the same data as the violins. Were the inferences from the above exercises correct?\n\n\n\n\n\n\n\n\n\n\n\n\n\nProblem 15\nYou receive data about voting intention in a Norwegian community. Each bar represents the percentage of people in this community that wants to vote for a given party. Only people who intend to vote have been counted.\n\nWhich parties are most and least popular in this community?\nWhat is the approximate percentage point difference between the most and least popular parties?\nWhat problems could have been run into by using a pie chart instead of a bar plot?\n\n\n\n\n\n\n\n\n\n\nExercises about interpreting plots\n\n\nShow solutions\n\n\nIt clear that AP (Labour party) is the most popular and R (Red party) is the least popular in this community.\nAP has a voter turnout of about 20 percent wheres R has a turnout of about 5 percent. We thus get a difference of about 15 percentage points between the most and least popular parties.\nFor one, the pie chart would not clearly indicate the relative popularities of each party. It could for instance be very hard to tell which party was more popular between MDG and R, as they are very close in size here. Secondly, this pie chart would be very distracting top look at and hard to read, as you would not be able to read the information directly from the chart. Underneath there is an example. You could for instance add percentage labels to the chart below, but that would be visually much busier tha nthe bar plot as well as would work poorly when slices get very small.\n\n\n\n\n\n\n\n\n\n\n\n\n\nProblem 16\nConsider the histogram below. The y-axis can be interpreted as percentages. The histogram shows the distributions of yearly salaries of a population in an area.\n\nWhat percentage of people in this population have salaries in the most common interval?\nWhat are key differences between histograms and bar plots.\n\n\n\n\n\n\n\n\n\n\n\n\nShow solutions\n\n\nThe tallest bin indicates the most common interval for salaries and it seems to include about 15 percent of the people in the population. The tallest bin seems to be somewhere between 600k and 700k NOK.\nIn a histogram each bin represents an interval of values, and any datapoint which falls within that interval will contribute to the height of that bin. A histogram will also necessarily represent the distribution of numerical sample data. A bar plot, however, represents the relative frequency of categorical data.\n\n\n\n\nProblem 17\nComment on the below scatterplot, focus on whether or not you believe the X and Y variables are dependent or not.\n\n\n\n\n\n\n\n\n\n\n\nShow solutions\n\nIn this scatterplot there are no clear signs of correlation between X and Y. The closest thing one might get is that there seem to be fewer X and Y observations of more extreme values, however, that still doesn’t imply dependence. Each point seems randomly scattered out and there is no pattern to see here.\n\n\n\nProblem 18\nComment on the below scatterplot, focus on whether or not you believe the X and Y variables are dependent or not.\n\n\n\n\n\n\n\n\n\n\n\nShow solutions\n\nHere there is a clear linear dependence between the X and Y variables. There correlation seems to be almost a perfect positive correlation of 1. This can very easily be illustrated by drawing a best fit line through the point. The image below illustrates the strong linear relationship.\n\n\n\n\n\n\n\n\n\n\n\n\nProblem 19\nWhich of the following histograms are closer to a normal (Gaussian) distribution?\n\n\n\n\n\n\n\n\n\n\n\nShow solutions\n\nObviously 1 is far closer to a normal distribution than 2 is. We can clearly see that 1 is very close to the characteristic bell shape and a very symmetric distribution of points around the mean. Histogram 2 on the other hand is not at all symmetric around its mean and does not have the iconic shape, so it can easily be identified as non-normally distributed.\n\n\n\nProblem 20\nDetermine whether the below histograms display distributions that are right skewed, left skewed or neither.\n\n\n\n\n\n\n\n\n\n\n\nShow solutions\n\nHistogram number 1 is clearly extremely right skewed, which we can tell from the comparative length of the right tail and that the mass is concentrated on the left side of the plot. Number 2 is also very right skewed, but not quite to the extreme of 1.\nHistogram number 3 on the other hand is very left skewed, which should be obvious as its almost the complete opposite of the number 1.\nHistogram number 4 doesn’t seem to have any particular skewness, in actuality it seems remarkably symmetric.\n\n\n\nProblem 21\nConsider the 4 following histograms. Rank them from 1 to 4, where 1 indicates the “lightest tails” and 4 indicates “the heaviest tails”\n\n\n\n\n\n\n\n\n\n\n\nShow solutions\n\nThe correct ranking is 1-D, 2-A, 3-B, 4-C\nRecall that heavier tails implies that more mass will be farther away from the mean. Once one realises this fact it should be simple to order the histograms based on weights of tails. No points reach very far away from the mean of D, a few more for A, as one can tell by the longer interval on the x axis, even more are far away from the mean in B, and a very large amount in C.\n\n\n\nProblem 22\nRank these histograms created from normally (gaussian) distributed data in 3 different ways, from 1-6. In the first ranking, 1 should be the lowest mode and 6 should be the highest mode. In the second ranking do the same with the median and in the third with the mean. As there are a lot of data points in each of these histograms, you can treat them as perfect normal distributions.\n\n\n\n\n\n\n\n\n\n\n\nShow solutions\n\nThe twist here is that since the data is all normally distributed the mode, mean and median is equal. I.e., if you’ve found one ranking you’ve found them all.\nThe correct ranking is 1-D, 2-F, 3-C, 4-B, 5-E, 6-A\nThe mode should be obvious in each one at it is indicated by the tallest bin. Furthermore, since all the histograms are so symmetric both the medians and mean should also be well within the middle bins as well. As such we end up with the same ranking in all three cases.\n\n\n\nProblem 23\nRank the following three histograms from lowest to highest variance. Consider the data in each case to eb normally distributed.\n\n\n\n\n\n\n\n\n\n\n\nShow solutions\n\nThe correct ranking is\n1-B, 2-A, 3-C\nRecall that variance increases as spread of data increases. It’s clear that the data in B all fits within a pretty narrow interval, whereas the data in C needs a much larger interval to fit all of its observatrions. It’s not even entirely clear that we see all observations of C in the given interval. A seems relatively average, it at least does not stand out in any direction like B and C do.\n\n\n\nProblem 24\nFor this and the next problem you will need to recall how you compute means an variances.\n\\[ \\overline{X}=\\frac{1}{n}\\sum^{n}_{i=1}X_i, \\\nS^2_X=\\frac{1}{n-1}\\sum^n_{i=1}\\left(X_i-\\overline{X}\\right)^2\\]\nConsider the data \\[A=\\{a_1, a_2, ..., a_n\\}\\]. Now add 5 to every observation such that you get \\[A'=\\{(a_1+5),...,(a_n+5)\\}\\]. What would happen to the mean and the variance?\n\n\nShow solutions\n\nWe now have the situation\n\\[ A=\\{a_1,...,a_n\\}\\rightarrow A'=\\{(a_1+5),...,(a_n+5)\\} \\] Let’s the old and new means and variances have the following names.\n\\[\\begin{align*}\n\\overline{X}_{A} \\rightarrow \\overline{X}_{A'} \\\\\nS^2_{A} \\rightarrow S^2_{A'} \\end{align*}\\]\nWe will first consider what happens to the mean. Let’s try to compute the mean for \\(A'\\) \\[\\begin{align*}\n\\overline{X}_{A'}=\\frac{1}{n}\\sum^n_{i=1}(a_i+5)=\\frac{1}{n}\\sum^n_{i=1}5+\\sum^n_{i=1}a_i \\\\\n=\\frac{1}{n}5n+\\sum^n_{i=1}a_i=5+\\overline{X}_{A}=\\overline{X}_{A}+5 \\end{align*}\\] From this argument it should be clear that the 5 greater for \\(A'\\) than for \\(A\\), just like the case is for each of our observations.\nWe can now move on to considering what will happen to the variance. \\[\\begin{align*}\nS_{A'}=\\frac{1}{n-1}\\sum^n_{i=1}\\left((a_i+5)-\\overline{X}_{A'}\\right)^2\n=\\frac{1}{n-1}\\sum^n_{i=1}\\left((a_i+5)-(\\overline{X}_{A}+5)\\right)^2 \\\\\n=\\frac{1}{n-1}\\sum^n_{i=1}\\left(a_i+5-\\overline{X}_{A}-5\\right)^2 =\\frac{1}{n-1}\\sum^n_{i=1}\\left(a_i+5-\\overline{X}_{A}-5\\right)^2 \\\\\n=\\frac{1}{n-1}\\sum^n_{i=1}\\left(a_i-\\overline{X}_{A}\\right)^2\n=S_{A}^2\n\\end{align*}\\] Alas, the variance has not changed at all! The reasoning here is actually quite simple. All our observations may have increased by 5, but so has the mean. This means that the absolute difference between each individual point and the mean is the same for both data sets. So even though the data is different, it doesn’t vary any more or less in one case or the other.\nExtra challenge: There is only one key difference that would come to be if you were to subtract 5 to every datapoint instead of adding 5. What is that difference?\n\n\n\nProblem 25\nConsider the data \\(B=\\{b_1, b_2, ..., b_n\\}\\). Now double every observation such that you get \\(B'=\\{2b_1,...,2b_n\\}\\). What would happen to the mean and the variance?\n\n\nShow solutions\n\nWe now have the situation\n\\[ B=\\{b_1,...,b_n\\}\\rightarrow B'=\\{2b_1,...,2b_n\\} \\] Let’s the old and new means and variances have the following names.\n\\[\\begin{align*}\n\\overline{X}_{B} \\rightarrow \\overline{X}_{B'} \\\\\nS^2_{B} \\rightarrow S^2_{B'} \\end{align*}\\]\nWe will first consider what happens to the mean. Let’s try to compute the mean for \\(B'\\) \\[\\begin{align*}\n\\overline{X}_{B'}=\\frac{1}{n}\\sum^n_{i=1}2b_i=\n2\\frac{1}{n}\\sum^n_{i=1}b_i=2\\overline{X}_B \\end{align*}\\] It’s very clear that if every datapoint was to double, so would the mean.\nNow we compute the new variance. \\[\\begin{align*}\nS_{B'}=\\frac{1}{n-1}\\sum^n_{i=1}\\left(2b_i-\\overline{X}_{B'}\\right)^2\n=\\frac{1}{n-1}\\sum^n_{i=1}\\left(2b_i-2\\overline{X}_{B}\\right)^2 \\\\\n=\\frac{1}{n-1}\\sum^n_{i=1}\\left(2(b_i-\\overline{X}_B)\\right)^2 =\\frac{1}{n-1}\\sum^n_{i=1}2^2\\left(b_i-\\overline{X}_B\\right)^2 \\\\\n=\\frac{1}{n-1}\\sum^n_{i=1}4\\left(b_i-\\overline{X}_B\\right)^2\n=4\\frac{1}{n-1}\\sum^n_{i=1}\\left(b_i-\\overline{X}_B\\right)^2\n=4S_{B}^2\n\\end{align*}\\] So, by multiplying each datapoint by 2, we end up multiplying the variance by 4 (2 squared!!). This also makes surprisingly intuitive sense. When we double each datapoint, we are also going to double the distance between that point and the mean. When computing for the variance we find the squared difference between the data points and the mean, and thus, we end up multiplying our variance with 2 squared.\nExtra challenge: What would happen to the variance if we were to halve the value of each datapoint instead of doubling them?\n\n\n\nProblem 26\nConsider the data \\[C=\\{c_1, c_2, ..., c_n\\}\\]. Now consider what would happen if you were to amend the data by adding more and more of a single observation, \\(d\\) until you’ve added an endless amount of it. What would happen to the mode, median, 1st quartile, 3rd quartile, IQR, max, min, mean, variance and standard deviation? (Note: this exercise should be solved by reasoning rather than with rigorous mathematical arguments!!)\n\n\nShow solutions\n\nTo make every point clear, let’s go summary statistic by summary statistic.\nMode: The consequence on the mode is probably the most obvious one. Recall that the mode is the most common observation, i.e. the observation that occurs most often in the data. If the observation we are adding, \\(d\\), is not already the mode, then by adding an endless amount of it, and only it, at some point it will become the mode. It is inevitable that when we have limited data, adding an endless amount of a singular observartion will eventually result in it becoming the mode. Consider a quick example \\[C=\\{1,2,1,1,1,3\\}, \\ d=5\\] If we were to add \\(d=5\\) once or twice, the mode would still be 1. However, if we keep adding \\(d=5\\), then eventually 5 will become the mode. Once \\(d\\) has become the mode, the mode will not change if we keep adding to the data.\nMedian: The consequence on the median is similar to the one on the mode, in that eventually, the median will end up equaling \\(d\\). Eventually, by adding \\(d\\) more and more to the data, more than half of the observartions will be \\(d\\), and as such, no matter what \\(d\\) equals, it will be the median. Let’s consider the an example. \\[C=\\{1,1,1,1,2,3\\}\\] If we consider \\(d=5\\), as above, we see what by adding it once, it won’t be the median. However if we were to add it 10 times for instance such that we get, more than half of the data would consist of \\(d\\), which means that the middlemost values of the data would also have to be \\(d\\). (The exact same case would play out if \\(d\\) was a much higher maximum or the minimum value of the data). The cases where it would take the most for \\(d\\) to become the median are the ones where it’s either the maximum or the minimum, in all other cases it will take less for it to become the median.\n1st and 3rd quartile: The argument for the 1st and 3rd quartiles is exactly the same as for the median. The more iterations you add, the more of the data will be \\(d\\). After a while close to all data in would equal \\(d\\), and at that point it would also cover both the 1st and 3rd quartile.\nIQR: Recall that IQR is the difference between quartiles. From the argument above we know that at some point both the 1st and 3rd quartiles will be equal to \\(d\\), and at that point the IQR will obviously be 0. Depending on the data the IQR could increase for a bit at first, but after either the 1st or 3rd quartile becomes \\(d\\), it will start approaching 0.\nMin and max: The min and max will change only if \\(d\\) is either the minimum or maximum value in the set, and only if \\(d\\) is not part of the original data. However, if there are values that are larger and smaller than \\(d\\) in the original data, they will not change, no matter how many iterations we add. Consider for example: \\(C=\\{1,2,1,1,1,3\\}, \\ d=2\\) We would then end up with $ C’={1,1,1,1,2,2,2,2,…,2,2,2,3}$ No matte how many times 2 is added, the max and min will not change.\nMean: In problem 8 we considered what would happen if we were to add a new observation with the same value as the mean, and we concluded there would be no change. So if \\(\\overline{X}_{C}=d\\), the mean would not change no matter how many times we were to iterate on the data by adding another observation, \\(d\\). Now consider what would happen if \\(d\\) is different from the. Let \\(m\\) be the number of observations in the original data, and \\(n\\) be the number we get when we add more observations. Computing the mean would then give us. \\[\\begin{align*}\n\\overline{X}_{C'}=\\frac{1}{n}\\left((n-m)d+\\sum^m_{i=1}c_i\\right)\n=\\frac{1}{n}\\left((n-m)d+m\\overline{X}_C\\right)\\\\\n=\\frac{n-m}{n}d+\\frac{m}{n}\\overline{X}_C\n\\end{align*}\\]\nNow we let \\(n\\) approach infinity.\n\\[\\begin{align*}\n\\lim_{n\\rightarrow\\infty}\\overline{X}_{C'}\n=\\lim_{n\\rightarrow\\infty}\\left(\\frac{n-m}{n}d+\\frac{m}{n}\\overline{X}_C\\right)\\\\\n=\\lim_{n\\rightarrow\\infty}\\frac{n-m}{n}d+\\lim_{n\\rightarrow\\infty}\\frac{m}{n}\\overline{X}_C\\\\\n=1*d+0*\\overline{X}_{C}=d\n\\end{align*}\\]\nThis essentially means, that after a while the mean will end up very close to \\(d\\), and it will keep approaching until it reaches \\(d\\). The nature of a limit like this means that it will end up infinitesimally close as long as we iterate a finite amount of data points, but the limit still holds true. Summed up, we can think that when a single observation makes up a vast vast majority of the data, other single observations will have basically no effect on the mean.\nVariance and standard deviation: In problem 8 we also concluded that the variance would decrease when adding more observations close or equal to the mean. Since we know the limit of the mean is \\(d\\) from above let’s argue based on that. When almost all of the data is centered around one point, and every other observation is negligible in comparison, they will contribute almost nothing until the variance reaches 0. Directly following from this, we get that the standard deviation will also approach 0, as it is the positive square root of the variance.\n(Note: the algebraic argument below is very abridged, it’s more to get the point across)\n\\[\\begin{align*}\n\\lim_{n\\rightarrow\\infty}S_{C'}^2\n=\\lim_{n\\rightarrow\\infty}\\frac{1}{n-1}\\left(\\sum_{i=m+1}^n(d-\\overline{X}_{C'})^2\n+\\sum^m_{i=1}(c_i-\\overline{X}_{C'})^2\\right) \\\\\n=\\lim_{n\\rightarrow\\infty}\\frac{1}{n-1}\\sum_{i=m+1}^n(d-\\overline{X}_{C'})^2+\n\\lim_{n\\rightarrow\\infty}\\frac{1}{n-1}\\sum^m_{i=1}(c_i-\\overline{X}_{C'})^2=\\cdots\n\\end{align*}\\]\nWe know that the first of these two sums will approach 0, as the difference between the mean and \\(d\\) will approach 0. As such we will treat this first sum as 0.\n\\[\\begin{align*}\n\\cdots=0+\\lim_{n\\rightarrow\\infty}\\frac{1}{n-1}\\sum^m_{i=1}(c_i-\\overline{X}_{C'})^2\\\\\n=\\lim_{n\\rightarrow\\infty}\\frac{1}{n-1}\\sum^m_{i=1}(c_i^2-2c_i\\overline{X}_{C'}+\\overline{X}_{C'}^2)\\\\\n=\\lim_{n\\rightarrow\\infty}\\frac{1}{n-1}*\n\\lim_{n\\rightarrow\\infty}\\sum^m_{i=1}(c_i^2-2c_i\\overline{X}_{C'}+\\overline{X}_{C'}^2)\\\\\n=\\lim_{n\\rightarrow\\infty}\\frac{1}{n-1}*\n\\left(c_i^2-2c_id+d^2\\right)=0*\\left(c_i^2-2c_id+d^2\\right)=0\n\\end{align*}\\]\n\n\n\nProblem 27\nYou are given the following dataset containing information about survival of people travelling on the Titanic in 1912. The data contains counts of how many who survived or not of groups by ticket class, sex, age (child/adult). Can you categorize the columns based on whether they are categorical, numeric, binary, integer or real?\n\n\n\n\n\nClass\nSex\nAge\nSurvived\nFreq\n\n\n\n\n1st\nMale\nChild\nNo\n0\n\n\n2nd\nMale\nChild\nNo\n0\n\n\n3rd\nMale\nChild\nNo\n35\n\n\nCrew\nMale\nChild\nNo\n0\n\n\n1st\nFemale\nChild\nNo\n0\n\n\n2nd\nFemale\nChild\nNo\n0\n\n\n3rd\nFemale\nChild\nNo\n17\n\n\nCrew\nFemale\nChild\nNo\n0\n\n\n1st\nMale\nAdult\nNo\n118\n\n\n2nd\nMale\nAdult\nNo\n154\n\n\n\n\n\n\n\nShow suggested solutions\n\n\nClass: Categorical, although could be treated as ordinal (1st &gt; 2nd &gt; 3rd &gt; Crew)\nSex: Binary (Male/female)\nAge: Binary (Child/adult)\nSurvived: Binary (no/yes)\nFreq: Numerical, integer.",
    "crumbs": [
      "Modules",
      "Module 1: Summarizing and visualizing data",
      "Exercises"
    ]
  },
  {
    "objectID": "1-data-visualization.html",
    "href": "1-data-visualization.html",
    "title": "Data visualization principles",
    "section": "",
    "text": "Slides for “Principles of good visualization”\nSlides for “Accommodating human limitations”\n\n\n\nWhy should you always show the data?\nWhat does it mean to maximize the data/ink ratio?\nYou may think that bar plots in 3D looks cool. Why should we not use that?\nWhat is distorting the data?\nIf you want to compared Gross Domestic Product (GDP) between the US and Norway, how should you adjust the data?\nWhat does it mean to be “kind to the colorblind”?\nWhy should you never use a pie chart? Ever.\nWhat would be a good alternative to a pie chart?",
    "crumbs": [
      "Modules",
      "Module 1: Summarizing and visualizing data",
      "Data visualization principles"
    ]
  },
  {
    "objectID": "1-data-visualization.html#controll-questions",
    "href": "1-data-visualization.html#controll-questions",
    "title": "Data visualization principles",
    "section": "",
    "text": "Why should you always show the data?\nWhat does it mean to maximize the data/ink ratio?\nYou may think that bar plots in 3D looks cool. Why should we not use that?\nWhat is distorting the data?\nIf you want to compared Gross Domestic Product (GDP) between the US and Norway, how should you adjust the data?\nWhat does it mean to be “kind to the colorblind”?\nWhy should you never use a pie chart? Ever.\nWhat would be a good alternative to a pie chart?",
    "crumbs": [
      "Modules",
      "Module 1: Summarizing and visualizing data",
      "Data visualization principles"
    ]
  },
  {
    "objectID": "1-gapminder.html",
    "href": "1-gapminder.html",
    "title": "Gapminder example of good visualization",
    "section": "",
    "text": "Hans Rosling (deceased) is a legend when it comes data visualization. He was a professor of International Health at Karolinska Insitute and well-known for his visualizations of global data related to international heath. The youtube video below is made by BBC in collaboration with Hans Rosling and can (hopefully) be inspirational for your own visualizatios. The data in the video is up to 2010, but for updated data, see gapminder.org.\n\n\n\n\n\nWhat do we call the kind of plots that Rosling uses here?\nHow many variables is visualized at the same time in Rosling’s plot?\nHow many data points did Rosling visualize in the video (according to himself)?\nWhich country had the highest life expectancy and income per person in 2010?\nWhich country had the lowest life expectancy and income per person in 2010?\nCan you see a dip in life expectancy due to Covid-19 in the updated data?",
    "crumbs": [
      "Modules",
      "Module 1: Summarizing and visualizing data",
      "Gapminder example of good visualization"
    ]
  },
  {
    "objectID": "1-gapminder.html#controll-questions",
    "href": "1-gapminder.html#controll-questions",
    "title": "Gapminder example of good visualization",
    "section": "",
    "text": "What do we call the kind of plots that Rosling uses here?\nHow many variables is visualized at the same time in Rosling’s plot?\nHow many data points did Rosling visualize in the video (according to himself)?\nWhich country had the highest life expectancy and income per person in 2010?\nWhich country had the lowest life expectancy and income per person in 2010?\nCan you see a dip in life expectancy due to Covid-19 in the updated data?",
    "crumbs": [
      "Modules",
      "Module 1: Summarizing and visualizing data",
      "Gapminder example of good visualization"
    ]
  },
  {
    "objectID": "1-intro.html",
    "href": "1-intro.html",
    "title": "TECH3 Applied statistics",
    "section": "",
    "text": "In this module, you will learn about summarizing and visualizing data. All (almost) applied statistical analysis starts with some data, and beeing able to summarizing visualize data is an important tool for any user of statistics.\n\nFocus: Making the data more accessible.\nKey topics: Central statistics summarizing data, histograms, scatterplots, boxplots, human perceptional limitations.\nSpecial Emphasis: What is statistical thinking? Never create a pie chart.",
    "crumbs": [
      "Modules",
      "Module 1: Summarizing and visualizing data",
      "Introduction"
    ]
  },
  {
    "objectID": "1-statistical-thinking.html",
    "href": "1-statistical-thinking.html",
    "title": "Statistical Thinking",
    "section": "",
    "text": "Slides for “Statistical Thinking”\n\n\n\nWhat are the three things statistics can help us with?\nWhich word is not a part of statistical thinking definitions?\n\nData\nUncertainty\nIntuition\nReasoning\n\nWhat is the “big idea” of aggregation in statistics?\nDoes more data generally imply higher uncertainty?",
    "crumbs": [
      "Modules",
      "Module 1: Summarizing and visualizing data",
      "Statistical Thinking"
    ]
  },
  {
    "objectID": "1-statistical-thinking.html#controll-questions",
    "href": "1-statistical-thinking.html#controll-questions",
    "title": "Statistical Thinking",
    "section": "",
    "text": "What are the three things statistics can help us with?\nWhich word is not a part of statistical thinking definitions?\n\nData\nUncertainty\nIntuition\nReasoning\n\nWhat is the “big idea” of aggregation in statistics?\nDoes more data generally imply higher uncertainty?",
    "crumbs": [
      "Modules",
      "Module 1: Summarizing and visualizing data",
      "Statistical Thinking"
    ]
  },
  {
    "objectID": "1-summary-statistics.html",
    "href": "1-summary-statistics.html",
    "title": "Summary statistics",
    "section": "",
    "text": "Slides for “Summary statistics”\n\n\n\nWhat does the mean, median and mode aim to describe?\nHow would you find the median if you had an even number of observations?\nWhich percentile is the median?\nWhat is the 100-percentile usually called?\nWhat is the relationship between variance and standard deviation?\nWhy prefer the standard deviation instead of the variance?",
    "crumbs": [
      "Modules",
      "Module 1: Summarizing and visualizing data",
      "Summary statistics"
    ]
  },
  {
    "objectID": "1-summary-statistics.html#controll-questions",
    "href": "1-summary-statistics.html#controll-questions",
    "title": "Summary statistics",
    "section": "",
    "text": "What does the mean, median and mode aim to describe?\nHow would you find the median if you had an even number of observations?\nWhich percentile is the median?\nWhat is the 100-percentile usually called?\nWhat is the relationship between variance and standard deviation?\nWhy prefer the standard deviation instead of the variance?",
    "crumbs": [
      "Modules",
      "Module 1: Summarizing and visualizing data",
      "Summary statistics"
    ]
  },
  {
    "objectID": "1-videos.html",
    "href": "1-videos.html",
    "title": "Working with data",
    "section": "",
    "text": "Here comes a video"
  },
  {
    "objectID": "2-basic-concepts-in-probability.html",
    "href": "2-basic-concepts-in-probability.html",
    "title": "Basic concepts in probability",
    "section": "",
    "text": "Slides for “Basic concepts in probability”\n\n\n\nWhat is an experiment?\nWhat is an outcome?\nWhat is the sample space?\nWhat is an event?\nIn a dice-throwing experiment, if event 𝐴 is “rolling a 1, 2, or 3,” what is the complement of 𝐴?\nFor events 𝐴 (rolling a 1 or 2) and 𝐵 (rolling a 2, 3, or 4), describe their intersection and how it would appear in a Venn diagram.\nUsing the same events 𝐴 and 𝐵 as above, describe their union. What does it represent, and how would you illustrate it in a Venn diagram?",
    "crumbs": [
      "Modules",
      "Module 2: Probability, random variables, probability distributions and simulations.",
      "Basic concepts in probability"
    ]
  },
  {
    "objectID": "2-basic-concepts-in-probability.html#controll-questions",
    "href": "2-basic-concepts-in-probability.html#controll-questions",
    "title": "Basic concepts in probability",
    "section": "",
    "text": "What is an experiment?\nWhat is an outcome?\nWhat is the sample space?\nWhat is an event?\nIn a dice-throwing experiment, if event 𝐴 is “rolling a 1, 2, or 3,” what is the complement of 𝐴?\nFor events 𝐴 (rolling a 1 or 2) and 𝐵 (rolling a 2, 3, or 4), describe their intersection and how it would appear in a Venn diagram.\nUsing the same events 𝐴 and 𝐵 as above, describe their union. What does it represent, and how would you illustrate it in a Venn diagram?",
    "crumbs": [
      "Modules",
      "Module 2: Probability, random variables, probability distributions and simulations.",
      "Basic concepts in probability"
    ]
  },
  {
    "objectID": "2-bayes-rule.html",
    "href": "2-bayes-rule.html",
    "title": "Bayes rule",
    "section": "",
    "text": "Slides for “Bayes rule”\n\n\n\nWhat is Bayes rule?\nWhen do we need the Bayes rule?\nWhat is the base rate fallacy?",
    "crumbs": [
      "Modules",
      "Module 2: Probability, random variables, probability distributions and simulations.",
      "Bayes rule"
    ]
  },
  {
    "objectID": "2-bayes-rule.html#controll-questions",
    "href": "2-bayes-rule.html#controll-questions",
    "title": "Bayes rule",
    "section": "",
    "text": "What is Bayes rule?\nWhen do we need the Bayes rule?\nWhat is the base rate fallacy?",
    "crumbs": [
      "Modules",
      "Module 2: Probability, random variables, probability distributions and simulations.",
      "Bayes rule"
    ]
  },
  {
    "objectID": "2-discrete-random-variables-and-distributions.html",
    "href": "2-discrete-random-variables-and-distributions.html",
    "title": "Discrete random variables and distributions",
    "section": "",
    "text": "Slides for “Discrete random variables and distributions?”\n\n\n\nWhat is a random variable?\nWhat is a discrete random variable?\nWhat is the probability distribution of a discrete random variable?\nHow do you visualize a discrete probability distribution?\nCan you name one discrete probability distribution?",
    "crumbs": [
      "Modules",
      "Module 2: Probability, random variables, probability distributions and simulations.",
      "Discrete random variables and distributions"
    ]
  },
  {
    "objectID": "2-discrete-random-variables-and-distributions.html#controll-questions",
    "href": "2-discrete-random-variables-and-distributions.html#controll-questions",
    "title": "Discrete random variables and distributions",
    "section": "",
    "text": "What is a random variable?\nWhat is a discrete random variable?\nWhat is the probability distribution of a discrete random variable?\nHow do you visualize a discrete probability distribution?\nCan you name one discrete probability distribution?",
    "crumbs": [
      "Modules",
      "Module 2: Probability, random variables, probability distributions and simulations.",
      "Discrete random variables and distributions"
    ]
  },
  {
    "objectID": "2-exp-var-discrete.html",
    "href": "2-exp-var-discrete.html",
    "title": "Expectation and variance of discrete random variables",
    "section": "",
    "text": "Slides for “Expectation and variance of discrete random variables”\n\n\n\nHow do you compute the expected value of discrete random variables?\nHow do you interpret an expected value?\nHow do you compute the variance of a discrete random variable?\nHow do you interpret the variance?",
    "crumbs": [
      "Modules",
      "Module 2: Probability, random variables, probability distributions and simulations.",
      "Expectation and variance of discrete random variables"
    ]
  },
  {
    "objectID": "2-exp-var-discrete.html#controll-questions",
    "href": "2-exp-var-discrete.html#controll-questions",
    "title": "Expectation and variance of discrete random variables",
    "section": "",
    "text": "How do you compute the expected value of discrete random variables?\nHow do you interpret an expected value?\nHow do you compute the variance of a discrete random variable?\nHow do you interpret the variance?",
    "crumbs": [
      "Modules",
      "Module 2: Probability, random variables, probability distributions and simulations.",
      "Expectation and variance of discrete random variables"
    ]
  },
  {
    "objectID": "2-intro.html",
    "href": "2-intro.html",
    "title": "TECH3 Applied statistics",
    "section": "",
    "text": "Probability, random variables, probability distributions and simulations\n\nFocus: Random variables, distributions and the link to simulations.\nKey topics: Probability distributions, conditional probability, independence, the law of large numbers, pseudo-random numbers.\nSpecial Emphasis: Estimating the probability of complex outcomes through simulations.",
    "crumbs": [
      "Modules",
      "Module 2: Probability, random variables, probability distributions and simulations.",
      "Introduction"
    ]
  },
  {
    "objectID": "2-textbook.html",
    "href": "2-textbook.html",
    "title": "TECH3 Applied statistics",
    "section": "",
    "text": "The following is a redistribution of chapters 6 of Statistical Thinking for the 21st Century by Russell A. Poldrack, 2019, under the CC BY-NC 4.0 licence.",
    "crumbs": [
      "Modules",
      "Module 2: Probability, random variables, probability distributions and simulations.",
      "Textbook"
    ]
  },
  {
    "objectID": "2-textbook.html#probability",
    "href": "2-textbook.html#probability",
    "title": "TECH3 Applied statistics",
    "section": "Probability",
    "text": "Probability",
    "crumbs": [
      "Modules",
      "Module 2: Probability, random variables, probability distributions and simulations.",
      "Textbook"
    ]
  },
  {
    "objectID": "3-bootstrap.html",
    "href": "3-bootstrap.html",
    "title": "Bootstrap",
    "section": "",
    "text": "Slides for “Bootstrap”\nSlides for “Parametric Bootstrap”\n\n\n\nWhat is the difference between sampling with and without replacement?\nWhat is a bootstrap sample?\nWhat is the purpose of the bootstrap?\nWhat does bootstraps have to do with the bootstrap method?\nWhat is the difference between bootstrap and parametric bootstrap?\nWhat are the main purposes of parametric bootstrap?",
    "crumbs": [
      "Modules",
      "Module 3: Estimation, sampling distributions and resampling",
      "Bootstrap"
    ]
  },
  {
    "objectID": "3-bootstrap.html#control-questions",
    "href": "3-bootstrap.html#control-questions",
    "title": "Bootstrap",
    "section": "",
    "text": "What is the difference between sampling with and without replacement?\nWhat is a bootstrap sample?\nWhat is the purpose of the bootstrap?\nWhat does bootstraps have to do with the bootstrap method?\nWhat is the difference between bootstrap and parametric bootstrap?\nWhat are the main purposes of parametric bootstrap?",
    "crumbs": [
      "Modules",
      "Module 3: Estimation, sampling distributions and resampling",
      "Bootstrap"
    ]
  },
  {
    "objectID": "3-exercises.html",
    "href": "3-exercises.html",
    "title": "Exercises",
    "section": "",
    "text": "Problem 1\nThe diameter of a randomly selected mechanical nut is a random variable with mean 10 mm and standard deviation 0.04 mm.\n\nIf \\(\\bar X\\) is the sample mean diameter of a random sample of \\(n=16\\) nuts, where is the sampling distribution of \\(\\bar X\\) centered, and what is the standard deviation of the \\(\\bar X\\) distribution?\nAnswer the question in (a) for a sample size of \\(n=64\\) nuts.\nFor which of the two random samples from (a) or (b), is \\(\\bar X\\) more likely to be within 0.01 mm from 10 mm? Explain your reasoning.\n\nSuppose the distrution of the diameter is normal.\n\nCalculate \\(P(9.99\\le \\bar X \\le 10.01)\\) when \\(n=16\\).\nHow likely is it that the sample mean diameter exceeds 10.01 when \\(n=25\\).\n\n\n\nShow solutions\n\n\nCentered at 10 with standard deviation \\(\\sigma/\\sqrt{n}=0.04/\\sqrt{16}= 0.01\\).\nCentered at 10 with standard deviation \\(\\sigma/\\sqrt{n}=0.04/\\sqrt{64}= 0.005\\).\nWith less variability, the second sample is more closely centered near 10.\n\\(P(9.99\\le \\bar X \\le 10.01)=P(\\bar X \\le 10.01)-P(\\bar X \\le 9.99) = 0.84-0.16= 0.68\\), getting the numbers from the code below:\n\n\nfrom scipy import stats\nimport numpy as np\nprint(\n  stats.norm.cdf(10.01, loc = 10, scale = 0.04/np.sqrt(16))-\n  stats.norm.cdf(9.99, loc = 10, scale = 0.04/np.sqrt(16))\n  )\n\n0.6826894921370756\n\n\n\n\\(P(\\bar X_{25}&gt;10.01)=0.106\\).\n\n\nprint(\n  1-stats.norm.cdf(10.01, loc = 10, scale = 0.04/np.sqrt(25))\n  )\n\n0.10564977366686001\n\n\n\n\n\nProblem 2\nThe tip percentage at a restaurant has a mean value of 18% and standard deviation of 6%.\n\nWhat is the apporixmate probability that the sample mean tip percentage for a random sample of 40 bills is between 16% and 19%?\nIf the sample size had been rather 15 than 40, could the probability requested in part (a) be calculated from the given information?\n\n\n\nShow solutions\n\n\nWith \\(n=40\\) observations, we can approximate the probability using the central limit theorem (CLT). That is \\(\\bar X_{40}\\sim N(0.18, 0.06/\\sqrt{40})\\): \\[P(16\\%\\le \\bar X_{40}\\le 19\\%)=P(\\bar X_{40}\\le 19\\%)-P(\\bar X_{40}\\le 16\\%)=0.8366\\]\n\n\nfrom scipy import stats\nmean = 0.18\nsd = 0.06/np.sqrt(40)\nprint(stats.norm.cdf(0.19, loc=mean, scale =sd)-stats.norm.cdf(0.16, loc=mean, scale = sd))\n\n0.8365722369182748\n\n\n\nIn (a) we used the Central Limit Theorem (CLT) to approximate the probability using a normal distribution. This relies on a large enough sample size. With only \\(n=15\\) observations, the sample size is likely too low for the CLT. If we use it anyway, we get 0.6423 for the probability.\n\n\nmean = 0.18\nsd = 0.06/np.sqrt(15)\nprint(stats.norm.cdf(0.19, loc=mean, scale =sd)-stats.norm.cdf(0.16, loc=mean, scale = sd))\n\n0.6423446905561638\n\n\n\n\n\nProblem 3\nSuppose the sugar content (g per cm³) of a randomly selected fruit from a certain orchard is normally distributed with a mean of 2.65 and a standard deviation of 0.85.\n\nIf a random sample of 25 fruits is selected, what is the probability that the sample average sugar content is at most 3.00? Between 2.65 and 3.00?\nHow large a sample size would be required to ensure that the first probability in part (a) is at least 0.99?\n\n\n\nShow solutions\n\n\nLet \\(X_i\\), \\(i=1,\\ldots, 25\\) denote the sugar content of 25 randomly selected fruits. We then have from the CLT that \\(\\overline X_{25}=\\frac{1}{25}\\sum_{i=1}^{25}X_i\\) is approximately \\(N(\\mu,\\sigma^2/n)=N(2.65, 0.85^2/25)\\).\n\nWe can thus find the probabilities by\n\\[P(\\overline X_{25} \\le 3) = P\\left(\\frac{\\overline X_{25}-\\mu}{\\sigma/\\sqrt{n}}\\le\\frac{3-2.65}{0.85/\\sqrt{25}}\\right)=P(Z\\le 2.059)=0.9802,\\] where the latter equality comes from\n\nfrom scipy.stats import norm\nprint(norm.cdf(2.059))\n\n0.9802528807563333\n\n\n\\[P(\\overline X_{25} \\le 2.65) = P\\left(\\frac{\\overline X_{25}-\\mu}{\\sigma/\\sqrt{n}}\\le\\frac{2.65-2.65}{0.85/\\sqrt{25}}\\right)=P(Z\\le 0)=0.5,\\] since Z is N(0,1) and the normal distribution is symmetric around its mean. We thus have that \\[\\begin{align*}\nP(2.65\\le\\overline X_{25}\\le 3)&=P(0\\le Z\\le 2.059)=P(Z\\le 2.059)-P(Z\\le 0)\\\\\n&= 0.9802-05=0.4802.\n\\end{align*}\\]\nWe could also find the probabilities without finding the Z-scores:\n\nimport numpy as np\nprint(\"P(Xbar &lt;= 3) = \", norm.cdf(3, loc = 2.65, scale = np.sqrt(0.0289)))\nprint(\"P(Xbar &lt;= 2.65) = \", norm.cdf(3, loc = 2.65, scale = np.sqrt(0.0289)) - norm.cdf(2.65, loc = 2.65, scale = np.sqrt(0.0289)))\n\nP(Xbar &lt;= 3) =  0.980244426611219\nP(Xbar &lt;= 2.65) =  0.480244426611219\n\n\n\nWe then need to find the value of n that satisfies: \\[P(\\overline X_{n} \\le 3) = P\\left(\\frac{\\overline X_{n}-\\mu}{\\sigma/\\sqrt{n}}\\le\\frac{3-2.65}{0.85/\\sqrt{n}}\\right)=P(Z\\le \\frac{0.35}{0.85}\\sqrt{n})=0.99.\\] The us then find the z-score that give a probability of 0.99 using the quantile function or percent point function (the inverse of cdf):\n\n\nprint(norm.ppf(0.99))\n\n2.3263478740408408\n\n\nThat is \\(P(Z\\le 2.3263) = 0.99\\), so to find \\(n\\) we need to solve \\[\\frac{0.35}{0.85}\\sqrt n = 2.3263\\Leftrightarrow n=2.3263^2\\frac{0.85^2}{0.35^2}=31.91\\approx 32.\\] Note that we round off \\(n\\) to an integer and we should here always round up to ensure that the probability is not less than 0.99.\n\n\n\nProblem 4\nThe Central Limit Theorem states that the sample mean \\(\\bar X\\) follows an approximately normal distribution when the sample size is sufficiently large. More precisely, the theorem asserts that the standardized version of \\(\\bar X\\) given by \\[\\frac{\\bar X-\\mu}{\\sigma/\\sqrt{n}},\\] converges in distribution to the standard normal distribution as \\(n\\) increases.\nHow does this relate to the Law of Large Numbers? If the standardized \\(\\bar X\\) follows an approximate standard normal distribution, what does this imply about the distribution of \\(\\bar X\\) itself?\n\n\nShow solutions\n\nThe Law of Large numbers states that \\(\\bar X\\) will approach \\(\\mu\\) when \\(n\\) becomes large. The central limit theorem implies that \\(\\bar X\\) will follow a \\(N(\\mu, \\sigma^2/n)\\) distribution asymptotically.\n\n\n\nProblem 5\nSuppose \\(X_1, X_2, \\dots, X_n\\) are independent and identically distributed (i.i.d.) random variables from an exponential distribution with parameter \\(\\lambda &gt; 0\\), i.e.,\n\\[f(x \\mid \\lambda) = \\lambda e^{-\\lambda x}, \\quad x &gt; 0.\\]\nFind the Maximum Likelihood Estimator (MLE) for \\(\\lambda\\).\n\n\nShow solutions\n\nThe likelihood function is: \\[L(\\lambda) = \\prod_{i=1}^{n} \\lambda e^{-\\lambda X_i} = \\lambda^n e^{-\\lambda \\sum X_i}.\\]\nThe log-likelihood function is: \\[\\ell(\\lambda) = n \\log \\lambda - \\lambda \\sum X_i.\\]\nDifferentiating with respect to \\(\\lambda\\): \\[\\frac{d\\ell}{d\\lambda} = \\frac{n}{\\lambda} - \\sum X_i.\\]\nSetting it equal to zero: \\[\\frac{n}{\\lambda} = \\sum X_i.\\]\nSolving for \\(\\lambda\\): \\[\\hat{\\lambda} = \\frac{n}{\\sum X_i}.\\]\n\n\n\nProblem 6\nLet \\(X_1, X_2, \\dots, X_n\\) be i.i.d. random variables from a normal distribution with unknown mean \\(\\mu\\) and known variance \\(\\sigma^2\\). That is, \\[X_i \\sim N(\\mu, \\sigma^2),\\] such that the density of the X’s is \\[f(x,\\mu,\\sigma)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp \\left( -\\frac{(x - \\mu)^2}{2\\sigma^2}\\right).\\]\nFind the MLE of \\(\\mu\\).\n\n\nShow solutions\n\nThe likelihood function is: \\[L(\\mu) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp \\left( -\\frac{(X_i - \\mu)^2}{2\\sigma^2} \\right).\\]\nThe log-likelihood function is: \\[\\ell(\\mu) = -\\frac{n}{2} \\log (2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (X_i - \\mu)^2.\\]\nDifferentiating with respect to \\(\\mu\\): \\[\\frac{d\\ell}{d\\mu} = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} (X_i - \\mu).\\]\nSetting it equal to zero: \\[\\sum_{i=1}^{n} (X_i - \\mu) = 0.\\]\nSolving for \\(\\mu\\): \\[\\hat{\\mu} = \\frac{1}{n} \\sum_{i=1}^{n} X_i.\\]\n\n\n\nProblem 7\nLet \\(X_1, X_2, \\dots, X_n\\) be i.i.d. random variables from a Bernoulli distribution with unknown parameter \\(p\\). That is, \\[P(X_i = 1) = p, \\quad P(X_i = 0) = 1 - p.\\] Find the MLE for \\(p\\).\nHint: We can also write the Bernoulli distribution as \\(P(X=x) =p^x(1-p)^{1-x}\\), where \\(x\\in \\{0,1\\}\\).\n\n\nShow solutions\n\nThe likelihood function is: \\[L(p) = \\prod_{i=1}^{n} p^{X_i} (1 - p)^{1 - X_i}.\\]\nThe log-likelihood function is: \\[\\ell(p) = \\sum_{i=1}^{n} X_i \\log p + (1 - X_i) \\log (1 - p).\\]\nDifferentiating with respect to \\(p\\): \\[\\frac{d\\ell}{dp} = \\sum_{i=1}^{n} \\frac{X_i}{p} - \\sum_{i=1}^{n} \\frac{1 - X_i}{1 - p}.\\]\nSetting it equal to zero: \\[\\sum X_i \\frac{1}{p} - \\sum (1 - X_i) \\frac{1}{1 - p} = 0.\\] Rearranging: \\[\\sum X_i (1 - p) = (n - \\sum X_i) p.\\]\nSolving for \\(p\\): \\[\\hat{p} = \\frac{1}{n} \\sum X_i.\\]\n\n\n\nProblem 8\nLet \\(\\hat{\\theta}\\) be an estimator of a parameter \\(\\theta\\). The bootstrap estimate of the bias is given by:\n\\[\\hat{\\text{Bias}} = \\frac{1}{B} \\sum_{b=1}^{B} \\hat{\\theta}^*_b - \\hat{\\theta},\\]\nwhere \\(\\hat{\\theta}^*_b\\) is the estimate from bootstrap sample \\(b\\).\nUse bootstrap resampling to estimate the bias of the maximum likelihood estimator of the variance;\\[s^2=\\frac1n \\sum_{i}(x_i-\\bar x)^2,\\] for a set of data \\(x_1,\\ldots, x_n\\).\nDescribe the procedure and discuss the sign of the bias. If you want to implement the code, use the following data set:\n\\[X = \\{4.2, 3.9, 5.1, 4.8, 4.3, 5.0, 3.7, 4.5, 4.9, 4.6\\}.\\]\n\n\nShow solutions\n\n\nCompute the sample variance \\(s^2\\) from the original dataset.\nGenerate 1000 bootstrap samples and compute \\(s^2\\) for each.\nCompute the bootstrap estimate of bias using the formula above.\nInterpret the result: If the bias is positive, \\(s^2\\) is overestimating the true variance; if negative, it is underestimating.\n\n\n\n\nProblem 9\nWhen playing the casino game american roulette, there are 38 possible outcomes: 0,00, and 1-36. The 0 and 00 are colored green, while odd numbers from 1-35 are black and even numbers from 2-36 are red.\n\nWhat is the probability of winning if you bet on a black outcome?\n\nA gambler has $100 and bets $1 each time he plays. He plays until he is either out of money or has $150.\n\nHow would you construct a Monte Carlo simulation of a situation where you always bet on black, and want to find the probability of going bankrupt.\nWhat would the probability be of the gambler reaches his goal of $150?\n\n\n\nShow solutions\n\n\nThe probability of winning when betting on black is \\(18/38=0.474\\)\nIt would make sense to use a while loop and continue to simulate unless the gambler is bankrupt or has $200. To simulate the game, use a Bernoulli distribution for each game with success probability \\(18/38=0.474\\). If the Bernoulli variable is 1, increase the “bank” of the gambler by 1 dollar. If it is 0, decrease it by 1 dollar. Repeat the game 10 000 times and calculate the relative frequency of going bankrupt.\n\n\nimport random\nrandom.seed(123)\n\ndef simulate_game_simple(starting_money=100, goal_money=150, bet_amount=1):\n    money = starting_money\n    while 0 &lt; money &lt; goal_money:\n        # Simulate the outcome: +1 with prob 18/38, -1 with prob 20/38\n        outcome = random.choices([-1, 1], weights=[20, 18])[0]\n        money += outcome * bet_amount\n    return money == 0  # True if bankrupt\n\ndef monte_carlo_simulation(n_simulations=10000):\n    bankrupt_count = sum(simulate_game_simple() for _ in range(n_simulations))\n    return bankrupt_count / n_simulations\n\n# Run the simplified simulation\nestimated_prob = monte_carlo_simulation(10000)\nprint(f\"Estimated probability of going bankrupt: {estimated_prob:.4f}\")\n\nEstimated probability of going bankrupt: 0.9949\n\n\n\nIn the end, there are only two outcomes of the experiment. Either the gambler ends up with $0 (bankrupt) or $150. The probability achieving $150 would therefore be 1-the probability of going bankrupt.\n\nFrom the python simulation above, it would then be \\[1-P(bankrupt)=1-0.9949=0.0051\\]\n\n\n\nProblem 10\nLet \\(X_1,\\ldots, X_n\\) be a random sample from the probability density function \\[f(x|\\theta) = \\theta x^{\\theta -1},\\quad 0&lt;\\theta&lt;\\infty,\\quad 0\\le x\\le1.\\] Find the MLE of \\(\\theta\\).\n\n\nShow solutions\n\n\\[L(\\theta)=f(x_1,\\ldots, x_n|\\theta) = \\theta^n \\prod_{i=1}^nx_i^{\\theta -1}\\] \\[\\log L = n\\log \\theta + (\\theta-1)\\sum_{i=1}^n\\log x_i\\] \\[\\frac{\\partial \\log L}{\\partial \\theta} = \\frac{n}{\\theta} + \\sum_{i=1}^n\\log x_i=0\\] \\[\\widehat \\theta = \\frac{-n}{\\sum_{i=1}^n \\log x_i}.\\]",
    "crumbs": [
      "Modules",
      "Module 3: Estimation, sampling distributions and resampling",
      "Exercises"
    ]
  },
  {
    "objectID": "3-maximum-likelihood.html",
    "href": "3-maximum-likelihood.html",
    "title": "Maximum likelihood",
    "section": "",
    "text": "Slides for “Maximum Likelihood Estimation”\nSlides for “Maximum Likelihood Estimator of mu”\n\n\n\nWhat is the likelihood?\nWhy do we use the log-likelihood?\nWhat are the assumptions for MLE?\nWhat is the Maximum Likelihood Estimator of the mean in a normal distribution?",
    "crumbs": [
      "Modules",
      "Module 3: Estimation, sampling distributions and resampling",
      "Maximum likelihood"
    ]
  },
  {
    "objectID": "3-maximum-likelihood.html#control-questions",
    "href": "3-maximum-likelihood.html#control-questions",
    "title": "Maximum likelihood",
    "section": "",
    "text": "What is the likelihood?\nWhy do we use the log-likelihood?\nWhat are the assumptions for MLE?\nWhat is the Maximum Likelihood Estimator of the mean in a normal distribution?",
    "crumbs": [
      "Modules",
      "Module 3: Estimation, sampling distributions and resampling",
      "Maximum likelihood"
    ]
  },
  {
    "objectID": "3-population-vs-sample.html",
    "href": "3-population-vs-sample.html",
    "title": "Population vs sample",
    "section": "",
    "text": "Slides for “Population vs sample”\n\n\n\nWhat is the difference between a population and sample?\nWhat do we call numbers derived from a population?\nWhat do we call numbers derived from a sample?\nWhat does it mean that a sample is representative?\nWhat is sampling bias?",
    "crumbs": [
      "Modules",
      "Module 3: Estimation, sampling distributions and resampling",
      "Population vs sample"
    ]
  },
  {
    "objectID": "3-population-vs-sample.html#control-questions",
    "href": "3-population-vs-sample.html#control-questions",
    "title": "Population vs sample",
    "section": "",
    "text": "What is the difference between a population and sample?\nWhat do we call numbers derived from a population?\nWhat do we call numbers derived from a sample?\nWhat does it mean that a sample is representative?\nWhat is sampling bias?",
    "crumbs": [
      "Modules",
      "Module 3: Estimation, sampling distributions and resampling",
      "Population vs sample"
    ]
  },
  {
    "objectID": "3-sampling-error.html",
    "href": "3-sampling-error.html",
    "title": "Sampling error and distribution",
    "section": "",
    "text": "Slides for “Sampling error and distribution”\n\n\n\nWhat is a sampling error?\nWhat is a sampling distribution?\nHow does the sample mean standard deviation depend on the sample size?",
    "crumbs": [
      "Modules",
      "Module 3: Estimation, sampling distributions and resampling",
      "Sampling error and distribution"
    ]
  },
  {
    "objectID": "3-sampling-error.html#control-questions",
    "href": "3-sampling-error.html#control-questions",
    "title": "Sampling error and distribution",
    "section": "",
    "text": "What is a sampling error?\nWhat is a sampling distribution?\nHow does the sample mean standard deviation depend on the sample size?",
    "crumbs": [
      "Modules",
      "Module 3: Estimation, sampling distributions and resampling",
      "Sampling error and distribution"
    ]
  },
  {
    "objectID": "3-what-is-a-model.html",
    "href": "3-what-is-a-model.html",
    "title": "What is a model?",
    "section": "",
    "text": "Slides for “What is a model?”\n\n\n\nWhat is a statistical model modelling?\nWhat is a residual?\nWhat does it mean to be identically distributed?\nWhat does it mean to be independently distributed?",
    "crumbs": [
      "Modules",
      "Module 3: Estimation, sampling distributions and resampling",
      "What is a model?"
    ]
  },
  {
    "objectID": "3-what-is-a-model.html#control-questions",
    "href": "3-what-is-a-model.html#control-questions",
    "title": "What is a model?",
    "section": "",
    "text": "What is a statistical model modelling?\nWhat is a residual?\nWhat does it mean to be identically distributed?\nWhat does it mean to be independently distributed?",
    "crumbs": [
      "Modules",
      "Module 3: Estimation, sampling distributions and resampling",
      "What is a model?"
    ]
  },
  {
    "objectID": "4-exercises.html",
    "href": "4-exercises.html",
    "title": "Exercises",
    "section": "",
    "text": "For the following exercises, we will use the test statistic for one-sample t-test with null hypothesis \\(H_0:\\mu=\\mu_0\\): \\[T=\\frac{\\bar x-\\mu_0}{s/\\sqrt{n}},\\] where \\(n\\) is the sample size, \\(\\bar x\\) is the sample mean, \\(s\\) is the sample standard deviation and \\(\\mu_0\\) is the expected value under the null hypothesis.\n\n\nA company selling premium coffee beans wants to determine whether reducing the price of its product leads to a significant increase in the average number of bags sold per day. The marketing team conducted a test by lowering the price for a month in select stores and measuring the sales volume. They want to analyze whether the mean daily sales after the price reduction are significantly higher than before.\n\nSet up the null- and alternative hypothesis, and decide on a suitable significance level.\n\nBefore the price reduction, the mean daily sales were 200 bags. After the price reduction, a sample of 30 days of sales data showed a sample mean of 215 bags, with standard deviation 25 bags.\n\nCalculate the test statistic.\nFind the p-value. What is the conclusion?\n\n\n\nShow solutions\n\n\n\\(H_0: \\mu\\le 200\\) vs \\(H_1: \\mu&gt;200\\). We could allow for a relatively high significance level in this context, because rejecting a true null hypothesis (Type I error) will most likely not have large consequences, e.g. \\(\\alpha =0.1\\).\n\\(T=(215-200)/(25/\\sqrt{30})=3.29\\)\n\\(\\text{p-value}=P(T_{29}&gt;3.29)=0.00132\\). The p-value is very low, indicating that we may reject the null hypothesis at (almost) any significance level in (a). The company should continue with the reduced price.\n\n\nfrom scipy import stats\nprint(1-stats.t.cdf(3.29, 29))\n\n0.0013169979736814552\n\n\n\n\n\n\nA large company is trying to encourage employees to save more for retirement. Previously, employees had to actively sign up for the company’s pension savings plan (opt-in system). Recently, the company changed its policy so that employees are automatically enrolled in the plan but can opt out if they wish (opt-out system). Behavioral economists suggest that default options strongly influence decision-making, leading to higher participation rates in savings plans.\nThe HR department wants to test with a significance level of 5% whether the average monthly contribution to the retirement plan has increased significantly after implementing the opt-out system.\n\nSet up the null- and alternative hypothesis.\n\nBefore the change, the average monthly contribution was $250 per employee. After the policy change, a random sample of 50 employees showed: Sample mean of $270 and standard deviation of $90.\n\nCalculate the test statistic.\nFind the p-value. What is the conclusion?\n\n\n\nShow solutions\n\n\n\\(H_0: \\mu\\le 250\\) vs \\(H_1: \\mu&gt;250\\).\n\\(T=(270-250)/(90/\\sqrt{50})=1.57\\)\n\\(\\text{p-value}=P(T_{29}&gt;3.29)=0.0613\\). The p-value is higher than 5%, indicating that we may reject not the null hypothesis. The pension payments have not increased due to the change from opting in to opting out.\n\n\nfrom scipy import stats\nprint(1-stats.t.cdf(2.94, 49))\n\n0.002498152330066006\n\n\n\n\n\n\nA nutritionist wants to test whether a new diet plan leads to a significant reduction in average daily calorie intake. A sample of 40 individuals followed the diet for one month, and their average daily calorie intake was recorded.\nThe recommended daily intake before the diet was 2200 kcal. After one month, the sample had a mean intake of 2100 kcal with a standard deviation of 250 kcal.\n\nSet up the null- and alternative hypotheses.\nCalculate the test statistic.\nFind the p-value and interpret the result.\n\n\n\nShow solutions\n\n\n\\(H_0: \\mu\\geq 2200\\) vs \\(H_1: \\mu&lt;2200\\).\n\\[T=\\frac{2100-2200}{250/\\sqrt{40}}=-2.53\\]\n\\[\\text{p-value}=P(T_{39}&lt;-2.53)=0.0079\\]\n\nSince the p-value is lower than the conventional 5% significance level, we reject the null hypothesis, suggesting that the new diet plan significantly reduces daily calorie intake.\n\nfrom scipy import stats\nprint(stats.t.cdf(-2.53, 39))\n\n0.007778305846562239\n\n\n\n\n\n\nA smartphone manufacturer wants to determine whether the battery life of their new model is significantly different from the advertised 24 hours. A sample of 50 devices was tested, showing a mean battery life of 23.5 hours with a standard deviation of 1.2 hours.\n\nFormulate the hypotheses.\nCompute the test statistic.\nDetermine the p-value and state your conclusion.\n\n\n\nShow solutions\n\n\n\\(H_0: \\mu=24\\) vs \\(H_1: \\mu\\neq 24\\).\n\\[T=\\frac{23.5-24}{1.2/\\sqrt{50}}=-2.946\\]\n\\[\\text{p-value}=2 \\times P(T_{49}&lt;-2.94)=0.0049\\]\n\nSince the p-value is very small, we reject the null hypothesis, indicating that the battery life is significantly different from 24 hours.\n\nprint(2*stats.t.cdf(-2.946, 49))\n\n0.004914894166068594",
    "crumbs": [
      "Modules",
      "Module 4: Designing studies, hypothesis testing, and quantifying effects",
      "Exercises"
    ]
  },
  {
    "objectID": "4-exercises.html#one-sample-t-test",
    "href": "4-exercises.html#one-sample-t-test",
    "title": "Exercises",
    "section": "",
    "text": "For the following exercises, we will use the test statistic for one-sample t-test with null hypothesis \\(H_0:\\mu=\\mu_0\\): \\[T=\\frac{\\bar x-\\mu_0}{s/\\sqrt{n}},\\] where \\(n\\) is the sample size, \\(\\bar x\\) is the sample mean, \\(s\\) is the sample standard deviation and \\(\\mu_0\\) is the expected value under the null hypothesis.\n\n\nA company selling premium coffee beans wants to determine whether reducing the price of its product leads to a significant increase in the average number of bags sold per day. The marketing team conducted a test by lowering the price for a month in select stores and measuring the sales volume. They want to analyze whether the mean daily sales after the price reduction are significantly higher than before.\n\nSet up the null- and alternative hypothesis, and decide on a suitable significance level.\n\nBefore the price reduction, the mean daily sales were 200 bags. After the price reduction, a sample of 30 days of sales data showed a sample mean of 215 bags, with standard deviation 25 bags.\n\nCalculate the test statistic.\nFind the p-value. What is the conclusion?\n\n\n\nShow solutions\n\n\n\\(H_0: \\mu\\le 200\\) vs \\(H_1: \\mu&gt;200\\). We could allow for a relatively high significance level in this context, because rejecting a true null hypothesis (Type I error) will most likely not have large consequences, e.g. \\(\\alpha =0.1\\).\n\\(T=(215-200)/(25/\\sqrt{30})=3.29\\)\n\\(\\text{p-value}=P(T_{29}&gt;3.29)=0.00132\\). The p-value is very low, indicating that we may reject the null hypothesis at (almost) any significance level in (a). The company should continue with the reduced price.\n\n\nfrom scipy import stats\nprint(1-stats.t.cdf(3.29, 29))\n\n0.0013169979736814552\n\n\n\n\n\n\nA large company is trying to encourage employees to save more for retirement. Previously, employees had to actively sign up for the company’s pension savings plan (opt-in system). Recently, the company changed its policy so that employees are automatically enrolled in the plan but can opt out if they wish (opt-out system). Behavioral economists suggest that default options strongly influence decision-making, leading to higher participation rates in savings plans.\nThe HR department wants to test with a significance level of 5% whether the average monthly contribution to the retirement plan has increased significantly after implementing the opt-out system.\n\nSet up the null- and alternative hypothesis.\n\nBefore the change, the average monthly contribution was $250 per employee. After the policy change, a random sample of 50 employees showed: Sample mean of $270 and standard deviation of $90.\n\nCalculate the test statistic.\nFind the p-value. What is the conclusion?\n\n\n\nShow solutions\n\n\n\\(H_0: \\mu\\le 250\\) vs \\(H_1: \\mu&gt;250\\).\n\\(T=(270-250)/(90/\\sqrt{50})=1.57\\)\n\\(\\text{p-value}=P(T_{29}&gt;3.29)=0.0613\\). The p-value is higher than 5%, indicating that we may reject not the null hypothesis. The pension payments have not increased due to the change from opting in to opting out.\n\n\nfrom scipy import stats\nprint(1-stats.t.cdf(2.94, 49))\n\n0.002498152330066006\n\n\n\n\n\n\nA nutritionist wants to test whether a new diet plan leads to a significant reduction in average daily calorie intake. A sample of 40 individuals followed the diet for one month, and their average daily calorie intake was recorded.\nThe recommended daily intake before the diet was 2200 kcal. After one month, the sample had a mean intake of 2100 kcal with a standard deviation of 250 kcal.\n\nSet up the null- and alternative hypotheses.\nCalculate the test statistic.\nFind the p-value and interpret the result.\n\n\n\nShow solutions\n\n\n\\(H_0: \\mu\\geq 2200\\) vs \\(H_1: \\mu&lt;2200\\).\n\\[T=\\frac{2100-2200}{250/\\sqrt{40}}=-2.53\\]\n\\[\\text{p-value}=P(T_{39}&lt;-2.53)=0.0079\\]\n\nSince the p-value is lower than the conventional 5% significance level, we reject the null hypothesis, suggesting that the new diet plan significantly reduces daily calorie intake.\n\nfrom scipy import stats\nprint(stats.t.cdf(-2.53, 39))\n\n0.007778305846562239\n\n\n\n\n\n\nA smartphone manufacturer wants to determine whether the battery life of their new model is significantly different from the advertised 24 hours. A sample of 50 devices was tested, showing a mean battery life of 23.5 hours with a standard deviation of 1.2 hours.\n\nFormulate the hypotheses.\nCompute the test statistic.\nDetermine the p-value and state your conclusion.\n\n\n\nShow solutions\n\n\n\\(H_0: \\mu=24\\) vs \\(H_1: \\mu\\neq 24\\).\n\\[T=\\frac{23.5-24}{1.2/\\sqrt{50}}=-2.946\\]\n\\[\\text{p-value}=2 \\times P(T_{49}&lt;-2.94)=0.0049\\]\n\nSince the p-value is very small, we reject the null hypothesis, indicating that the battery life is significantly different from 24 hours.\n\nprint(2*stats.t.cdf(-2.946, 49))\n\n0.004914894166068594",
    "crumbs": [
      "Modules",
      "Module 4: Designing studies, hypothesis testing, and quantifying effects",
      "Exercises"
    ]
  },
  {
    "objectID": "4-exercises.html#two-sample-t-test",
    "href": "4-exercises.html#two-sample-t-test",
    "title": "Exercises",
    "section": "Two-sample t-test",
    "text": "Two-sample t-test\nFor the following exercises, we will use the test statistic for two-sample t-test with null hypothesis \\(H_0:\\mu_1=\\mu_2+d_0\\): \\[T=\\frac{\\bar x_1-\\bar x_2-d_0}{\\sqrt{s_1^2/n_1+s_2^2/n_2}},\\] where \\(n_1\\) and \\(n_2\\) are the sample sizes of the two samples, \\(\\bar x_1\\) and \\(\\bar x_2\\) are the sample means, \\(s_1\\) and \\(s_2\\) are the sample standard deviations and \\(d_0\\) is the difference in expected value under the null hypothesis (often \\(d_0=0\\)). The test statistic follows a t-distribution with degrees of freedom (df): \\[\\text{df}=\\frac{\\big(\\frac{s_1^2}{n_1}+\\frac{s_2^2}{n_2}\\big)^2}{\\frac{(s_1^2/n_1)^2}{n_1-1}+\\frac{(s_1^2/n_2)^2}{n_2-1}}\\]\n\nProblem 2.1\nAn e-commerce company wants to test whether showing customer reviews on product pages increases the average number of purchases. Behavioral economics suggests that social proof (i.e., seeing others’ positive experiences) influences decision-making and can lead to higher sales.\nTo test this, the company randomly selected two groups of products: one group displayed customer reviews, and the other did not. They then compared the average daily sales per product between the two groups.\n\nSet up the null- and alternative hypothesis. Use 10% significance level.\nDiscuss how the test statistic will behave under the various hypothesis.\n\nThe experiment result in the following data:\n\nProducts with reviews: Sample mean daily sales = 120 units, Sample standard deviation = 30, Sample size = 40\nProducts without reviews: Sample mean daily sales = 105 units, Sample standard deviation = 28, Sample size = 40\n\n\nCalculate the test statistic and the degrees of freedom.\nFind the p-value. What is the conclusion?\n\n\n\nShow solutions\n\nLet \\(\\mu_1\\) be the expected daily sales with reviews and \\(\\mu_2\\) without reviewes.\n\n\\(H_0: \\mu_1\\le \\mu_2\\) vs \\(H_A: \\mu_1&gt;\\mu_2\\).\nUnder the null hypothesis, we expect the numerator of the test statistic to be close to zero, making the test statistic close to zero. Under the alternative hypothesis, we expect \\(\\bar x_1&gt;\\bar x_2\\) such that the numerator of \\(T\\) will be positive. Higher values will therefore favor the alternative hypothesis.\n\n\n\nimport numpy as np\ns1 = 30;  s2 = 28; n1 = 40; n2 = 40\nm1 = 120; m2 = 105;\ndf = (s1**2/n1 + s2**2/n2)**2/((s1**2/n1)**2/(n1-1)+(s2**2/n2)**2/(n2-1))\nt = (m1-m2)/np.sqrt(s1*s1/n1+s2*s2/n2)\nprint(\"df=\",df)\nprint(\"T=\",t)\n\ndf= 77.63164160330635\nT= 2.311799743112827\n\n\n\\(T=(102-105)/\\sqrt{30^2/40+28^2/40}=-0.462\\) \\[\\text{df}=\\frac{\\big(\\frac{s_1^2}{n_1}+\\frac{s_2^2}{n_2}\\big)^2}{\\frac{(s_1^2/n_1)^2}{n_1-1}+\\frac{(s_1^2/n_2)^2}{n_2-1}}\\]\n\n\\(\\text{p-value}=P(T_{29}&gt;0.462)=0.32\\). The p-value is lower than 10%, indicating that we may reject the null hypothesis. There is enough evidence to suggest that the reviews increase the sales at a 10% significance level. The company should show reviews of their products to increase sales.\n\n\nfrom scipy import stats\nprint(1-stats.t.cdf(t, df))\n\n0.011720746167147467\n\n\n\n\n\nProblem 2.2\nA company is testing whether offering a performance-based bonus increases employee productivity. They implement a new bonus system in one office location (Group A), while another office continues without bonuses (Group B). After three months, they measure the average number of completed sales calls per week in both offices. The company wants you to determine if the bonus system leads to higher productivity (mean number of sales calls).\n\nFormulate the hypothesis problem including \\(H_0\\), \\(H_A\\) and setting a suitable significance level.\n\nGroup A (Bonus group):\n\nSample mean: 52 sales calls per week\nSample standard deviation: 8\nSample size: n = 35\n\nGroup B (No bonus group):\n\nSample mean: 48 sales calls per week\nSample standard deviation: 9\nSample size: n = 35\n\n\nCalculate the test statistic based on this information. Find the p-value.\nWhat is your conclusion and your recommendation to the company?\n\n\n\nShow solutions\n\n\n\\(H_0: \\mu_A =\\mu_B\\) vs \\(H_A: \\mu_A&gt;\\mu_B\\). In this case, the bonus scheme might be expensive, so we want to avoid type I errors (rejecting the null, when it is truly no difference in productivity). We use \\(\\alpha = 1\\%\\).\n\\(T=1.96\\) and p-value \\(=0.0267\\).\nThe p-value is low, indicating evidence for the alternative hypothesis, but we have set a significance level of 1%, for which the p-value exceeds. This would lead us to not recommend implementing the bonus, because it does not improve productivity to a high enough degree at this significance level.\n\n\nm1 = 52; s1 = 8; n1 = 35\nm2 = 48; s2 = 9; n2 = 35\nstats.ttest_ind_from_stats(m1,s1,n1,m2,s2,n2,alternative=\"greater\")\n\nTtest_indResult(statistic=np.float64(1.9652147377620703), pvalue=np.float64(0.026737892346682984))\n\n\n\n\n\nProblem 2.3\nA university wants to determine if students who attend tutoring sessions perform better on exams than those who do not. Two independent samples of students were taken:\n\nTutoring group: Mean score = 78, Standard deviation = 10, Sample size = 30\nNon-tutoring group: Mean score = 72, Standard deviation = 12, Sample size = 35\n\n\nState the hypotheses.\nCalculate the test statistic and degrees of freedom.\nFind the p-value and interpret the result.\n\n\n\nShow solutions\n\n\n\\(H_0: \\mu_1 = \\mu_2\\) vs \\(H_A: \\mu_1 &gt; \\mu_2\\).\n\n\n\ns1 = 10; s2 = 12; n1 = 30; n2 = 35\nm1 = 78; m2 = 72;\ndf = (s1**2/n1 + s2**2/n2)**2 / ((s1**2/n1)**2/(n1-1) + (s2**2/n2)**2/(n2-1))\nt = (m1-m2) / ((s1**2/n1 + s2**2/n2)**0.5)\nprint(\"df=\", df)\nprint(\"T=\", t)\n\ndf= 62.958820084919275\nT= 2.1985812677253573\n\n\n\nThe computed p-value is \\(P(T_{63}&gt;t)=0.0158\\). The evidence thus points towards the conclusion that attending tutoring improves performance, due to a low p-value.\n\n\nprint(1-stats.t.cdf(t, df))\n\n0.01579534729247911",
    "crumbs": [
      "Modules",
      "Module 4: Designing studies, hypothesis testing, and quantifying effects",
      "Exercises"
    ]
  },
  {
    "objectID": "4-exercises.html#tests-of-proportions-one-sample",
    "href": "4-exercises.html#tests-of-proportions-one-sample",
    "title": "Exercises",
    "section": "Tests of proportions: One sample",
    "text": "Tests of proportions: One sample\nLet \\(p\\) denote a proportion and \\(p_0\\) be the proportion under the null hypothesis. We can then test the null hypothesis \\(H_0: p=p_0\\) against an alternative hypothesis using the test statistic \\[Z = \\frac{\\hat p-p_0}{\\sqrt{p_0(1-p_0)/n}},\\] which is approximately normally distributed when \\(n\\) is sufficiently large and the probabilies are not too small (often \\(np&gt;5\\) is used as a rule of thumb).\n\nProblem 3.1\nA retail chain is considering phasing out cash payments in favor of digital transactions. Management wants to determine whether at least 60% of customers are already using mobile payment apps (such as Apple Pay or Google Pay) when shopping. If the proportion is significantly higher than 60%, they may move forward with reducing cash payment options.\n\nSet up the null hypothesis and alternative hypothesis. Discuss how the test statistic will behave under the various hypotheses.\n\nThe retail chain conducts a survey where they ask customers whether they used a mobile payment app for their most recent purchase. Out of the 400 customers asked, 260 used mobile payment apps.\n\nCalculate \\(Z\\) based on this information. What is the p-value?\nWhat does the p-value tell you about people’s payment preferences?\n\n\n\nShow solutions\n\n\n\\(H_0: p=0.6\\) vs \\(H_A: \\mu&gt;0.6\\). The numerator of \\(Z\\) will be expected close to 0 if the null is zero, making Z close to zero. If \\(H_A\\) is true, \\(\\widehat p\\) will be expected larger than 0.6, making Z positive. Large values of \\(Z\\) will favor \\(H_A\\).\n\\(Z = (0.65-0.6)/\\sqrt{0.6\\cdot 0.4/400}=2.04\\) and p-value\\(=P(Z&gt;2.04)=0.0206\\).\n\n\nphat = 260/400\nz = (phat-0.6)/np.sqrt(0.6*(1-0.6)/400)\nprint(\"Z=\", z)\nprint(\"Pvalue: \", 1-stats.norm.cdf(z))\n\nZ= 2.041241452319317\nPvalue:  0.02061341666858174\n\n\n\nThe p-value is low, which indicates evidence against \\(H_0\\). It indicates that the proportion of mobile app payments is above 60%.\n\n\n\n\nProblem 3.2\nA sustainable fashion brand wants to know whether at least 30% of consumers are willing to pay a premium for eco-friendly clothing. If the proportion is significantly higher than 30% with significance level 10%, they will launch a new premium-priced, eco-friendly clothing line.\nThey conduct a market survey where respondents indicate whether they would be willing to pay 10% more for sustainably produced clothing.\n\nSet up the null- and alternative hypothesis.\n\nThe market survey got 500 respondents, and out of those, 166 were willing to pay 10% more for sustainable clothing.\n\nCalculate \\(Z\\) based on this information. What is the p-value?\nWhat would your recommendation to the fashion brand be?\n\n\n\nShow solutions\n\n\n\\(H_0: p\\le 30\\%\\) vs \\(H_A: p&gt;30\\%\\).\n\\(\\hat p=166/500= 33.2\\%\\), \\(z=(0.332-0.3)/\\sqrt{0.3*0.7/500}=1.56\\) and \\(P(Z&gt;z)=P(Z&gt;1.56)=1-\\Phi(1.56)=0.0592\\).\nAt a 10% signifance level, we would reject the null hypothesis, since the p-value of 0.06 is smaller than 0.1. At this significance level, we would recommend the brand to go ahead with the launch of their eco-friendly clothing.\n\n\nphat = 166/500\nz = (phat-0.3)/np.sqrt(0.3*(1-0.3)/500)\nprint(\"Z=\", z)\nprint(\"Pvalue: \", 1-stats.norm.cdf(z))\n\nZ= 1.5614401167176546\nPvalue:  0.05920997135406203\n\n\n\n\n\nProblem 3.3\nA public transportation agency wants to determine whether at least 70% of residents support a new subway line expansion. A survey of 600 residents finds that 435 support the expansion.\n\nFormulate the hypotheses.\nCompute the test statistic and p-value.\nWhat is the conclusion?\n\n\n\nShow solutions\n\n\n\\(H_0: p = 0.7\\) vs \\(H_A: p &gt; 0.7\\).\n\\[Z=\\frac{0.725-0.7}{\\sqrt{0.7\\cdot 0.3/600}}=1.63\\]\n\np-value \\(=P(Z&gt;1.63)=0.0907\\)\n\np_hat = 435/600\nz = (p_hat-0.7) / ( (0.7*0.3/600)**0.5 )\nprint(\"Z=\", z)\nprint(\"p-value=\", 1-stats.norm.cdf(z))\n\nZ= 1.336306209562123\np-value= 0.09072460386070991\n\n\n\nA p-value of 9% indicates that the evidence against the null hypothesis is not very strong, and at a 5% significance level, we would not reject the null hypothesis.",
    "crumbs": [
      "Modules",
      "Module 4: Designing studies, hypothesis testing, and quantifying effects",
      "Exercises"
    ]
  },
  {
    "objectID": "4-exercises.html#tests-of-proportions-two-samples",
    "href": "4-exercises.html#tests-of-proportions-two-samples",
    "title": "Exercises",
    "section": "Tests of proportions: Two samples",
    "text": "Tests of proportions: Two samples\nLet \\(p_1\\) and \\(p_2\\) denote two proportions. We can then test the null hypothesis \\(H_0: p_1=p_2\\) against an alternative hypothesis using the test statistic \\[Z = \\frac{\\hat p_1-\\hat p_2}{\\sqrt{p_\\text{pool}(1-p_\\text{pool})(1/n_1+1/n_2)}},\\] where \\[p_\\text{pool}=\\frac{n_1\\widehat p_1+n_2\\widehat p_2}{n_1+n_2}.\\] The test statistic Z is approximately normally distributed.\n\nProblem 4.1\nAn insurance company is testing how the way they frame their sales pitch affects customers’ willingness to buy a travel insurance policy. They randomly assign potential customers into two groups:\n\nLoss-framed message: “If you don’t buy travel insurance, you could lose thousands in unexpected expenses.”\nGain-framed message: “If you buy travel insurance, you’ll have peace of mind and financial protection.”\n\nBehavioral economics suggests that loss aversion makes people more sensitive to potential losses than gains, meaning the loss-framed message might lead to higher purchase rates.\n\nSet up the null- and alternative hypothesis.\nDiscuss how the test statistic will behave under the various hypothesis.\n\nThe experiment result in the following data:\n\nLoss-framed message: 200 customers surveyed, 45% purchased insurance\nGain-framed message: 200 customers surveyed, 35% purchased insurance\n\n\nCalculate the test statistic and the p-value.\nConclude the test.\n\n\n\nShow solutions\n\nLet \\(p_L\\) denote proportion of purchases receiving a loss-framed message and \\(p_G\\) denote proportion of purchases receiving a gain-framed message. a) \\(H_0: p_L=p_G\\) vs \\(H_A: p_L&gt;p_G\\).\n\nWe use \\(p_1=p_L\\) and \\(p_2 =p_G\\) in refence to the formula for the test statistic. If the null hypothesis is true, Z is expected to be close to zero since the numerator is expected to be near 0. If the alternative hypothesis is true, we expect \\(\\hat p_L&gt;\\hat p_G\\), giving a positive numerator. Thus, large values of Z will favor the alternative hypothesis.\n\n\n\npL = 0.45\npG = 0.35\nnL = 200\nnG = 200\npPool = (nL*pL + nG*pG)/(nL+nG)\nz = (pL-pG)/np.sqrt(pPool*(1-pPool)*(1/nL+1/nG))\nprint(\"Z=\",z)\nprint(\"pvalue=\",1-stats.norm.cdf(z))\n\nZ= 2.041241452319316\npvalue= 0.02061341666858174\n\n\n\nThe p-value is low (around 2%). This indicates that the probability of making a Type I error is low and favor rejecting the null hypothesis. In that sense, the evidence points towards people being willing to pay to avoid loss than they are to potentially gain.\n\n\n\n\nProblem 4.2\nAn online retailer wants to determine whether offering free shipping increases the proportion of customers who complete their purchases. They run an A/B test, where one group of customers sees free shipping on all orders (Group A), while another group sees the standard shipping fees (Group B). After a week, they compare the proportion of customers who completed a purchase in each group. The retailer asks you to use 5% significance level.\n\nFormulate the hypothesis problem.\n\nFor the two groups, the retailer gathered the following data:\nGroup A (Standard shipping):\n\nSample size: 469\nCompleted purchases: 191\n\nGroup B (Free shipping)\n\nSample size: 483\nCompleted purchases: 231\n\n\nPerform the test: Find Z and the p-value.\nWhat is your conclusion?\nThe chief economist in the company has taken a statistics course and learned about the difference between statistical significance and economical significance. How would this distinction influence your recommendation to the company?\n\n\n\nShow solutions\n\n\n\\(H_0: p_A=p_B\\) vs \\(H_A: p_A&lt;p_B\\).\n\n\n\nnA = 469\nnB = 483\npA = 191/nA\npB = 231/nB\nprint(pB-pA)\npPool = (nA*pA + nB*pB)/(nA+nB)\nz = (pA-pB)/np.sqrt(pPool*(1-pPool)*(1/nA+1/nB))\nprint(\"Z=\",z)\nprint(\"pvalue=\",stats.norm.cdf(z))\n\n0.07101140261425792\nZ= -2.2050192819722274\npvalue= 0.01372637075205902\n\n\n\nAt a 5% significance level, the free shipping group have a higher completing rate than the standard shipping group.\nHere the effect size \\(|\\widehat p_A-\\widehat p_B| = 7\\%\\), that is 7 percentage points higher rate of completing the order if the shipping is free. This increase in completed orders increases the volume, but the free shipping will reduce the margin on each order. We would need more information to assess whether this specific experiment that resulted in a statistically significant increase in volume, also resulted in a economically significant increase of profits.\n\n\n\n\nProblem 4.3\nA company is testing whether a new website layout increases the proportion of users who complete a purchase. They conduct an A/B test with the following results:\n\nNew layout: 500 users, 210 completed a purchase\nOld layout: 500 users, 180 completed a purchase\n\n\nFormulate the hypotheses.\nCompute the test statistic and p-value.\nInterpret the result.\n\n\n\nShow solutions\n\n\n\\(H_0: p_1 = p_2\\) vs \\(H_A: p_1 &gt; p_2\\).\n\n\n\nfrom scipy import stats\nn1, n2 = 500, 500\np1, p2 = 210/n1, 180/n2\np_pool = (210 + 180) / (n1 + n2)\nz = (p1 - p2) / ((p_pool * (1 - p_pool) * (1/n1 + 1/n2)) ** 0.5)\nprint(\"Z=\", z)\nprint(\"p-value=\", 1-stats.norm.cdf(z))\n\nZ= 1.9450198311991271\np-value= 0.025886295779031898\n\n\n\nSince the p-value is low, this gives reason to doubt the null hypothesis and indicates that the new layout do increase the proportion of customers placing an order.",
    "crumbs": [
      "Modules",
      "Module 4: Designing studies, hypothesis testing, and quantifying effects",
      "Exercises"
    ]
  },
  {
    "objectID": "4-intro.html",
    "href": "4-intro.html",
    "title": "TECH3 Applied statistics",
    "section": "",
    "text": "Designing studies, hypothesis testing, and quantifying effects\n\nFocus: Modern hypothesis testing using bootstrap methods.\nKey topics: classical test statistics, confidence intervals, effect size, power analysis, AB-testing.\nSpecial Emphasis: Statistical significance versus practical significance. Describe the proper interpretations of a p-value and common misinterpretations",
    "crumbs": [
      "Modules",
      "Module 4: Designing studies, hypothesis testing, and quantifying effects",
      "Introduction"
    ]
  },
  {
    "objectID": "4-videos.html",
    "href": "4-videos.html",
    "title": "Videos",
    "section": "",
    "text": "Videos",
    "crumbs": [
      "Modules",
      "Module 4: Designing studies, hypothesis testing, and quantifying effects",
      "Videos"
    ]
  },
  {
    "objectID": "5-categorical-relationships.html",
    "href": "5-categorical-relationships.html",
    "title": "Modeling categorical relationships",
    "section": "",
    "text": "Slides for “Modeling categorical relationships”\n\n\n\nWhat do we test when doing this kind of chi-squared test?\nWhat is the relationship between chi-squared distribution and a standard normal distribution?\nDo we reject the null hypothesis for small or large values of the chi-squared test statistic?",
    "crumbs": [
      "Modules",
      "Module 5: Measuring relationships and fitting models",
      "Modeling categorical relationships"
    ]
  },
  {
    "objectID": "5-categorical-relationships.html#control-questions",
    "href": "5-categorical-relationships.html#control-questions",
    "title": "Modeling categorical relationships",
    "section": "",
    "text": "What do we test when doing this kind of chi-squared test?\nWhat is the relationship between chi-squared distribution and a standard normal distribution?\nDo we reject the null hypothesis for small or large values of the chi-squared test statistic?",
    "crumbs": [
      "Modules",
      "Module 5: Measuring relationships and fitting models",
      "Modeling categorical relationships"
    ]
  },
  {
    "objectID": "5-continuous-relationships-correlation.html",
    "href": "5-continuous-relationships-correlation.html",
    "title": "Modeling continuous relationships: Correlation",
    "section": "",
    "text": "Slides for “Modeling continuous relationships: Correlation”\n\n\n\nIf an increase in X is associated with an increase in Y, what sign will the correlation coefficient have?\nIf an increase in Y is associated with an decrease in X, what sign will the correlation coefficient have?\nWhat type of dependence is described by correlations?\nWhat does zero correlation mean for the relationship between X and Y?\nWhat kind of distribution do we use in a hypothesis test of correlation?",
    "crumbs": [
      "Modules",
      "Module 5: Measuring relationships and fitting models",
      "Modeling continuous relationships: Correlation"
    ]
  },
  {
    "objectID": "5-continuous-relationships-correlation.html#control-questions",
    "href": "5-continuous-relationships-correlation.html#control-questions",
    "title": "Modeling continuous relationships: Correlation",
    "section": "",
    "text": "If an increase in X is associated with an increase in Y, what sign will the correlation coefficient have?\nIf an increase in Y is associated with an decrease in X, what sign will the correlation coefficient have?\nWhat type of dependence is described by correlations?\nWhat does zero correlation mean for the relationship between X and Y?\nWhat kind of distribution do we use in a hypothesis test of correlation?",
    "crumbs": [
      "Modules",
      "Module 5: Measuring relationships and fitting models",
      "Modeling continuous relationships: Correlation"
    ]
  },
  {
    "objectID": "5-correlation-and-regression.html",
    "href": "5-correlation-and-regression.html",
    "title": "Correlation and regression",
    "section": "",
    "text": "Slides for “Correlation and regression”"
  },
  {
    "objectID": "5-intro.html",
    "href": "5-intro.html",
    "title": "TECH3 Applied statistics",
    "section": "",
    "text": "Measuring relationships and fitting models\n\nFocus: The difference between modelling continuous and categorical relationships.\nKey topics: Correlation, causation, linear regression, predictions, odds ratio, binary regression, chi-squared tests.\n\nSpecial Emphasis: Address overfitting using cross-validation.",
    "crumbs": [
      "Modules",
      "Module 5: Measuring relationships and fitting models",
      "Introduction"
    ]
  },
  {
    "objectID": "5-linear-regression.html",
    "href": "5-linear-regression.html",
    "title": "Linear regression",
    "section": "",
    "text": "“Linear regression”\n“Least squares estimation”\n“Adding covariates”\n“Least squares estimators”\n“Correlation and regression”\n“Hypothesis testing on regression parameters”\n“Adding more covariates”\n“Adding interactions between variables”\n“Nonlinear linear regression”\n\n\n\n\n\nWhat is the interpretation of \\(\\beta_0\\) is a simple linear regression \\(Y=\\beta_0+\\beta_1 X+\\epsilon\\)?\nWhat is the interpretation of \\(\\beta_1\\)?\nWhat are we minimizing when estimating \\(\\beta_0\\) and \\(\\beta_1\\)?\nWhen is there a one-to-one relationship between the correlation between \\(X\\) and \\(Y\\) and the regression coefficient for \\(X\\)?\nWhat is the null hypothesis when we are looking at the default table from a linear regression in Python?\nIf \\(\\text{Height_i}=\\beta_0+\\beta_a \\text{Age}_i + \\beta_m\\text{Male}_i+\\epsilon_i\\), where \\(\\text{Male}_i\\) is 1 if the child is male, what is the intercept for male and females?\nIf \\(\\text{Height_i}=\\beta_0+\\beta_a \\text{Age}_i + \\beta_m\\text{Male}_i+\\beta_{a\\times m}\\text{Age}_i\\text{Male}_i+\\epsilon_i\\), where \\(\\text{Male}_i\\) is 1 if the child is male, what is the slope for male and females?\nHow can we implement a nonlinear relations in a linear regression?",
    "crumbs": [
      "Modules",
      "Module 5: Measuring relationships and fitting models",
      "Linear regression"
    ]
  },
  {
    "objectID": "5-linear-regression.html#slides",
    "href": "5-linear-regression.html#slides",
    "title": "Linear regression",
    "section": "",
    "text": "“Linear regression”\n“Least squares estimation”\n“Adding covariates”\n“Least squares estimators”\n“Correlation and regression”\n“Hypothesis testing on regression parameters”\n“Adding more covariates”\n“Adding interactions between variables”\n“Nonlinear linear regression”",
    "crumbs": [
      "Modules",
      "Module 5: Measuring relationships and fitting models",
      "Linear regression"
    ]
  },
  {
    "objectID": "5-linear-regression.html#control-questions",
    "href": "5-linear-regression.html#control-questions",
    "title": "Linear regression",
    "section": "",
    "text": "What is the interpretation of \\(\\beta_0\\) is a simple linear regression \\(Y=\\beta_0+\\beta_1 X+\\epsilon\\)?\nWhat is the interpretation of \\(\\beta_1\\)?\nWhat are we minimizing when estimating \\(\\beta_0\\) and \\(\\beta_1\\)?\nWhen is there a one-to-one relationship between the correlation between \\(X\\) and \\(Y\\) and the regression coefficient for \\(X\\)?\nWhat is the null hypothesis when we are looking at the default table from a linear regression in Python?\nIf \\(\\text{Height_i}=\\beta_0+\\beta_a \\text{Age}_i + \\beta_m\\text{Male}_i+\\epsilon_i\\), where \\(\\text{Male}_i\\) is 1 if the child is male, what is the intercept for male and females?\nIf \\(\\text{Height_i}=\\beta_0+\\beta_a \\text{Age}_i + \\beta_m\\text{Male}_i+\\beta_{a\\times m}\\text{Age}_i\\text{Male}_i+\\epsilon_i\\), where \\(\\text{Male}_i\\) is 1 if the child is male, what is the slope for male and females?\nHow can we implement a nonlinear relations in a linear regression?",
    "crumbs": [
      "Modules",
      "Module 5: Measuring relationships and fitting models",
      "Linear regression"
    ]
  },
  {
    "objectID": "5-prediction-models.html",
    "href": "5-prediction-models.html",
    "title": "Prediction models",
    "section": "",
    "text": "“Explain vs predict”\n“Prediction and overfitting”\n“Confidence and prediction intervals”\n“Cross validation”\n\n\n\n\n\nWhat is your goal if you are concerned about model assumptions being fulfilled?\nWhat is your goal if you are mostly concerned with out-of-sample performance?\nWhat is overfitting and how can we avoid it?\nWhich is widest - confidence interval of prediction interval?\nWhy do we need a train-test split?\nWhat is the end-goal of a cross validation routine?",
    "crumbs": [
      "Modules",
      "Module 5: Measuring relationships and fitting models",
      "Prediction models"
    ]
  },
  {
    "objectID": "5-prediction-models.html#slides",
    "href": "5-prediction-models.html#slides",
    "title": "Prediction models",
    "section": "",
    "text": "“Explain vs predict”\n“Prediction and overfitting”\n“Confidence and prediction intervals”\n“Cross validation”",
    "crumbs": [
      "Modules",
      "Module 5: Measuring relationships and fitting models",
      "Prediction models"
    ]
  },
  {
    "objectID": "5-prediction-models.html#control-questions",
    "href": "5-prediction-models.html#control-questions",
    "title": "Prediction models",
    "section": "",
    "text": "What is your goal if you are concerned about model assumptions being fulfilled?\nWhat is your goal if you are mostly concerned with out-of-sample performance?\nWhat is overfitting and how can we avoid it?\nWhich is widest - confidence interval of prediction interval?\nWhy do we need a train-test split?\nWhat is the end-goal of a cross validation routine?",
    "crumbs": [
      "Modules",
      "Module 5: Measuring relationships and fitting models",
      "Prediction models"
    ]
  },
  {
    "objectID": "5-textbook.html",
    "href": "5-textbook.html",
    "title": "TECH3 Applied statistics",
    "section": "",
    "text": "The following is a redistribution of chapters 12, 13, 14, and 17 of Statistical Thinking for the 21st Century by Russell A. Poldrack, 2019, under the CC BY-NC 4.0 licence.",
    "crumbs": [
      "Modules",
      "Module 5: Measuring relationships and fitting models",
      "Textbook"
    ]
  },
  {
    "objectID": "5-textbook.html#modeling-categorical-relationships",
    "href": "5-textbook.html#modeling-categorical-relationships",
    "title": "TECH3 Applied statistics",
    "section": "Modeling categorical relationships",
    "text": "Modeling categorical relationships",
    "crumbs": [
      "Modules",
      "Module 5: Measuring relationships and fitting models",
      "Textbook"
    ]
  },
  {
    "objectID": "5-textbook.html#modeling-continuous-relationships",
    "href": "5-textbook.html#modeling-continuous-relationships",
    "title": "TECH3 Applied statistics",
    "section": "Modeling continuous relationships",
    "text": "Modeling continuous relationships",
    "crumbs": [
      "Modules",
      "Module 5: Measuring relationships and fitting models",
      "Textbook"
    ]
  },
  {
    "objectID": "5-textbook.html#the-general-linear-model",
    "href": "5-textbook.html#the-general-linear-model",
    "title": "TECH3 Applied statistics",
    "section": "The General Linear Model",
    "text": "The General Linear Model",
    "crumbs": [
      "Modules",
      "Module 5: Measuring relationships and fitting models",
      "Textbook"
    ]
  },
  {
    "objectID": "5-textbook.html#practical-statistical-modeling",
    "href": "5-textbook.html#practical-statistical-modeling",
    "title": "TECH3 Applied statistics",
    "section": "Practical statistical modeling",
    "text": "Practical statistical modeling",
    "crumbs": [
      "Modules",
      "Module 5: Measuring relationships and fitting models",
      "Textbook"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Sondre Hølleland,  Assistant professor\n\n\n\n\n\nGeir Drage Berentsen,  Associate professor\n\n\n\nThis course and website has been developed by Sondre Hølleland, assistant professor, and Geir Drage Berentsen, associate professor, both at Norwegian School of Economics, Department of Business and Management Science. More about us will be added."
  },
  {
    "objectID": "case-1.html",
    "href": "case-1.html",
    "title": "Case 1",
    "section": "",
    "text": "Case 1\n\n\n\nPoster by Hege Landsvik ©\n\n\nSource: Landsvik, H., Martuza, J., Skard, S., Pedersen, L. J. T., & Jørgensen, S. Group Identity and Pro-Environmental Behavior in Public Settings: Using Identity-based Nudges to Enhance Recycling in the Field. Preprint.\nIn this case session, you will be making a descriptive analysis of a dataset from a field experiment on Brann Stadium, the home of Bergen’s local soccer team.\nThe researchers gathered data on the number of cups sold on the away stand at Brann Stadium. After each match, they went through the garbage, at counted how many cups was correctly recycled, that is, put in the bin for soda- and coffee cups. The experiment was constructed using two treatments. Firstly, they used a nudge, by putting a sign over the bins asking “Who is the most environmental away supporters in Eliteserien (Norway’s highest soccer league)?” Sort you garbage here.” The sign would also have the logo of the away team. This is called an ingroup appeal. The other treatment is gamification. Here they would set up a table showing the sorting rate of the other away teams. The intent of the researchers is to make it a sorting competition against the other teams.\nThe dataset has the following variables:\n\nMatch_Number: Match ID. Starting from the second home match of the 2023 season.\nOpponent: Name of the away team - adjusted for æøå.\nAway_Conditon: Baseline, Ingroup or Outgroup\nGamification_Days: Baseline, No, Yes.\nWinner: Who won the match? Home, tie or Away.\nSortedCup: Was the cup sorted? 0: No, 1: Yes.\n\nEach observation is a cup sold. If the cup was put in the correct bin, sortedCup column is 1, otherwise it is zero.\n\nCreate a frequency table for each match counting how many cups was sold and how many was correctly sorted. Which away team supporters are the largest consumers of coffee?\nAdd a relative frequency (sorting rate) column to your table. What team is the best recyclers?\nSummarize the sorting rates across teams and treatments, using mean, median, standard deviation, 1st and 3rd quartile.\nCreate a good visualization of the sorting rate for the various teams.\nNow, the different teams were exposed to different treatments, both for the in- or out-group message on the banner and whether they were exposed to gamification or not. If we disregard the individual teams and only consider these treatments marginally (marginally means one treatment at a time).\n\n\nUse suitable summary statistics to summarize the effect of different treatments.\nMake suitable figures to visualize the effects. One suggestion: Use boxplots.\n\nDiscuss your summary statistics and visuals within the group."
  },
  {
    "objectID": "data/countries/country_data_wrangling.html",
    "href": "data/countries/country_data_wrangling.html",
    "title": "Country data - wrangling",
    "section": "",
    "text": "Wrangling of countries data into a single dataset\n\nlibrary(tidyverse)\n\nWarning: package 'ggplot2' was built under R version 4.4.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\ncountry codes with continent from https://datahub.io/core/country-codes\n\n# north america is coded as NA so need to exclude it from NA list\ncountry_codes &lt;- read_delim('country_codes.csv', na=c('')) %&gt;%\n    rename(CountryCode = `ISO3166-1-Alpha-3`, \n           Name = `UNTERM English Formal`) %&gt;%\n    select(Continent, CountryCode)\n\nRows: 250 Columns: 56\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (48): FIFA, Dial, ISO3166-1-Alpha-3, MARC, is_independent, ISO3166-1-num...\ndbl  (5): Intermediate Region Code, M49, Sub-region Code, Region Code, Geona...\nnum  (2): GAUL, ISO4217-currency_minor_unit\nlgl  (1): Global Code\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nfrom google, this contains latitude/longitude\n\ncountries &lt;- read_delim('countries/countries.csv') %&gt;%\n  rename(CountryName = name)\n\nRows: 245 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): country, name\ndbl (2): latitude, longitude\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nfrom worldbank, this contains population over time\n\n\nyears are in columns\n\npopulation &lt;- read_csv('worldbank/API_SP.POP.TOTL_DS2_en_csv_v2_3469297.csv', show_col_types = FALSE, skip=4) %&gt;% \n  select(-`...66`) %&gt;%\n  rename(CountryName = `Country Name`,\n         CountryCode = `Country Code`,\n         Population2020 = `2020`) %&gt;%\n  select(CountryName, CountryCode, Population2020)\n\nNew names:\n• `` -&gt; `...66`\n\n\n\ncountry_metadata &lt;- read_csv('worldbank/Metadata_Country_API_SP.POP.TOTL_DS2_en_csv_v2_3469297.csv', show_col_types = FALSE) %&gt;% \n  select(-`...6`) %&gt;%\n  rename(CountryCode = `Country Code`) %&gt;%\n  select(-SpecialNotes, -TableName)\n\nNew names:\n• `` -&gt; `...6`\n\n\n\npopdata &lt;- merge(population, country_metadata, by='CountryCode') %&gt;%\n  drop_na(Region)  # remove regional summaries\n\npopdata &lt;- merge(popdata, country_codes, by='CountryCode') %&gt;%\n  drop_na(Continent)\n\n\npopdata_with_geodata &lt;- merge(popdata, countries, by=\"CountryName\")\n\n\ngdp &lt;- read_csv('worldbank_gdp/API_NY.GDP.MKTP.CD_DS2_en_csv_v2_3469429.csv', show_col_types = FALSE, skip=4) %&gt;% \n  select(-`...66`) %&gt;%\n  rename(CountryName = `Country Name`,\n         CountryCode = `Country Code`,\n         GDP2020 = `2020`)  %&gt;%\n  select(CountryCode, GDP2020) %&gt;%\n  drop_na(GDP2020)\n\nNew names:\n• `` -&gt; `...66`\n\npopdata_with_geodata_and_gdp &lt;- merge(popdata_with_geodata, gdp)\n\n\nwrite_csv(popdata_with_geodata_and_gdp, 'country_data.csv')"
  },
  {
    "objectID": "oldexams.html",
    "href": "oldexams.html",
    "title": "Old exams",
    "section": "",
    "text": "Old exams\nOld exams will be made avaiable here.",
    "crumbs": [
      "Old exams"
    ]
  },
  {
    "objectID": "reference.html",
    "href": "reference.html",
    "title": "References",
    "section": "",
    "text": "References"
  }
]